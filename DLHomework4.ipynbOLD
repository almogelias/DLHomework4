{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DLHomework4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "history_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/almogelias/DLHomework4/blob/main/DLHomework4.ipynbOLD\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCy8CL1baoyJ"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQOanPh61S3T",
        "outputId": "86f1f0a1-d7ec-4bca-bb08-d10cc5d77fd8"
      },
      "source": [
        "\n",
        "from keras.datasets import mnist\n",
        "from keras.layers import Input, Dense, Reshape, Flatten\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.models import Sequential, Model\n",
        "from keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy.spatial import distance\n",
        "\n",
        "!git clone -s https://github.com/almogelias/DLHomework4.git DLHomework4\n",
        "\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'DLHomework4' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmisa9Ktasvz"
      },
      "source": [
        "# Preprocess of diabetes.arff file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wm6DWTA1n9A"
      },
      "source": [
        "\n",
        "from scipy.io import arff\n",
        "import pandas as pd\n",
        "import os\n",
        "from sklearn import preprocessing\n",
        "\n",
        "\n",
        "#Load the data using \"arff.loadarff\" then convert it to dataframe\n",
        "\n",
        "repository_path = os.path.join(os.getcwd(), 'DLHomework4')\n",
        "train_diabetes_path = os.path.join(repository_path, 'diabetes.arff')\n",
        "\n",
        "data = arff.loadarff(train_diabetes_path)\n",
        "df = pd.DataFrame(data[0])\n",
        "\n",
        "y_train = df['class'].replace({b'tested_positive':'1', b'tested_negative':'0'})\n",
        "\n",
        "        \n",
        "# Drop last column of a dataframe\n",
        "x_train = df.iloc[: , :-1]\n",
        "min_max_scaler = preprocessing.MinMaxScaler()\n",
        "x_scaled = min_max_scaler.fit_transform(x_train)\n",
        "x_train = pd.DataFrame(x_scaled)\n",
        "#x_train, x_test, y_train, y_test = train_test_split(x, y,test_size=3)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PK7Tv9oz1hp7"
      },
      "source": [
        "#Define input dimensions\n",
        "diabetes_dim = x_train.shape[1]\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dSkANdxazYE"
      },
      "source": [
        "# Generator Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QEHK-wMh6mjf",
        "outputId": "70b51ec8-a894-4703-ed27-21a8aa90210e"
      },
      "source": [
        "#Given input of noise (latent) vector, the Generator produces an sample.\n",
        "def build_generator():\n",
        "\n",
        "    noise_shape = (8,) #1D array of size 100 (latent vector / noise)\n",
        "\n",
        "#Define your generator network \n",
        "#Here we are only using Dense layers. But network can be complicated based\n",
        "#on the application. For example, you can use VGG for super res. GAN.         \n",
        "\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(diabetes_dim, input_shape=noise_shape))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(Dense(diabetes_dim*diabetes_dim))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(Dense(diabetes_dim*diabetes_dim*diabetes_dim))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    \n",
        "    model.add(Dense(np.prod(diabetes_dim), activation='tanh'))\n",
        "    #model.add(Reshape(diabetes_dim))\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    noise = Input(shape=noise_shape)\n",
        "    img = model(noise)    #Generated sample\n",
        "    \n",
        "    return Model(noise, img)\n",
        "\n",
        "#Alpha — α is a hyperparameter which controls the underlying value to which the\n",
        "#function saturates negatives network inputs.\n",
        "#Momentum — Speed up the training\n",
        "\n",
        "generator = build_generator()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_46 (Dense)             (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_28 (LeakyReLU)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_16 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "dense_47 (Dense)             (None, 64)                576       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_29 (LeakyReLU)   (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_17 (Batc (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "dense_48 (Dense)             (None, 512)               33280     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_30 (LeakyReLU)   (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_18 (Batc (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "dense_49 (Dense)             (None, 8)                 4104      \n",
            "=================================================================\n",
            "Total params: 40,368\n",
            "Trainable params: 39,200\n",
            "Non-trainable params: 1,168\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOStgE9na2bM"
      },
      "source": [
        "#Discriminator Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQCE_DE_805i",
        "outputId": "ed645839-e811-4195-a574-7d627211d2dd"
      },
      "source": [
        "\n",
        "#Given an input sample, the Discriminator outputs the likelihood of the sample being real.\n",
        "#Binary classification - true or false (we're calling it validity)\n",
        "\n",
        "def build_discriminator():\n",
        "\n",
        "    diabetes_dim_shape = (diabetes_dim,)\n",
        "    model1 = Sequential()\n",
        "\n",
        "    model1.add(Dense(diabetes_dim, input_shape=diabetes_dim_shape))\n",
        "    model1.add(Dense(diabetes_dim*diabetes_dim*diabetes_dim))\n",
        "    model1.add(LeakyReLU(alpha=0.2))\n",
        "    model1.add(Dense(diabetes_dim*diabetes_dim))\n",
        "    model1.add(LeakyReLU(alpha=0.2))\n",
        "    model1.add(Dense(1, activation='sigmoid'))\n",
        "    model1.summary()\n",
        "\n",
        "    diabetes_input = Input(shape=diabetes_dim)\n",
        "    validity = model1(diabetes_input)\n",
        "\n",
        "    return Model(diabetes_input, validity)\n",
        "#The validity is the Discriminator’s guess of input being real or not.\n",
        "discriminator = build_discriminator()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_50 (Dense)             (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "dense_51 (Dense)             (None, 512)               4608      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_31 (LeakyReLU)   (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_52 (Dense)             (None, 64)                32832     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_32 (LeakyReLU)   (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_53 (Dense)             (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 37,577\n",
            "Trainable params: 37,577\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9nzYdKFa7o0"
      },
      "source": [
        "#Training flow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djqKY7Y2-y5m"
      },
      "source": [
        "#Now that we have constructed our two models it’s time to pit them against each other.\n",
        "#We do this by defining a training function, loading the data set, re-scaling our training\n",
        "#samples and setting the ground truths. \n",
        "def train(epochs, batch_size=128, save_interval=50):\n",
        "\n",
        "    half_batch = int(batch_size / 2)\n",
        "#We then loop through a number of epochs to train our Discriminator by first selecting\n",
        "#a random batch of samples from our true dataset, generating a set of samples from our\n",
        "#Generator, feeding both set of samples into our Discriminator, and finally setting the\n",
        "#loss parameters for both the real and fake samples, as well as the combined loss. \n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "\n",
        "      # ---------------------\n",
        "      #  Train Discriminator\n",
        "      # ---------------------\n",
        "      \n",
        "      # Select a random half batch of real samples\n",
        "      idx = np.random.randint(0, x_train.shape[0], half_batch)\n",
        "      diabetes = x_train.iloc[idx]\n",
        "\n",
        "\n",
        "      noise = np.random.normal(0, 1, (half_batch, 8))\n",
        "\n",
        "      # Generate a half batch of fake samples\n",
        "      gen_diabetes = generator.predict(noise)\n",
        "\n",
        "      # Train the discriminator on real and fake samples, separately\n",
        "      #Research showed that separate training is more effective. \n",
        "      d_loss_real = discriminator.train_on_batch(diabetes, np.ones((half_batch, 1)))\n",
        "      d_loss_fake = discriminator.train_on_batch(gen_diabetes, np.zeros((half_batch, 1)))\n",
        "    #take average loss from real and fake samples. \n",
        "\n",
        "      d_loss = 0.5 * np.add(d_loss_real, d_loss_fake) \n",
        "      euclidean_distance = distance.euclidean(d_loss_real,d_loss_fake)\n",
        "      '''\n",
        "      if (euclidean_distance > 1.1):\n",
        "        print (\"real: \"+str(d_loss_real)+\" , fake: \"+str(d_loss_fake) + \" , Duclidean distance: \"+ str(euclidean_distance))\n",
        "        print ()\n",
        "        print(\"real diabetes values:\")\n",
        "        print(pd.DataFrame(diabetes))\n",
        "        print(\"fake diabetes values:\")\n",
        "        print(pd.DataFrame(gen_diabetes))\n",
        "      '''\n",
        "      \n",
        "      \n",
        "     \n",
        "        \n",
        "      \n",
        "    #And within the same loop we train our Generator, by setting the input noise and\n",
        "    #ultimately training the Generator to have the Discriminator label its samples as valid\n",
        "    #by specifying the gradient loss.\n",
        "      # ---------------------\n",
        "      #  Train Generator\n",
        "      # ---------------------\n",
        "    #Create noise vectors as input for generator. \n",
        "    #Create as many noise vectors as defined by the batch size. \n",
        "    #Based on normal distribution. Output will be of size (batch size, 100)\n",
        "      noise = np.random.normal(0, 1, (batch_size, 8)) \n",
        "\n",
        "      # The generator wants the discriminator to label the generated samples\n",
        "      # as valid (ones)\n",
        "      #This is where the genrator is trying to trick discriminator into believing\n",
        "      #the generated sample is true (hence value of 1 for y)\n",
        "      valid_y = np.array([1] * batch_size) #Creates an array of all ones of size=batch size\n",
        "\n",
        "      # Generator is part of combined where it got directly linked with the discriminator\n",
        "      # Train the generator with noise as x and 1 as y. \n",
        "      # Again, 1 as the output as it is adversarial and if generator did a great\n",
        "      #job of folling the discriminator then the output would be 1 (true)\n",
        "      g_loss = combined.train_on_batch(noise, valid_y)\n",
        "      if (g_loss > 1):\n",
        "        print(\"fake diabetes noise values that fooled the model:\")\n",
        "        print(pd.DataFrame(noise))\n",
        "\n",
        "    #Additionally, in order for us to keep track of our training process, we print the\n",
        "    #progress and save the sample sample output depending on the epoch interval specified.  \n",
        "    #Plot the progress\n",
        "      \n",
        "      print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
        "      \n"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3T_Mcdy1pBvB",
        "outputId": "0a7d0820-46a0-42e0-fcff-b405b7474c1d"
      },
      "source": [
        "optimizer = Adam(0.0002, 0.5)  #Learning rate and momentum.\n",
        "discriminator = build_discriminator()\n",
        "discriminator.compile(loss='binary_crossentropy',\n",
        "    optimizer=optimizer,\n",
        "    metrics=['accuracy'])\n",
        "\n",
        "#build and compile our Discriminator, pick the loss function\n",
        "\n",
        "#SInce we are only generating (faking) samples, let us not track any metrics.\n",
        "generator = build_generator()\n",
        "generator.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "\n",
        "##This builds the Generator and defines the input noise. \n",
        "#In a GAN the Generator network takes noise z as an input to produce its samples.  \n",
        "z = Input(shape=(8,))   #Our random input to the generator\n",
        "test_if_diabete = generator(z)\n",
        "\n",
        "#This ensures that when we combine our networks we only train the Generator.\n",
        "#While generator training we do not want discriminator weights to be adjusted. \n",
        "#This Doesn't affect the above descriminator training.     \n",
        "discriminator.trainable = False  \n",
        "\n",
        "#This specifies that our Discriminator will take the samples generated by our Generator\n",
        "#and true dataset and set its output to a parameter called valid, which will indicate\n",
        "#whether the input is real or not.  \n",
        "valid = discriminator(test_if_diabete)  #Validity check on the generated sample\n",
        "\n",
        "\n",
        "#Here we combined the models and also set our loss function and optimizer. \n",
        "#Again, we are only training the generator here. \n",
        "#The ultimate goal here is for the Generator to fool the Discriminator.  \n",
        "# The combined model  (stacked generator and discriminator) takes\n",
        "# noise as input => generates samples => determines validity\n",
        "\n",
        "combined = Model(z, valid)\n",
        "combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "\n",
        "\n",
        "train(epochs=150, batch_size=32, save_interval=10)\n",
        "\n",
        "#Save model for future use to generate fake samples\n",
        "#Not tested yet... make sure right model is being saved..\n",
        "#Compare with GAN4\n",
        "\n",
        "generator.save('generator_model.h5')  #Test the model on GAN4_predict...\n",
        "#Change epochs back to 30K\n",
        "                \n",
        "#Epochs dictate the number of backward and forward propagations, the batch_size\n",
        "#indicates the number of training samples per backward/forward propagation, and the\n",
        "#sample_interval specifies after how many epochs we call our sample function."
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_54 (Dense)             (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "dense_55 (Dense)             (None, 512)               4608      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_33 (LeakyReLU)   (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_56 (Dense)             (None, 64)                32832     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_34 (LeakyReLU)   (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_57 (Dense)             (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 37,577\n",
            "Trainable params: 37,577\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_58 (Dense)             (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_35 (LeakyReLU)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_19 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "dense_59 (Dense)             (None, 64)                576       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_36 (LeakyReLU)   (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_20 (Batc (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "dense_60 (Dense)             (None, 512)               33280     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_37 (LeakyReLU)   (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_21 (Batc (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "dense_61 (Dense)             (None, 8)                 4104      \n",
            "=================================================================\n",
            "Total params: 40,368\n",
            "Trainable params: 39,200\n",
            "Non-trainable params: 1,168\n",
            "_________________________________________________________________\n",
            "0 [D loss: 0.689017, acc.: 43.75%] [G loss: 0.670700]\n",
            "1 [D loss: 0.675107, acc.: 71.88%] [G loss: 0.674603]\n",
            "2 [D loss: 0.659903, acc.: 65.62%] [G loss: 0.665813]\n",
            "3 [D loss: 0.651357, acc.: 71.88%] [G loss: 0.669828]\n",
            "4 [D loss: 0.644697, acc.: 71.88%] [G loss: 0.673722]\n",
            "5 [D loss: 0.649565, acc.: 62.50%] [G loss: 0.687264]\n",
            "6 [D loss: 0.638924, acc.: 65.62%] [G loss: 0.705829]\n",
            "7 [D loss: 0.613966, acc.: 71.88%] [G loss: 0.673945]\n",
            "8 [D loss: 0.608075, acc.: 75.00%] [G loss: 0.661796]\n",
            "9 [D loss: 0.585219, acc.: 84.38%] [G loss: 0.694530]\n",
            "10 [D loss: 0.638335, acc.: 62.50%] [G loss: 0.688558]\n",
            "11 [D loss: 0.649885, acc.: 59.38%] [G loss: 0.694096]\n",
            "12 [D loss: 0.652960, acc.: 62.50%] [G loss: 0.709238]\n",
            "13 [D loss: 0.606862, acc.: 68.75%] [G loss: 0.697488]\n",
            "14 [D loss: 0.596763, acc.: 81.25%] [G loss: 0.693143]\n",
            "15 [D loss: 0.598815, acc.: 71.88%] [G loss: 0.682523]\n",
            "16 [D loss: 0.578185, acc.: 68.75%] [G loss: 0.689308]\n",
            "17 [D loss: 0.627652, acc.: 56.25%] [G loss: 0.696697]\n",
            "18 [D loss: 0.644935, acc.: 62.50%] [G loss: 0.672691]\n",
            "19 [D loss: 0.623789, acc.: 59.38%] [G loss: 0.689117]\n",
            "20 [D loss: 0.648579, acc.: 62.50%] [G loss: 0.703317]\n",
            "21 [D loss: 0.619609, acc.: 65.62%] [G loss: 0.697005]\n",
            "22 [D loss: 0.601339, acc.: 68.75%] [G loss: 0.721309]\n",
            "23 [D loss: 0.610116, acc.: 65.62%] [G loss: 0.681219]\n",
            "24 [D loss: 0.660453, acc.: 62.50%] [G loss: 0.759843]\n",
            "25 [D loss: 0.607543, acc.: 68.75%] [G loss: 0.693544]\n",
            "26 [D loss: 0.619932, acc.: 68.75%] [G loss: 0.706712]\n",
            "27 [D loss: 0.586406, acc.: 75.00%] [G loss: 0.736247]\n",
            "28 [D loss: 0.631643, acc.: 62.50%] [G loss: 0.678319]\n",
            "29 [D loss: 0.612518, acc.: 62.50%] [G loss: 0.713042]\n",
            "30 [D loss: 0.574631, acc.: 78.12%] [G loss: 0.760056]\n",
            "31 [D loss: 0.603325, acc.: 71.88%] [G loss: 0.786374]\n",
            "32 [D loss: 0.618760, acc.: 68.75%] [G loss: 0.761826]\n",
            "33 [D loss: 0.610830, acc.: 75.00%] [G loss: 0.828671]\n",
            "34 [D loss: 0.598310, acc.: 78.12%] [G loss: 0.721213]\n",
            "35 [D loss: 0.585375, acc.: 78.12%] [G loss: 0.745121]\n",
            "36 [D loss: 0.583466, acc.: 75.00%] [G loss: 0.751612]\n",
            "37 [D loss: 0.627706, acc.: 71.88%] [G loss: 0.797771]\n",
            "38 [D loss: 0.624008, acc.: 75.00%] [G loss: 0.787546]\n",
            "39 [D loss: 0.578457, acc.: 78.12%] [G loss: 0.761761]\n",
            "40 [D loss: 0.602869, acc.: 81.25%] [G loss: 0.815576]\n",
            "41 [D loss: 0.637797, acc.: 68.75%] [G loss: 0.788464]\n",
            "42 [D loss: 0.601552, acc.: 81.25%] [G loss: 0.770000]\n",
            "43 [D loss: 0.601788, acc.: 78.12%] [G loss: 0.794091]\n",
            "44 [D loss: 0.555687, acc.: 81.25%] [G loss: 0.804331]\n",
            "45 [D loss: 0.599754, acc.: 78.12%] [G loss: 0.803363]\n",
            "46 [D loss: 0.625837, acc.: 65.62%] [G loss: 0.807180]\n",
            "47 [D loss: 0.590439, acc.: 81.25%] [G loss: 0.827205]\n",
            "48 [D loss: 0.604084, acc.: 78.12%] [G loss: 0.808025]\n",
            "49 [D loss: 0.598948, acc.: 75.00%] [G loss: 0.797969]\n",
            "50 [D loss: 0.597061, acc.: 78.12%] [G loss: 0.830133]\n",
            "51 [D loss: 0.621549, acc.: 78.12%] [G loss: 0.793372]\n",
            "52 [D loss: 0.625445, acc.: 78.12%] [G loss: 0.832013]\n",
            "53 [D loss: 0.609066, acc.: 75.00%] [G loss: 0.890995]\n",
            "54 [D loss: 0.600363, acc.: 78.12%] [G loss: 0.905506]\n",
            "55 [D loss: 0.582132, acc.: 78.12%] [G loss: 0.833795]\n",
            "56 [D loss: 0.551525, acc.: 90.62%] [G loss: 0.855082]\n",
            "57 [D loss: 0.576438, acc.: 81.25%] [G loss: 0.889306]\n",
            "58 [D loss: 0.593661, acc.: 78.12%] [G loss: 0.902629]\n",
            "59 [D loss: 0.527724, acc.: 93.75%] [G loss: 0.844643]\n",
            "60 [D loss: 0.575330, acc.: 75.00%] [G loss: 0.861733]\n",
            "61 [D loss: 0.535797, acc.: 87.50%] [G loss: 0.836051]\n",
            "62 [D loss: 0.552207, acc.: 87.50%] [G loss: 0.864576]\n",
            "63 [D loss: 0.596285, acc.: 78.12%] [G loss: 0.875415]\n",
            "64 [D loss: 0.585411, acc.: 81.25%] [G loss: 0.920772]\n",
            "65 [D loss: 0.573450, acc.: 78.12%] [G loss: 0.879271]\n",
            "66 [D loss: 0.594247, acc.: 71.88%] [G loss: 0.880955]\n",
            "67 [D loss: 0.593132, acc.: 78.12%] [G loss: 0.832331]\n",
            "68 [D loss: 0.633593, acc.: 68.75%] [G loss: 0.830147]\n",
            "69 [D loss: 0.585304, acc.: 78.12%] [G loss: 0.913111]\n",
            "70 [D loss: 0.603152, acc.: 78.12%] [G loss: 0.880220]\n",
            "71 [D loss: 0.577162, acc.: 81.25%] [G loss: 0.898837]\n",
            "72 [D loss: 0.509110, acc.: 90.62%] [G loss: 0.884165]\n",
            "73 [D loss: 0.608358, acc.: 78.12%] [G loss: 0.902501]\n",
            "74 [D loss: 0.548403, acc.: 78.12%] [G loss: 0.958394]\n",
            "75 [D loss: 0.622803, acc.: 68.75%] [G loss: 0.960182]\n",
            "76 [D loss: 0.584997, acc.: 75.00%] [G loss: 0.818500]\n",
            "77 [D loss: 0.628132, acc.: 71.88%] [G loss: 0.842451]\n",
            "78 [D loss: 0.588979, acc.: 75.00%] [G loss: 0.959714]\n",
            "79 [D loss: 0.609628, acc.: 68.75%] [G loss: 0.959371]\n",
            "80 [D loss: 0.561417, acc.: 75.00%] [G loss: 0.871017]\n",
            "81 [D loss: 0.610524, acc.: 78.12%] [G loss: 0.869542]\n",
            "82 [D loss: 0.626660, acc.: 71.88%] [G loss: 0.984820]\n",
            "83 [D loss: 0.624614, acc.: 68.75%] [G loss: 0.980215]\n",
            "84 [D loss: 0.571900, acc.: 78.12%] [G loss: 0.960141]\n",
            "85 [D loss: 0.628434, acc.: 68.75%] [G loss: 0.947542]\n",
            "86 [D loss: 0.635260, acc.: 65.62%] [G loss: 0.869581]\n",
            "87 [D loss: 0.681135, acc.: 56.25%] [G loss: 0.980155]\n",
            "88 [D loss: 0.573368, acc.: 84.38%] [G loss: 0.927911]\n",
            "89 [D loss: 0.648729, acc.: 68.75%] [G loss: 0.909546]\n",
            "fake diabetes noise values that fooled the model:\n",
            "           0         1         2  ...         5         6         7\n",
            "0  -1.885145  1.030923 -1.247345  ...  0.530479  0.577867 -1.686584\n",
            "1   1.575668  0.818985  1.969891  ...  2.135207 -2.355490 -0.717941\n",
            "2  -0.298108 -0.711467  1.527148  ...  0.886243  0.262268 -0.334906\n",
            "3  -0.452423  1.827028  1.436743  ... -0.449221  0.318084  0.293047\n",
            "4   0.470505 -0.418646  0.109882  ...  1.535487 -0.101130  0.579297\n",
            "5  -1.120778 -0.229029  0.366890  ... -1.139996 -0.542518 -0.266858\n",
            "6   1.054412  1.485572 -0.454389  ...  0.525491 -1.498848  0.538579\n",
            "7  -0.268110  1.361880 -0.187989  ...  0.487773  0.990194  0.276684\n",
            "8  -1.506080 -1.803760 -1.455083  ...  1.145701  1.071855  1.307689\n",
            "9   1.760323 -0.238697 -0.495462  ... -0.579199  0.276228 -0.034142\n",
            "10  0.703299 -0.777289 -1.413944  ...  0.701482 -2.074885  1.854166\n",
            "11 -0.875325  0.648144  0.096288  ...  0.595953 -0.007124 -1.880847\n",
            "12 -1.269796  0.478473  0.191284  ...  2.519068  0.079920  1.122564\n",
            "13 -1.750874  0.433565  1.190517  ... -0.280334  0.808254  1.214357\n",
            "14 -0.819596 -0.210983  0.822118  ...  1.508076  1.517539  1.034835\n",
            "15  0.974017  1.369413  0.824753  ...  1.856016 -0.694995  0.490656\n",
            "16  1.193489  0.768172  1.993104  ...  1.063136  0.534507 -0.233299\n",
            "17 -0.836831 -0.885822  1.387320  ... -0.471522  0.392583 -1.005556\n",
            "18 -1.713068  0.026552  0.302622  ...  1.154372  0.945123 -0.429668\n",
            "19 -0.817846 -0.249283  2.151103  ...  0.562505 -0.746595  0.491833\n",
            "20  0.295081  0.713972  1.777386  ... -1.306259  0.507098 -0.223860\n",
            "21  1.622290  1.174931 -0.097729  ... -0.556248 -1.038308  0.245506\n",
            "22  1.732601  0.334636  0.089372  ... -0.524810 -0.327744 -1.333443\n",
            "23  0.883532  0.304045  0.012524  ... -1.001114  0.404542 -1.984233\n",
            "24  0.288070 -1.581924  0.600646  ...  0.343618 -0.081658  0.521589\n",
            "25 -0.254404  0.039451 -1.386829  ... -0.063661 -1.870105 -0.132863\n",
            "26 -1.131256 -0.584800 -1.738790  ... -0.702333  0.677957  1.238658\n",
            "27  0.844638 -0.222351  0.052454  ... -1.565196 -0.259085  2.574846\n",
            "28  0.448191 -0.330473 -0.804929  ...  1.104702  1.299772 -0.196358\n",
            "29 -0.975234  1.556477 -1.042997  ...  1.178364 -0.898455 -0.885271\n",
            "30 -1.313321 -0.867931 -0.462637  ...  1.790503 -0.234791  0.127918\n",
            "31 -0.486200 -0.969846  0.519430  ...  1.657246 -0.048124 -0.216265\n",
            "\n",
            "[32 rows x 8 columns]\n",
            "90 [D loss: 0.643924, acc.: 68.75%] [G loss: 1.001990]\n",
            "91 [D loss: 0.656285, acc.: 62.50%] [G loss: 0.911367]\n",
            "92 [D loss: 0.581836, acc.: 78.12%] [G loss: 0.965130]\n",
            "93 [D loss: 0.600736, acc.: 68.75%] [G loss: 0.902161]\n",
            "94 [D loss: 0.626610, acc.: 62.50%] [G loss: 0.981908]\n",
            "95 [D loss: 0.607192, acc.: 78.12%] [G loss: 0.881497]\n",
            "96 [D loss: 0.583830, acc.: 78.12%] [G loss: 0.978741]\n",
            "97 [D loss: 0.613516, acc.: 81.25%] [G loss: 0.964258]\n",
            "98 [D loss: 0.582218, acc.: 78.12%] [G loss: 0.921264]\n",
            "99 [D loss: 0.669868, acc.: 53.12%] [G loss: 0.910752]\n",
            "fake diabetes noise values that fooled the model:\n",
            "           0         1         2  ...         5         6         7\n",
            "0   0.394414  1.461246  0.272806  ... -2.040588 -1.425905  0.844307\n",
            "1   0.497040 -0.205813  0.190491  ...  1.080504  0.020793  0.082872\n",
            "2   0.129759 -0.290213 -1.772230  ... -0.260786  0.214893  1.790143\n",
            "3  -0.204478 -0.168210 -0.162058  ... -0.994989  0.662946 -0.655589\n",
            "4  -0.151656  0.318886 -0.128273  ...  1.921351  0.047116  0.699636\n",
            "5   0.234636 -0.987838 -1.950150  ... -0.479907  0.789794 -1.507794\n",
            "6   0.753375  0.467961 -0.377811  ...  0.152722  0.039201  0.713583\n",
            "7   1.194880  0.628630 -0.540703  ... -0.781224 -0.709054  1.070998\n",
            "8  -0.854434 -1.893681  0.425268  ...  0.051794  1.222637 -0.021179\n",
            "9   0.574947 -1.916278  0.857046  ...  0.561833 -0.529364  0.700721\n",
            "10  0.638111  0.488901 -0.254357  ... -0.523303  0.529936  0.467946\n",
            "11  1.715237 -0.220720  0.062603  ... -0.377133  0.656630  0.516935\n",
            "12 -0.972845 -0.373156 -0.997803  ...  1.398664 -0.275513 -0.676969\n",
            "13  1.538812 -0.802153  0.663808  ... -0.567616  1.278647 -1.225330\n",
            "14  1.497915  0.015139  2.142132  ...  0.176468  0.786161 -0.083942\n",
            "15  0.354261 -0.803005  0.232023  ...  0.650418 -0.334078 -0.347281\n",
            "16 -2.518895 -0.361973 -0.621261  ... -0.404072  1.760946 -1.231272\n",
            "17  0.031263  1.668338 -1.859498  ... -2.614678 -1.045046  0.517176\n",
            "18 -0.751461 -1.321289 -1.282995  ...  1.764202 -0.483149 -1.255307\n",
            "19 -1.141929 -0.951729 -0.123837  ...  0.391416 -0.910648 -0.748976\n",
            "20 -0.185425 -2.664240  1.691728  ...  1.387835 -0.453646 -0.621136\n",
            "21  0.470160  1.927847  0.303827  ...  1.514174  1.342965  0.168428\n",
            "22  0.760169 -0.230200  1.022042  ... -0.176023  0.076347  0.830902\n",
            "23  0.927508 -0.333193 -1.324370  ...  0.073416 -1.792127  1.355064\n",
            "24 -0.915388  1.135631 -0.150481  ... -0.140427 -0.310430 -1.344019\n",
            "25 -0.296621 -0.053543  2.635626  ... -1.227592 -0.516831 -0.440165\n",
            "26  0.036369 -0.984054 -1.636378  ... -0.168336  0.727039 -0.750781\n",
            "27  1.732246 -0.519824  0.571446  ... -1.798270 -0.281368  1.344644\n",
            "28  0.321702  0.439804 -0.326663  ... -1.683443  0.908484 -0.849938\n",
            "29 -1.216312  1.097781 -0.415963  ...  0.791077  0.173472 -2.330699\n",
            "30  0.509636 -1.185566 -0.789575  ...  0.096060 -0.010959  0.376653\n",
            "31  0.854823  1.549048 -0.875378  ... -0.304566  0.863341  1.586009\n",
            "\n",
            "[32 rows x 8 columns]\n",
            "100 [D loss: 0.627024, acc.: 78.12%] [G loss: 1.045544]\n",
            "101 [D loss: 0.636287, acc.: 65.62%] [G loss: 0.927877]\n",
            "102 [D loss: 0.580768, acc.: 84.38%] [G loss: 0.968121]\n",
            "103 [D loss: 0.630082, acc.: 75.00%] [G loss: 0.986967]\n",
            "104 [D loss: 0.567103, acc.: 81.25%] [G loss: 0.948088]\n",
            "105 [D loss: 0.622487, acc.: 65.62%] [G loss: 0.885850]\n",
            "106 [D loss: 0.557540, acc.: 87.50%] [G loss: 0.841002]\n",
            "107 [D loss: 0.623726, acc.: 68.75%] [G loss: 0.861832]\n",
            "108 [D loss: 0.662237, acc.: 65.62%] [G loss: 0.814757]\n",
            "109 [D loss: 0.609541, acc.: 71.88%] [G loss: 0.909232]\n",
            "110 [D loss: 0.655697, acc.: 68.75%] [G loss: 0.937701]\n",
            "111 [D loss: 0.640622, acc.: 68.75%] [G loss: 0.890501]\n",
            "112 [D loss: 0.596769, acc.: 71.88%] [G loss: 0.917717]\n",
            "113 [D loss: 0.589921, acc.: 71.88%] [G loss: 0.920238]\n",
            "114 [D loss: 0.634386, acc.: 65.62%] [G loss: 0.937650]\n",
            "115 [D loss: 0.609583, acc.: 71.88%] [G loss: 0.909003]\n",
            "116 [D loss: 0.642189, acc.: 71.88%] [G loss: 0.961042]\n",
            "117 [D loss: 0.662962, acc.: 62.50%] [G loss: 0.900069]\n",
            "118 [D loss: 0.655132, acc.: 59.38%] [G loss: 0.938918]\n",
            "119 [D loss: 0.647487, acc.: 62.50%] [G loss: 0.958762]\n",
            "120 [D loss: 0.611476, acc.: 65.62%] [G loss: 0.948637]\n",
            "121 [D loss: 0.610471, acc.: 78.12%] [G loss: 0.866764]\n",
            "fake diabetes noise values that fooled the model:\n",
            "           0         1         2  ...         5         6         7\n",
            "0  -1.124778  0.627377 -0.719501  ...  0.641502 -0.466827  1.733672\n",
            "1   0.711291  0.445648  0.155235  ...  0.581923 -0.434881 -0.575472\n",
            "2  -2.067060 -0.113067 -1.000703  ...  1.150334  0.069569  0.211878\n",
            "3  -0.071806 -0.112543 -0.100775  ... -1.111468 -0.364156 -0.375828\n",
            "4   0.152937  0.930679  2.543092  ...  0.031515 -0.087036 -0.683673\n",
            "5   0.878350  0.876738 -0.954523  ...  0.384384 -0.615105 -1.567105\n",
            "6   0.134219 -2.000600  1.676737  ... -0.400308  1.769671 -1.236769\n",
            "7  -2.008984  1.307335  0.841563  ... -0.696230  0.716156  0.245899\n",
            "8  -1.012837 -2.020888  1.285390  ...  0.908079 -0.180061  1.138657\n",
            "9   0.818139 -0.084382  0.323499  ...  1.521289  0.717740  1.080711\n",
            "10 -0.393472 -0.720661 -1.142236  ... -0.496341 -1.031409  0.456780\n",
            "11 -0.361962 -1.924970 -0.623528  ...  0.116087  0.722473  1.864100\n",
            "12 -0.227534 -0.957467 -0.125876  ...  0.438773  0.718473  0.295886\n",
            "13  0.586475 -0.643505 -0.830128  ... -1.992965 -0.396265 -0.901429\n",
            "14  0.365607  0.336806 -0.512488  ... -0.521092 -1.153575  1.055605\n",
            "15 -0.705698  0.446082 -0.743611  ...  0.272930  1.897780 -1.803760\n",
            "16  1.028218  1.109072 -1.081486  ... -1.488627  0.491453 -2.224631\n",
            "17 -0.316577 -0.684311 -1.269960  ...  1.868172 -0.420211  1.050371\n",
            "18  0.379764  0.664731  2.002779  ... -0.563558 -0.350755  0.290984\n",
            "19  0.514664  0.437196  0.806344  ... -0.902069 -0.421181 -0.939119\n",
            "20 -0.929152 -1.290851 -0.379817  ... -0.189953 -0.902080 -0.740410\n",
            "21 -2.354627  0.247703  1.208082  ... -0.295767  1.140801 -0.487170\n",
            "22  0.452791 -0.501358  1.289569  ... -0.889165 -0.410087  0.293963\n",
            "23 -0.106102 -1.487618 -1.095084  ... -0.455377 -0.554334 -0.729208\n",
            "24 -0.970177  0.083880 -2.404515  ... -0.381251  0.269418  2.757017\n",
            "25 -0.877677 -0.350456  1.020324  ... -0.018511 -0.549491 -1.308228\n",
            "26  0.044671  0.290963 -0.397667  ... -0.204505  0.317965 -0.016399\n",
            "27  0.971004 -0.463879 -0.196355  ... -0.970003  0.695025 -0.759529\n",
            "28  0.663408  1.587338 -0.627668  ...  0.335710 -1.309814  0.912937\n",
            "29  0.198402 -0.500057 -0.229423  ...  1.789963 -0.463970 -0.391101\n",
            "30 -1.159305 -0.788891  1.496984  ... -1.655635  1.233814  1.545743\n",
            "31  0.556020  0.248785  0.021491  ... -1.772792  1.056007  0.088162\n",
            "\n",
            "[32 rows x 8 columns]\n",
            "122 [D loss: 0.686707, acc.: 59.38%] [G loss: 1.010805]\n",
            "123 [D loss: 0.580994, acc.: 75.00%] [G loss: 0.911095]\n",
            "124 [D loss: 0.668332, acc.: 56.25%] [G loss: 0.928451]\n",
            "125 [D loss: 0.552759, acc.: 87.50%] [G loss: 0.947305]\n",
            "126 [D loss: 0.627280, acc.: 65.62%] [G loss: 0.884249]\n",
            "127 [D loss: 0.623392, acc.: 68.75%] [G loss: 0.898296]\n",
            "128 [D loss: 0.643114, acc.: 62.50%] [G loss: 0.871117]\n",
            "129 [D loss: 0.692748, acc.: 56.25%] [G loss: 0.929497]\n",
            "130 [D loss: 0.630161, acc.: 68.75%] [G loss: 0.897628]\n",
            "131 [D loss: 0.571170, acc.: 71.88%] [G loss: 0.887505]\n",
            "132 [D loss: 0.668762, acc.: 59.38%] [G loss: 0.923873]\n",
            "133 [D loss: 0.660671, acc.: 59.38%] [G loss: 0.977969]\n",
            "134 [D loss: 0.733745, acc.: 43.75%] [G loss: 0.862949]\n",
            "135 [D loss: 0.682362, acc.: 50.00%] [G loss: 0.949744]\n",
            "136 [D loss: 0.576257, acc.: 68.75%] [G loss: 0.854875]\n",
            "137 [D loss: 0.693760, acc.: 50.00%] [G loss: 0.942401]\n",
            "138 [D loss: 0.651579, acc.: 53.12%] [G loss: 0.929765]\n",
            "139 [D loss: 0.693048, acc.: 46.88%] [G loss: 0.900585]\n",
            "140 [D loss: 0.665240, acc.: 56.25%] [G loss: 0.898812]\n",
            "141 [D loss: 0.648615, acc.: 68.75%] [G loss: 0.942575]\n",
            "142 [D loss: 0.664633, acc.: 53.12%] [G loss: 0.942845]\n",
            "143 [D loss: 0.663743, acc.: 62.50%] [G loss: 0.907185]\n",
            "fake diabetes noise values that fooled the model:\n",
            "           0         1         2  ...         5         6         7\n",
            "0  -0.014454 -2.839411  0.350133  ... -0.881838  0.123910 -0.499838\n",
            "1  -0.364841  0.227739 -0.343726  ... -0.230022 -1.365994 -0.982954\n",
            "2   0.356626 -1.872804  0.020455  ... -0.221461  0.356492  0.510677\n",
            "3  -1.077508  0.281831  0.440616  ... -1.642463  1.914570  1.475019\n",
            "4  -0.653923 -0.851162  0.106127  ...  1.440945 -1.941040 -0.216863\n",
            "5   0.238744 -0.556760 -0.440976  ... -1.454794  0.200127  1.601223\n",
            "6   1.071672 -0.522505 -0.075497  ... -1.408466 -2.188830  0.815035\n",
            "7  -0.334766 -0.411608 -0.167892  ... -0.369877 -0.052348  0.044289\n",
            "8  -0.033767  1.406072  0.642743  ... -1.226418  0.364577  0.085410\n",
            "9  -0.152418 -0.469078  0.229817  ... -0.992434 -0.560530  1.616267\n",
            "10 -0.422995  0.536172 -0.286465  ...  1.197546  0.683111 -1.178113\n",
            "11  1.907928  1.237284 -1.902117  ...  0.744241 -0.727240 -0.270674\n",
            "12 -1.490948  0.820343 -0.624403  ...  0.613435  0.426208 -0.073511\n",
            "13 -1.536717  1.345365  1.481892  ...  1.001024  0.935755  1.738065\n",
            "14 -1.826122 -0.328937  1.136913  ... -1.256092 -0.442285 -1.377851\n",
            "15  0.757906 -0.801369  1.082920  ...  0.194713  0.577360 -0.306123\n",
            "16 -1.087224  0.031513  1.806327  ... -1.181193 -0.412474  1.122023\n",
            "17  1.108761 -0.524100  0.392643  ... -0.456501  0.883284 -0.899899\n",
            "18  0.230587  1.584993 -0.439279  ...  2.567115  0.274487  0.740171\n",
            "19  0.985293 -1.387482 -1.168330  ... -0.439159 -1.681029 -1.292642\n",
            "20 -0.100957  0.828843 -0.036302  ... -0.286260  1.267281  1.090956\n",
            "21  0.519571 -1.153219 -0.534033  ... -0.765105  0.795227 -1.512336\n",
            "22  1.891452 -1.525010 -2.071682  ...  0.418288  1.562325  0.728575\n",
            "23  0.966195 -0.037160 -0.444454  ... -2.346213 -0.620270  0.278259\n",
            "24 -0.593238 -1.366914 -1.882073  ...  0.029297 -0.002039  1.233162\n",
            "25  1.056410 -1.416534  0.293997  ...  0.057022  0.080304 -1.812670\n",
            "26  0.164435  1.024913  1.175522  ... -0.344846 -0.859017 -0.331038\n",
            "27  0.424965 -1.062560 -0.500756  ... -0.608692 -0.826164  1.012989\n",
            "28  0.205618  0.997458  0.120005  ...  0.937846  0.874272 -1.681744\n",
            "29 -0.632246  0.830566  0.137077  ...  0.905370  1.457499  1.249963\n",
            "30 -2.450011 -0.191903 -1.021300  ... -1.781445  0.177902  0.293562\n",
            "31  0.794749 -0.669404  0.590073  ...  0.404954  0.021891 -1.151026\n",
            "\n",
            "[32 rows x 8 columns]\n",
            "144 [D loss: 0.587049, acc.: 68.75%] [G loss: 1.006496]\n",
            "145 [D loss: 0.607172, acc.: 68.75%] [G loss: 0.910534]\n",
            "146 [D loss: 0.634612, acc.: 56.25%] [G loss: 0.924933]\n",
            "147 [D loss: 0.682449, acc.: 50.00%] [G loss: 0.874101]\n",
            "148 [D loss: 0.684402, acc.: 46.88%] [G loss: 0.901718]\n",
            "149 [D loss: 0.630878, acc.: 62.50%] [G loss: 0.928623]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bHU4fDX1_vC"
      },
      "source": [
        "# German Credit DB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "id": "UmmiPqzE1_Qj",
        "outputId": "680fe755-5bc5-45dd-b932-2be126b93dd4"
      },
      "source": [
        "'''\n",
        "\n",
        "#Load the data using \"arff.loadarff\" then convert it to dataframe\n",
        "\n",
        "repository_path = os.path.join(os.getcwd(), 'DLHomework4')\n",
        "train_german_credit_path = os.path.join(repository_path, 'german_credit.arff')\n",
        "\n",
        "data = arff.loadarff(train_german_credit_path)\n",
        "df = pd.DataFrame(data[0])\n",
        "\n",
        "y_train = df['21'].replace({b'1':'1', b'2':'2'})\n",
        "\n",
        "        \n",
        "# Drop last column of a dataframe\n",
        "x_train = df.iloc[: , :-1]\n",
        "min_max_scaler = preprocessing.MinMaxScaler()\n",
        "x_scaled = min_max_scaler.fit_transform(x_train)\n",
        "x_train = pd.DataFrame(x_scaled)\n",
        "#x_train, x_test, y_train, y_test = train_test_split(x, y,test_size=3)\n",
        "'''"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n\\n#Load the data using \"arff.loadarff\" then convert it to dataframe\\n\\nrepository_path = os.path.join(os.getcwd(), \\'DLHomework4\\')\\ntrain_german_credit_path = os.path.join(repository_path, \\'german_credit.arff\\')\\n\\ndata = arff.loadarff(train_german_credit_path)\\ndf = pd.DataFrame(data[0])\\n\\ny_train = df[\\'21\\'].replace({b\\'1\\':\\'1\\', b\\'2\\':\\'2\\'})\\n\\n        \\n# Drop last column of a dataframe\\nx_train = df.iloc[: , :-1]\\nmin_max_scaler = preprocessing.MinMaxScaler()\\nx_scaled = min_max_scaler.fit_transform(x_train)\\nx_train = pd.DataFrame(x_scaled)\\n#x_train, x_test, y_train, y_test = train_test_split(x, y,test_size=3)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODzEaMEkbcFs"
      },
      "source": [
        "# Random Forest model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DjteC-kSbfnk",
        "outputId": "f1cc8405-74bd-4f99-c692-3111a0b15c47"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "Xtrain, Xtest, ytrain, ytest = train_test_split(x_train, y_train,\n",
        "                                                 test_size=0.33, random_state=42)\n",
        "\n",
        "def BB_Model(sample_Xtrain,sample_ytrain):\n",
        "  model = RandomForestClassifier(n_estimators=100, random_state=0)\n",
        "  \n",
        "  model = RandomForestClassifier(n_estimators=1000)\n",
        "  model.fit(Xtrain, ytrain)\n",
        "  return model\n",
        "\n",
        "bb_model=BB_Model(Xtrain,ytrain)\n",
        "ypred = bb_model.predict(Xtest)\n",
        "accuracy_score(ytest, ypred)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7598425196850394"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgb_3lHA-Ena"
      },
      "source": [
        "# BUILD new generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xyrOPdW5-DpI",
        "outputId": "bd939cc5-3b52-4104-98ef-73db17522541"
      },
      "source": [
        "from tensorflow.keras.layers import concatenate\n",
        "from tensorflow.keras import activations\n",
        "\n",
        "#Given input of noise (latent) vector, the Generator produces an sample.\n",
        "def build_generator(noise_shape, desired_confidence_shape):\n",
        "\n",
        "    input_noise = Input(shape=noise_shape)\n",
        "    input_confidence = Input(shape=desired_confidence_shape)\n",
        "\n",
        "#Define your generator network \n",
        "#Here we are only using Dense layers. But network can be complicated based\n",
        "#on the application. For example, you can use VGG for super res. GAN.         \n",
        "\n",
        "    x = Dense(noise_shape[0])(input_noise)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    x = BatchNormalization(momentum=0.2)(x)\n",
        "    x = Dense(noise_shape[0]*noise_shape[0])(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    x = BatchNormalization(momentum=0.2)(x)\n",
        "    x = Dense(noise_shape[0]*noise_shape[0]*noise_shape[0])(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    x = BatchNormalization(momentum=0.2)(x)\n",
        "    x = Dense(np.prod(noise_shape[0]))(x)\n",
        "    x = activations.tanh(x)\n",
        "    x = Model(inputs=input_noise, outputs=x)\n",
        "    \n",
        "\n",
        "    y = Dense(desired_confidence_shape[0])(input_confidence)\n",
        "    y = LeakyReLU(alpha=0.2)(y)\n",
        "    y = BatchNormalization(momentum=0.2)(y)\n",
        "    y = Dense(desired_confidence_shape[0]*desired_confidence_shape[0])(y)\n",
        "    y = LeakyReLU(alpha=0.2)(y)\n",
        "    y = BatchNormalization(momentum=0.2)(y)\n",
        "    y = Model(inputs=input_confidence, outputs=y)\n",
        "\n",
        "    combined = concatenate([x.output, y.output])\n",
        "\n",
        "    z = Dense(2, activation=\"relu\")(combined)\n",
        "    z = Dense(1, activation=\"linear\")(z)\n",
        "    outputs=Dense(8)(z)\n",
        "\n",
        "    model = Model(inputs=(x.input, y.input), outputs=outputs)\n",
        "    model.summary()\n",
        "    \n",
        "    return model\n",
        "\n",
        "#Alpha — α is a hyperparameter which controls the underlying value to which the\n",
        "#function saturates negatives network inputs.\n",
        "#Momentum — Speed up the training\n",
        "noise_shape = (8,)\n",
        "desired_confidence_shape=(1,)\n",
        "generator = build_generator(noise_shape,desired_confidence_shape)\n",
        "generator.compile(loss='binary_crossentropy', optimizer=optimizer)\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_25\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_27 (InputLayer)           [(None, 8)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_62 (Dense)                (None, 8)            72          input_27[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_38 (LeakyReLU)      (None, 8)            0           dense_62[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_22 (BatchNo (None, 8)            32          leaky_re_lu_38[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dense_63 (Dense)                (None, 64)           576         batch_normalization_22[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_39 (LeakyReLU)      (None, 64)           0           dense_63[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "input_28 (InputLayer)           [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_23 (BatchNo (None, 64)           256         leaky_re_lu_39[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dense_66 (Dense)                (None, 1)            2           input_28[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_64 (Dense)                (None, 512)          33280       batch_normalization_23[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_41 (LeakyReLU)      (None, 1)            0           dense_66[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_40 (LeakyReLU)      (None, 512)          0           dense_64[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_25 (BatchNo (None, 1)            4           leaky_re_lu_41[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_24 (BatchNo (None, 512)          2048        leaky_re_lu_40[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dense_67 (Dense)                (None, 1)            2           batch_normalization_25[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dense_65 (Dense)                (None, 8)            4104        batch_normalization_24[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_42 (LeakyReLU)      (None, 1)            0           dense_67[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.tanh_2 (TFOpLambda)     (None, 8)            0           dense_65[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_26 (BatchNo (None, 1)            4           leaky_re_lu_42[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.concat_4 (TFOpLambda)        (None, 9)            0           tf.math.tanh_2[0][0]             \n",
            "                                                                 batch_normalization_26[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dense_68 (Dense)                (None, 2)            20          tf.concat_4[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_69 (Dense)                (None, 1)            3           dense_68[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_70 (Dense)                (None, 8)            16          dense_69[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 40,419\n",
            "Trainable params: 39,247\n",
            "Non-trainable params: 1,172\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vitSixKHneuR",
        "outputId": "022b8e01-baa1-4f28-aa02-0f874c156f11"
      },
      "source": [
        "\n",
        "#Given an input sample, the Discriminator outputs the likelihood of the sample being real.\n",
        "#Binary classification - true or false (we're calling it validity)\n",
        "\n",
        "#def build_discriminator(sample_shape, desired_confidence_shape, BB_output_shape):\n",
        "def build_discriminator(sample_shape, desired_confidence_shape,BB_output_shape):\n",
        "    \n",
        "    \n",
        "    input_sample = Input(shape=sample_shape)\n",
        "    input_confidence = Input(shape=desired_confidence_shape)\n",
        "    input_BB_output_shape = Input(shape=BB_output_shape)\n",
        "#Define your generator network \n",
        "#Here we are only using Dense layers. But network can be complicated based\n",
        "#on the application. For example, you can use VGG for super res. GAN.         \n",
        "\n",
        "    x = Dense(sample_shape[0])(input_sample)\n",
        "    x = Dense(sample_shape[0]*sample_shape[0]*sample_shape[0])(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    x = Dense(sample_shape[0]*sample_shape[0])(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    x = Model(inputs=input_sample, outputs=x)\n",
        "\n",
        "    \n",
        "\n",
        "    y = Dense(desired_confidence_shape[0])(input_confidence)\n",
        "    y = LeakyReLU(alpha=0.2)(y)\n",
        "    y = Dense(desired_confidence_shape[0]*desired_confidence_shape[0])(y)\n",
        "    y = LeakyReLU(alpha=0.2)(y)\n",
        "    y = Model(inputs=input_confidence, outputs=y)\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "    combined = concatenate([x.output,y.output])\n",
        "\n",
        "    o = Dense(2, activation=\"sigmoid\")(combined)\n",
        "    \n",
        "    model = Model(inputs=[x.input, y.input], outputs=o)\n",
        "    \n",
        "    validity = model([x.input,input_BB_output_shape])\n",
        "    model.summary()\n",
        "    res_model= Model(x.input,y.input, validity)\n",
        "    \n",
        "    return res_model\n",
        "\n",
        "#The validity is the Discriminator’s guess of input being real or not.\n",
        "noise_shape = (8,)\n",
        "desired_confidence_shape=(1,)\n",
        "BB_output_shape=(1,)\n",
        "discriminator = build_discriminator(noise_shape,desired_confidence_shape,BB_output_shape)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_90\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_85 (InputLayer)           [(None, 8)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_185 (Dense)               (None, 8)            72          input_85[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "input_86 (InputLayer)           [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_186 (Dense)               (None, 512)          4608        dense_185[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_188 (Dense)               (None, 1)            2           input_86[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_115 (LeakyReLU)     (None, 512)          0           dense_186[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_117 (LeakyReLU)     (None, 1)            0           dense_188[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_187 (Dense)               (None, 64)           32832       leaky_re_lu_115[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense_189 (Dense)               (None, 1)            2           leaky_re_lu_117[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_116 (LeakyReLU)     (None, 64)           0           dense_187[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_118 (LeakyReLU)     (None, 1)            0           dense_189[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf.concat_22 (TFOpLambda)       (None, 65)           0           leaky_re_lu_116[0][0]            \n",
            "                                                                 leaky_re_lu_118[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense_190 (Dense)               (None, 2)            132         tf.concat_22[0][0]               \n",
            "==================================================================================================\n",
            "Total params: 37,648\n",
            "Trainable params: 37,648\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjdHV0fI5VEY"
      },
      "source": [
        "#Now that we have constructed our two models it’s time to pit them against each other.\n",
        "#We do this by defining a training function, loading the data set, re-scaling our training\n",
        "#samples and setting the ground truths. \n",
        "def train(epochs, batch_size=128, save_interval=50):\n",
        "\n",
        "    half_batch = int(batch_size / 2)\n",
        "#We then loop through a number of epochs to train our Discriminator by first selecting\n",
        "#a random batch of samples from our true dataset, generating a set of samples from our\n",
        "#Generator, feeding both set of samples into our Discriminator, and finally setting the\n",
        "#loss parameters for both the real and fake samples, as well as the combined loss. \n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "\n",
        "      # ---------------------\n",
        "      #  Train Discriminator\n",
        "      # ---------------------\n",
        "      '''\n",
        "      # Select a random half batch of real samples\n",
        "\n",
        "      idx = np.random.randint(0, Xtrain.shape[0], half_batch)\n",
        "      diabetes = Xtrain.iloc[idx]\n",
        "\n",
        "      '''\n",
        "   \n",
        "\n",
        "      noise = np.random.normal(0, 1, (half_batch, 8))\n",
        "      scalar = np.random.normal(0, 1, (half_batch, 8))\n",
        "      # Generate a half batch of fake samples\n",
        "      gen_diabetes = generator.predict((noise,scalar))\n",
        "\n",
        "      # Train the discriminator on real and fake samples, separately\n",
        "      #Research showed that separate training is more effective. \n",
        "      d_loss_one = discriminator.train_on_batch(gen_diabetes, np.ones((half_batch, 1)))\n",
        "      d_loss_zero = discriminator.train_on_batch(gen_diabetes, np.zeros((half_batch, 1)))\n",
        "    #take average loss from real and fake samples. \n",
        "\n",
        "      d_loss = 0.5 * np.add(d_loss_real, d_loss_fake) \n",
        "      euclidean_distance = distance.euclidean(d_loss_real,d_loss_fake)\n",
        "      '''\n",
        "      if (euclidean_distance > 1.1):\n",
        "        print (\"real: \"+str(d_loss_real)+\" , fake: \"+str(d_loss_fake) + \" , Duclidean distance: \"+ str(euclidean_distance))\n",
        "        print ()\n",
        "        print(\"real diabetes values:\")\n",
        "        print(pd.DataFrame(diabetes))\n",
        "        print(\"fake diabetes values:\")\n",
        "        print(pd.DataFrame(gen_diabetes))\n",
        "      '''\n",
        "      \n",
        "      \n",
        "     \n",
        "        \n",
        "      \n",
        "    #And within the same loop we train our Generator, by setting the input noise and\n",
        "    #ultimately training the Generator to have the Discriminator label its samples as valid\n",
        "    #by specifying the gradient loss.\n",
        "      # ---------------------\n",
        "      #  Train Generator\n",
        "      # ---------------------\n",
        "    #Create noise vectors as input for generator. \n",
        "    #Create as many noise vectors as defined by the batch size. \n",
        "    #Based on normal distribution. Output will be of size (batch size, 100)\n",
        "      noise = np.random.normal(0, 1, (batch_size, 8)) \n",
        "\n",
        "      # The generator wants the discriminator to label the generated samples\n",
        "      # as valid (ones)\n",
        "      #This is where the genrator is trying to trick discriminator into believing\n",
        "      #the generated sample is true (hence value of 1 for y)\n",
        "      valid_y = np.array([1] * batch_size) #Creates an array of all ones of size=batch size\n",
        "\n",
        "      # Generator is part of combined where it got directly linked with the discriminator\n",
        "      # Train the generator with noise as x and 1 as y. \n",
        "      # Again, 1 as the output as it is adversarial and if generator did a great\n",
        "      #job of folling the discriminator then the output would be 1 (true)\n",
        "      g_loss = combined.train_on_batch(noise, valid_y)\n",
        "      if (g_loss > 1):\n",
        "        print(\"fake diabetes noise values that fooled the model:\")\n",
        "        print(pd.DataFrame(noise))\n",
        "\n",
        "    #Additionally, in order for us to keep track of our training process, we print the\n",
        "    #progress and save the sample sample output depending on the epoch interval specified.  \n",
        "    #Plot the progress\n",
        "      \n",
        "      print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
        "      \n"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-CBB7aUanPzz",
        "outputId": "a81f9bc3-5e2c-469a-8efc-b37fc1835983"
      },
      "source": [
        "optimizer = Adam(0.0002, 0.5)  #Learning rate and momentum.\n",
        "noise_shape = (8,)\n",
        "desired_confidence_shape=(1,)\n",
        "BB_output_shape=(1,)\n",
        "discriminator = build_discriminator(noise_shape,desired_confidence_shape,BB_output_shape)\n",
        "\n",
        "discriminator.compile(loss='binary_crossentropy',\n",
        "    optimizer=optimizer,\n",
        "    metrics=['accuracy'])\n",
        "\n",
        "#build and compile our Discriminator, pick the loss function\n",
        "\n",
        "#SInce we are only generating (faking) samples, let us not track any metrics.\n",
        "generator = build_generator(noise_shape,desired_confidence_shape)\n",
        "generator.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "\n",
        "##This builds the Generator and defines the input noise. \n",
        "#In a GAN the Generator network takes noise z as an input to produce its samples.  \n",
        "z = Input(shape=(8,)) \n",
        "c = Input(shape=(1,))  #Our random input to the generator\n",
        "test_if_diabete = generator([z,c])\n",
        "\n",
        "#This ensures that when we combine our networks we only train the Generator.\n",
        "#While generator training we do not want discriminator weights to be adjusted. \n",
        "#This Doesn't affect the above descriminator training.     \n",
        "discriminator.trainable = False  \n",
        "\n",
        "#This specifies that our Discriminator will take the samples generated by our Generator\n",
        "#and true dataset and set its output to a parameter called valid, which will indicate\n",
        "#whether the input is real or not. \n",
        " \n",
        "valid = discriminator([test_if_diabete,c])  #Validity check on the generated sample\n",
        "\n",
        "\n",
        "#Here we combined the models and also set our loss function and optimizer. \n",
        "#Again, we are only training the generator here. \n",
        "#The ultimate goal here is for the Generator to fool the Discriminator.  \n",
        "# The combined model  (stacked generator and discriminator) takes\n",
        "# noise as input => generates samples => determines validity\n",
        "\n",
        "combined = Model([z, valid])\n",
        "combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "\n",
        "\n",
        "train(epochs=150, batch_size=1000, save_interval=10)\n",
        "\n",
        "#Save model for future use to generate fake samples\n",
        "#Not tested yet... make sure right model is being saved..\n",
        "#Compare with GAN4\n",
        "\n",
        "generator.save('generator_model2.h5')  #Test the model on GAN4_predict...\n",
        "#Change epochs back to 30K\n",
        "                \n",
        "#Epochs dictate the number of backward and forward propagations, the batch_size\n",
        "#indicates the number of training samples per backward/forward propagation, and the\n",
        "#sample_interval specifies after how many epochs we call our sample function."
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_94\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_88 (InputLayer)           [(None, 8)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_191 (Dense)               (None, 8)            72          input_88[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "input_89 (InputLayer)           [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_192 (Dense)               (None, 512)          4608        dense_191[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_194 (Dense)               (None, 1)            2           input_89[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_119 (LeakyReLU)     (None, 512)          0           dense_192[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_121 (LeakyReLU)     (None, 1)            0           dense_194[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_193 (Dense)               (None, 64)           32832       leaky_re_lu_119[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense_195 (Dense)               (None, 1)            2           leaky_re_lu_121[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_120 (LeakyReLU)     (None, 64)           0           dense_193[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_122 (LeakyReLU)     (None, 1)            0           dense_195[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf.concat_23 (TFOpLambda)       (None, 65)           0           leaky_re_lu_120[0][0]            \n",
            "                                                                 leaky_re_lu_122[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense_196 (Dense)               (None, 2)            132         tf.concat_23[0][0]               \n",
            "==================================================================================================\n",
            "Total params: 37,648\n",
            "Trainable params: 37,648\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Model: \"model_98\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_91 (InputLayer)           [(None, 8)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_197 (Dense)               (None, 8)            72          input_91[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_123 (LeakyReLU)     (None, 8)            0           dense_197[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_47 (BatchNo (None, 8)            32          leaky_re_lu_123[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense_198 (Dense)               (None, 64)           576         batch_normalization_47[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_124 (LeakyReLU)     (None, 64)           0           dense_198[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "input_92 (InputLayer)           [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_48 (BatchNo (None, 64)           256         leaky_re_lu_124[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense_201 (Dense)               (None, 1)            2           input_92[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_199 (Dense)               (None, 512)          33280       batch_normalization_48[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_126 (LeakyReLU)     (None, 1)            0           dense_201[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_125 (LeakyReLU)     (None, 512)          0           dense_199[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_50 (BatchNo (None, 1)            4           leaky_re_lu_126[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_49 (BatchNo (None, 512)          2048        leaky_re_lu_125[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense_202 (Dense)               (None, 1)            2           batch_normalization_50[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dense_200 (Dense)               (None, 8)            4104        batch_normalization_49[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_127 (LeakyReLU)     (None, 1)            0           dense_202[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.tanh_7 (TFOpLambda)     (None, 8)            0           dense_200[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_51 (BatchNo (None, 1)            4           leaky_re_lu_127[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "tf.concat_24 (TFOpLambda)       (None, 9)            0           tf.math.tanh_7[0][0]             \n",
            "                                                                 batch_normalization_51[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dense_203 (Dense)               (None, 2)            20          tf.concat_24[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dense_204 (Dense)               (None, 1)            3           dense_203[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_205 (Dense)               (None, 8)            16          dense_204[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 40,419\n",
            "Trainable params: 39,247\n",
            "Non-trainable params: 1,172\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-55-74c6db2b39f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m#whether the input is real or not.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mvalid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_if_diabete\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#Validity check on the generated sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    944\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_in_functional_construction_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m       return self._functional_construction_call(inputs, args, kwargs,\n\u001b[0;32m--> 946\u001b[0;31m                                                 input_list)\n\u001b[0m\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;31m# Maintains info about the `Layer.call` stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_functional_construction_call\u001b[0;34m(self, inputs, args, kwargs, input_list)\u001b[0m\n\u001b[1;32m   1082\u001b[0m       \u001b[0;31m# Check input assumptions set after layer building, e.g. input shape.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m       outputs = self._keras_tensor_symbolic_call(\n\u001b[0;32m-> 1084\u001b[0;31m           inputs, input_masks, args, kwargs)\n\u001b[0m\u001b[1;32m   1085\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1086\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_keras_tensor_symbolic_call\u001b[0;34m(self, inputs, input_masks, args, kwargs)\u001b[0m\n\u001b[1;32m    814\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeras_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKerasTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_signature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    815\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 816\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_infer_output_signature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_infer_output_signature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_infer_output_signature\u001b[0;34m(self, inputs, args, kwargs, input_masks)\u001b[0m\n\u001b[1;32m    854\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m           \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    449\u001b[0m         \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtensors\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mthere\u001b[0m \u001b[0mare\u001b[0m \u001b[0mmore\u001b[0m \u001b[0mthan\u001b[0m \u001b[0mone\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \"\"\"\n\u001b[0;32m--> 451\u001b[0;31m     raise NotImplementedError('When subclassing the `Model` class, you should '\n\u001b[0m\u001b[1;32m    452\u001b[0m                               'implement a `call` method.')\n\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: When subclassing the `Model` class, you should implement a `call` method."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        },
        "id": "u3HksBpdF5JD",
        "outputId": "782723d3-82f4-4df1-bc48-ef4a07faf21a"
      },
      "source": [
        "half_batch = int(500 / 2)\n",
        "#We then loop through a number of epochs to train our Discriminator by first selecting\n",
        "#a random batch of samples from our true dataset, generating a set of samples from our\n",
        "#Generator, feeding both set of samples into our Discriminator, and finally setting the\n",
        "#loss parameters for both the real and fake samples, as well as the combined loss. \n",
        "    \n",
        "for epoch in range(50):\n",
        "\n",
        "  # ---------------------\n",
        "  #  Train Discriminator\n",
        "  # ---------------------\n",
        "  '''\n",
        "  # Select a random half batch of real samples\n",
        "\n",
        "  idx = np.random.randint(0, Xtrain.shape[0], half_batch)\n",
        "  diabetes = Xtrain.iloc[idx]\n",
        "\n",
        "  '''\n",
        "\n",
        "\n",
        "  noise = np.random.normal(0, 1, (half_batch, 8))\n",
        "  scalar = np.random.uniform(0, 1,(half_batch,1))\n",
        "  # Generate a half batch of fake samples\n",
        "  gen_diabetes = generator.predict((noise,scalar))\n",
        "  x_scaled = min_max_scaler.fit_transform(gen_diabetes)\n",
        "  Xtest = pd.DataFrame(x_scaled)\n",
        "  #Xtest=pd.DataFrame(gen_diabetes)\n",
        "  ypred=bb_model.predict(Xtest).reshape(half_batch,1)\n",
        "  # Train the discriminator on real and fake samples, separately\n",
        "  #Research showed that separate training is more effective. \n",
        "  zero_indexes=np.where(ypred == '0')[0].tolist()\n",
        "  one_indexes=np.where(ypred == '1')[0].tolist()\n",
        "\n",
        "  x_scaled_one=x_scaled[one_indexes]\n",
        "  x_scaled_zero=x_scaled[zero_indexes]\n",
        "  scalar_one = scalar[one_indexes]\n",
        "  scalar_zero = scalar[zero_indexes]\n",
        "  #Slizing \n",
        "  if (x_scaled_one.shape[0]>x_scaled_zero.shape[0]):\n",
        "    #Trimming\n",
        "    x_scaled_one=x_scaled_one[0:x_scaled_zero.shape[0]]\n",
        "    scalar_one=scalar_one[0:scalar_zero.shape[0]]\n",
        "  else:\n",
        "    #Trimming\n",
        "    x_scaled_zero=x_scaled_zero[0:x_scaled_one.shape[0]]\n",
        "    scalar_zero=scalar_zero[0:scalar_one.shape[0]]\n",
        "  \n",
        "\n",
        "  ###Xtest_zero\n",
        "  d_loss_one = discriminator.train_on_batch(x_scaled_one,scalar_one,np.ones((x_scaled_one.shape[0], 1)))\n",
        "  d_loss_one_y = discriminator.train_on_batch([x_scaled_one,np.ones((x_scaled_one.shape[0], 1))])\n",
        "  d_loss_zero = discriminator.train_on_batch(gen_diabetes,np.zeros((half_batch, 1)))\n",
        "#take average loss from real and fake samples. \n",
        "\n",
        "  d_loss = 0.5 * np.add(d_loss_real, d_loss_fake) \n",
        "  euclidean_distance = distance.euclidean(d_loss_real,d_loss_fake)\n",
        "  '''\n",
        "  if (euclidean_distance > 1.1):\n",
        "    print (\"real: \"+str(d_loss_real)+\" , fake: \"+str(d_loss_fake) + \" , Duclidean distance: \"+ str(euclidean_distance))\n",
        "    print ()\n",
        "    print(\"real diabetes values:\")\n",
        "    print(pd.DataFrame(diabetes))\n",
        "    print(\"fake diabetes values:\")\n",
        "    print(pd.DataFrame(gen_diabetes))\n",
        "  '''\n",
        "  \n",
        "  \n",
        " \n",
        "    \n",
        "  \n",
        "#And within the same loop we train our Generator, by setting the input noise and\n",
        "#ultimately training the Generator to have the Discriminator label its samples as valid\n",
        "#by specifying the gradient loss.\n",
        "  # ---------------------\n",
        "  #  Train Generator\n",
        "  # ---------------------\n",
        "#Create noise vectors as input for generator. \n",
        "#Create as many noise vectors as defined by the batch size. \n",
        "#Based on normal distribution. Output will be of size (batch size, 100)\n",
        "  noise = np.random.normal(0, 1, (batch_size, 8)) \n",
        "\n",
        "  # The generator wants the discriminator to label the generated samples\n",
        "  # as valid (ones)\n",
        "  #This is where the genrator is trying to trick discriminator into believing\n",
        "  #the generated sample is true (hence value of 1 for y)\n",
        "  valid_y = np.array([1] * batch_size) #Creates an array of all ones of size=batch size\n",
        "\n",
        "  # Generator is part of combined where it got directly linked with the discriminator\n",
        "  # Train the generator with noise as x and 1 as y. \n",
        "  # Again, 1 as the output as it is adversarial and if generator did a great\n",
        "  #job of folling the discriminator then the output would be 1 (true)\n",
        "  g_loss = combined.train_on_batch(noise, valid_y)\n",
        "  if (g_loss > 1):\n",
        "    print(\"fake diabetes noise values that fooled the model:\")\n",
        "    print(pd.DataFrame(noise))\n",
        "\n",
        "#Additionally, in order for us to keep track of our training process, we print the\n",
        "#progress and save the sample sample output depending on the epoch interval specified.  \n",
        "#Plot the progress\n",
        "  \n",
        "  print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-60-dd0620b755d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m   \u001b[0;31m###Xtest_zero\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m   \u001b[0md_loss_one\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_scaled_one\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscalar_one\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_scaled_one\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m   \u001b[0md_loss_one_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_scaled_one\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_scaled_one\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m   \u001b[0md_loss_zero\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_diabetes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhalf_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[1;32m   1798\u001b[0m                                                     class_weight)\n\u001b[1;32m   1799\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1800\u001b[0;31m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1801\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1802\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    922\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 924\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    925\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3020\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3021\u001b[0m       (graph_function,\n\u001b[0;32m-> 3022\u001b[0;31m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0m\u001b[1;32m   3023\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m   3024\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3439\u001b[0m               call_context_key in self._function_cache.missed):\n\u001b[1;32m   3440\u001b[0m             return self._define_function_with_shape_relaxation(\n\u001b[0;32m-> 3441\u001b[0;31m                 args, kwargs, flat_args, filtered_flat_args, cache_key_context)\n\u001b[0m\u001b[1;32m   3442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3443\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_define_function_with_shape_relaxation\u001b[0;34m(self, args, kwargs, flat_args, filtered_flat_args, cache_key_context)\u001b[0m\n\u001b[1;32m   3361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3362\u001b[0m     graph_function = self._create_graph_function(\n\u001b[0;32m-> 3363\u001b[0;31m         args, kwargs, override_flat_arg_shapes=relaxed_arg_shapes)\n\u001b[0m\u001b[1;32m   3364\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marg_relaxed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrank_only_cache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3287\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3288\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3289\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3290\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    997\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    998\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 999\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1000\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    673\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    984\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: in user code:\n\n    /usr/local/lib/python3.7/dist-packages/keras/engine/training.py:830 train_function  *\n        return step_function(self, iterator)\n    /usr/local/lib/python3.7/dist-packages/keras/engine/training.py:813 run_step  *\n        outputs = model.train_step(data)\n    /usr/local/lib/python3.7/dist-packages/keras/engine/training.py:770 train_step  *\n        y_pred = self(x, training=True)\n    /usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py:1006 __call__  *\n        outputs = call_fn(inputs, *args, **kwargs)\n    /usr/local/lib/python3.7/dist-packages/keras/engine/training.py:451 call  *\n        raise NotImplementedError('When subclassing the `Model` class, you should '\n\n    NotImplementedError: When subclassing the `Model` class, you should implement a `call` method.\n"
          ]
        }
      ]
    }
  ]
}