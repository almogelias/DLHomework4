{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DLHomework4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "history_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/almogelias/DLHomework4/blob/main/DLHomework4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCy8CL1baoyJ"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQOanPh61S3T",
        "outputId": "78d02e4d-3e46-47e2-9b13-1e877c44bf32"
      },
      "source": [
        "\n",
        "from keras.datasets import mnist\n",
        "from keras.layers import Input, Dense, Reshape, Flatten\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.models import Sequential, Model\n",
        "from keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy.spatial import distance\n",
        "\n",
        "!git clone -s https://github.com/almogelias/DLHomework4.git DLHomework4\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'DLHomework4'...\n",
            "remote: Enumerating objects: 7, done.\u001b[K\n",
            "remote: Counting objects: 100% (7/7), done.\u001b[K\n",
            "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
            "remote: Total 7 (delta 1), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (7/7), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmisa9Ktasvz"
      },
      "source": [
        "# Preprocess of diabetes.arff file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wm6DWTA1n9A"
      },
      "source": [
        "\n",
        "from scipy.io import arff\n",
        "import pandas as pd\n",
        "import os\n",
        "from sklearn import preprocessing\n",
        "\n",
        "\n",
        "#Load the data using \"arff.loadarff\" then convert it to dataframe\n",
        "\n",
        "repository_path = os.path.join(os.getcwd(), 'DLHomework4')\n",
        "train_diabetes_path = os.path.join(repository_path, 'diabetes.arff')\n",
        "\n",
        "data = arff.loadarff(train_diabetes_path)\n",
        "df = pd.DataFrame(data[0])\n",
        "\n",
        "y_train = df['class'].replace({b'tested_positive':'1', b'tested_negative':'0'})\n",
        "\n",
        "        \n",
        "# Drop last column of a dataframe\n",
        "x_train = df.iloc[: , :-1]\n",
        "min_max_scaler = preprocessing.MinMaxScaler()\n",
        "x_scaled = min_max_scaler.fit_transform(x_train)\n",
        "x_train = pd.DataFrame(x_scaled)\n",
        "#x_train, x_test, y_train, y_test = train_test_split(x, y,test_size=3)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PK7Tv9oz1hp7"
      },
      "source": [
        "#Define input dimensions\n",
        "diabetes_dim = x_train.shape[1]\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dSkANdxazYE"
      },
      "source": [
        "# Generator Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QEHK-wMh6mjf",
        "outputId": "efc18be1-6c3b-4444-adb8-4c9c942e373b"
      },
      "source": [
        "#Given input of noise (latent) vector, the Generator produces an sample.\n",
        "def build_generator():\n",
        "\n",
        "    noise_shape = (8,) #1D array of size 100 (latent vector / noise)\n",
        "\n",
        "#Define your generator network \n",
        "#Here we are only using Dense layers. But network can be complicated based\n",
        "#on the application. For example, you can use VGG for super res. GAN.         \n",
        "\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(diabetes_dim, input_shape=noise_shape))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(Dense(diabetes_dim*diabetes_dim))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(Dense(diabetes_dim*diabetes_dim*diabetes_dim))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    \n",
        "    model.add(Dense(np.prod(diabetes_dim), activation='tanh'))\n",
        "    #model.add(Reshape(diabetes_dim))\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    noise = Input(shape=noise_shape)\n",
        "    img = model(noise)    #Generated sample\n",
        "    \n",
        "    return Model(noise, img)\n",
        "\n",
        "#Alpha — α is a hyperparameter which controls the underlying value to which the\n",
        "#function saturates negatives network inputs.\n",
        "#Momentum — Speed up the training\n",
        "\n",
        "generator = build_generator()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "leaky_re_lu (LeakyReLU)      (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 64)                576       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)    (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 512)               33280     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_2 (LeakyReLU)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 8)                 4104      \n",
            "=================================================================\n",
            "Total params: 40,368\n",
            "Trainable params: 39,200\n",
            "Non-trainable params: 1,168\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOStgE9na2bM"
      },
      "source": [
        "#Discriminator Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQCE_DE_805i",
        "outputId": "6ba31a6a-4cd9-4f34-efa5-5f5ef5a7ee96"
      },
      "source": [
        "\n",
        "#Given an input sample, the Discriminator outputs the likelihood of the sample being real.\n",
        "#Binary classification - true or false (we're calling it validity)\n",
        "\n",
        "def build_discriminator():\n",
        "\n",
        "    diabetes_dim_shape = (diabetes_dim,)\n",
        "    model1 = Sequential()\n",
        "\n",
        "    model1.add(Dense(diabetes_dim, input_shape=diabetes_dim_shape))\n",
        "    model1.add(Dense(diabetes_dim*diabetes_dim*diabetes_dim))\n",
        "    model1.add(LeakyReLU(alpha=0.2))\n",
        "    model1.add(Dense(diabetes_dim*diabetes_dim))\n",
        "    model1.add(LeakyReLU(alpha=0.2))\n",
        "    model1.add(Dense(1, activation='sigmoid'))\n",
        "    model1.summary()\n",
        "\n",
        "    diabetes_input = Input(shape=diabetes_dim)\n",
        "    validity = model1(diabetes_input)\n",
        "\n",
        "    return Model(diabetes_input, validity)\n",
        "#The validity is the Discriminator’s guess of input being real or not.\n",
        "discriminator = build_discriminator()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_4 (Dense)              (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 512)               4608      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_3 (LeakyReLU)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 64)                32832     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_4 (LeakyReLU)    (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 37,577\n",
            "Trainable params: 37,577\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9nzYdKFa7o0"
      },
      "source": [
        "#Training flow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djqKY7Y2-y5m"
      },
      "source": [
        "#Now that we have constructed our two models it’s time to pit them against each other.\n",
        "#We do this by defining a training function, loading the data set, re-scaling our training\n",
        "#samples and setting the ground truths. \n",
        "def train(epochs, batch_size=128, save_interval=50):\n",
        "\n",
        "    half_batch = int(batch_size / 2)\n",
        "#We then loop through a number of epochs to train our Discriminator by first selecting\n",
        "#a random batch of samples from our true dataset, generating a set of samples from our\n",
        "#Generator, feeding both set of samples into our Discriminator, and finally setting the\n",
        "#loss parameters for both the real and fake samples, as well as the combined loss. \n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "\n",
        "      # ---------------------\n",
        "      #  Train Discriminator\n",
        "      # ---------------------\n",
        "      \n",
        "      # Select a random half batch of real samples\n",
        "      idx = np.random.randint(0, x_train.shape[0], half_batch)\n",
        "      diabetes = x_train.iloc[idx]\n",
        "\n",
        "\n",
        "      noise = np.random.normal(0, 1, (half_batch, 8))\n",
        "\n",
        "      # Generate a half batch of fake samples\n",
        "      gen_diabetes = generator.predict(noise)\n",
        "\n",
        "      # Train the discriminator on real and fake samples, separately\n",
        "      #Research showed that separate training is more effective. \n",
        "      d_loss_real = discriminator.train_on_batch(diabetes, np.ones((half_batch, 1)))\n",
        "      d_loss_fake = discriminator.train_on_batch(gen_diabetes, np.zeros((half_batch, 1)))\n",
        "    #take average loss from real and fake samples. \n",
        "\n",
        "      d_loss = 0.5 * np.add(d_loss_real, d_loss_fake) \n",
        "      euclidean_distance = distance.euclidean(d_loss_real,d_loss_fake)\n",
        "      '''\n",
        "      if (euclidean_distance > 1.1):\n",
        "        print (\"real: \"+str(d_loss_real)+\" , fake: \"+str(d_loss_fake) + \" , Duclidean distance: \"+ str(euclidean_distance))\n",
        "        print ()\n",
        "        print(\"real diabetes values:\")\n",
        "        print(pd.DataFrame(diabetes))\n",
        "        print(\"fake diabetes values:\")\n",
        "        print(pd.DataFrame(gen_diabetes))\n",
        "      '''\n",
        "      \n",
        "      \n",
        "     \n",
        "        \n",
        "      \n",
        "    #And within the same loop we train our Generator, by setting the input noise and\n",
        "    #ultimately training the Generator to have the Discriminator label its samples as valid\n",
        "    #by specifying the gradient loss.\n",
        "      # ---------------------\n",
        "      #  Train Generator\n",
        "      # ---------------------\n",
        "    #Create noise vectors as input for generator. \n",
        "    #Create as many noise vectors as defined by the batch size. \n",
        "    #Based on normal distribution. Output will be of size (batch size, 100)\n",
        "      noise = np.random.normal(0, 1, (batch_size, 8)) \n",
        "\n",
        "      # The generator wants the discriminator to label the generated samples\n",
        "      # as valid (ones)\n",
        "      #This is where the genrator is trying to trick discriminator into believing\n",
        "      #the generated sample is true (hence value of 1 for y)\n",
        "      valid_y = np.array([1] * batch_size) #Creates an array of all ones of size=batch size\n",
        "\n",
        "      # Generator is part of combined where it got directly linked with the discriminator\n",
        "      # Train the generator with noise as x and 1 as y. \n",
        "      # Again, 1 as the output as it is adversarial and if generator did a great\n",
        "      #job of folling the discriminator then the output would be 1 (true)\n",
        "      g_loss = combined.train_on_batch(noise, valid_y)\n",
        "      if (g_loss > 1):\n",
        "        print(\"fake diabetes noise values that fooled the model:\")\n",
        "        print(pd.DataFrame(noise))\n",
        "\n",
        "    #Additionally, in order for us to keep track of our training process, we print the\n",
        "    #progress and save the sample sample output depending on the epoch interval specified.  \n",
        "    #Plot the progress\n",
        "      \n",
        "      print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
        "      \n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3T_Mcdy1pBvB",
        "outputId": "a18ae1f6-33a8-4db2-dc1a-e68d11c5a319"
      },
      "source": [
        "optimizer = Adam(0.0002, 0.5)  #Learning rate and momentum.\n",
        "discriminator = build_discriminator()\n",
        "discriminator.compile(loss='binary_crossentropy',\n",
        "    optimizer=optimizer,\n",
        "    metrics=['accuracy'])\n",
        "\n",
        "#build and compile our Discriminator, pick the loss function\n",
        "\n",
        "#SInce we are only generating (faking) samples, let us not track any metrics.\n",
        "generator = build_generator()\n",
        "generator.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "\n",
        "##This builds the Generator and defines the input noise. \n",
        "#In a GAN the Generator network takes noise z as an input to produce its samples.  \n",
        "z = Input(shape=(8,))   #Our random input to the generator\n",
        "test_if_diabete = generator(z)\n",
        "\n",
        "#This ensures that when we combine our networks we only train the Generator.\n",
        "#While generator training we do not want discriminator weights to be adjusted. \n",
        "#This Doesn't affect the above descriminator training.     \n",
        "discriminator.trainable = False  \n",
        "\n",
        "#This specifies that our Discriminator will take the samples generated by our Generator\n",
        "#and true dataset and set its output to a parameter called valid, which will indicate\n",
        "#whether the input is real or not.  \n",
        "valid = discriminator(test_if_diabete)  #Validity check on the generated sample\n",
        "\n",
        "\n",
        "#Here we combined the models and also set our loss function and optimizer. \n",
        "#Again, we are only training the generator here. \n",
        "#The ultimate goal here is for the Generator to fool the Discriminator.  \n",
        "# The combined model  (stacked generator and discriminator) takes\n",
        "# noise as input => generates samples => determines validity\n",
        "\n",
        "combined = Model(z, valid)\n",
        "combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "\n",
        "\n",
        "train(epochs=150, batch_size=32, save_interval=10)\n",
        "\n",
        "#Save model for future use to generate fake samples\n",
        "#Not tested yet... make sure right model is being saved..\n",
        "#Compare with GAN4\n",
        "\n",
        "generator.save('generator_model.h5')  #Test the model on GAN4_predict...\n",
        "#Change epochs back to 30K\n",
        "                \n",
        "#Epochs dictate the number of backward and forward propagations, the batch_size\n",
        "#indicates the number of training samples per backward/forward propagation, and the\n",
        "#sample_interval specifies after how many epochs we call our sample function."
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_8 (Dense)              (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 512)               4608      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_5 (LeakyReLU)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 64)                32832     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_6 (LeakyReLU)    (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 37,577\n",
            "Trainable params: 37,577\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_12 (Dense)             (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_7 (LeakyReLU)    (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 64)                576       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_8 (LeakyReLU)    (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 512)               33280     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_9 (LeakyReLU)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 8)                 4104      \n",
            "=================================================================\n",
            "Total params: 40,368\n",
            "Trainable params: 39,200\n",
            "Non-trainable params: 1,168\n",
            "_________________________________________________________________\n",
            "0 [D loss: 0.688002, acc.: 62.50%] [G loss: 0.641846]\n",
            "1 [D loss: 0.684147, acc.: 53.12%] [G loss: 0.628238]\n",
            "2 [D loss: 0.680335, acc.: 53.12%] [G loss: 0.618364]\n",
            "3 [D loss: 0.685389, acc.: 56.25%] [G loss: 0.628313]\n",
            "4 [D loss: 0.676873, acc.: 53.12%] [G loss: 0.620073]\n",
            "5 [D loss: 0.679291, acc.: 59.38%] [G loss: 0.627378]\n",
            "6 [D loss: 0.672550, acc.: 56.25%] [G loss: 0.655611]\n",
            "7 [D loss: 0.670786, acc.: 56.25%] [G loss: 0.659712]\n",
            "8 [D loss: 0.673222, acc.: 59.38%] [G loss: 0.681883]\n",
            "9 [D loss: 0.661851, acc.: 65.62%] [G loss: 0.704573]\n",
            "10 [D loss: 0.652161, acc.: 71.88%] [G loss: 0.739474]\n",
            "11 [D loss: 0.652500, acc.: 71.88%] [G loss: 0.746191]\n",
            "12 [D loss: 0.630769, acc.: 81.25%] [G loss: 0.767912]\n",
            "13 [D loss: 0.626683, acc.: 78.12%] [G loss: 0.806128]\n",
            "14 [D loss: 0.579770, acc.: 93.75%] [G loss: 0.811546]\n",
            "15 [D loss: 0.584818, acc.: 90.62%] [G loss: 0.800028]\n",
            "16 [D loss: 0.578257, acc.: 90.62%] [G loss: 0.807707]\n",
            "17 [D loss: 0.591059, acc.: 87.50%] [G loss: 0.810247]\n",
            "18 [D loss: 0.572026, acc.: 84.38%] [G loss: 0.842741]\n",
            "19 [D loss: 0.587640, acc.: 87.50%] [G loss: 0.839265]\n",
            "20 [D loss: 0.590249, acc.: 78.12%] [G loss: 0.804437]\n",
            "21 [D loss: 0.587122, acc.: 81.25%] [G loss: 0.807448]\n",
            "22 [D loss: 0.567932, acc.: 87.50%] [G loss: 0.804786]\n",
            "23 [D loss: 0.567074, acc.: 78.12%] [G loss: 0.801541]\n",
            "24 [D loss: 0.658495, acc.: 59.38%] [G loss: 0.790113]\n",
            "25 [D loss: 0.600165, acc.: 78.12%] [G loss: 0.802870]\n",
            "26 [D loss: 0.610532, acc.: 81.25%] [G loss: 0.818309]\n",
            "27 [D loss: 0.644358, acc.: 62.50%] [G loss: 0.842634]\n",
            "28 [D loss: 0.640225, acc.: 65.62%] [G loss: 0.804985]\n",
            "29 [D loss: 0.617888, acc.: 75.00%] [G loss: 0.826968]\n",
            "30 [D loss: 0.632265, acc.: 75.00%] [G loss: 0.808944]\n",
            "31 [D loss: 0.592371, acc.: 87.50%] [G loss: 0.822750]\n",
            "32 [D loss: 0.616296, acc.: 78.12%] [G loss: 0.819220]\n",
            "33 [D loss: 0.580894, acc.: 81.25%] [G loss: 0.796372]\n",
            "34 [D loss: 0.648208, acc.: 65.62%] [G loss: 0.839357]\n",
            "35 [D loss: 0.639181, acc.: 71.88%] [G loss: 0.822738]\n",
            "36 [D loss: 0.640015, acc.: 71.88%] [G loss: 0.818082]\n",
            "37 [D loss: 0.620495, acc.: 81.25%] [G loss: 0.818858]\n",
            "38 [D loss: 0.632694, acc.: 78.12%] [G loss: 0.870073]\n",
            "39 [D loss: 0.630792, acc.: 75.00%] [G loss: 0.834535]\n",
            "40 [D loss: 0.652344, acc.: 71.88%] [G loss: 0.843402]\n",
            "41 [D loss: 0.648048, acc.: 81.25%] [G loss: 0.887688]\n",
            "42 [D loss: 0.611361, acc.: 68.75%] [G loss: 0.850094]\n",
            "43 [D loss: 0.638002, acc.: 75.00%] [G loss: 0.885015]\n",
            "44 [D loss: 0.579066, acc.: 90.62%] [G loss: 0.875337]\n",
            "45 [D loss: 0.599364, acc.: 81.25%] [G loss: 0.887124]\n",
            "46 [D loss: 0.614204, acc.: 78.12%] [G loss: 0.830303]\n",
            "47 [D loss: 0.638397, acc.: 78.12%] [G loss: 0.856636]\n",
            "48 [D loss: 0.676776, acc.: 59.38%] [G loss: 0.886501]\n",
            "49 [D loss: 0.598449, acc.: 71.88%] [G loss: 0.870017]\n",
            "50 [D loss: 0.629571, acc.: 68.75%] [G loss: 0.835600]\n",
            "51 [D loss: 0.604352, acc.: 84.38%] [G loss: 0.837902]\n",
            "52 [D loss: 0.625270, acc.: 71.88%] [G loss: 0.868562]\n",
            "53 [D loss: 0.575964, acc.: 81.25%] [G loss: 0.852385]\n",
            "54 [D loss: 0.631075, acc.: 68.75%] [G loss: 0.883053]\n",
            "55 [D loss: 0.651851, acc.: 65.62%] [G loss: 0.870670]\n",
            "56 [D loss: 0.612749, acc.: 75.00%] [G loss: 0.819046]\n",
            "57 [D loss: 0.577263, acc.: 78.12%] [G loss: 0.877272]\n",
            "58 [D loss: 0.647173, acc.: 65.62%] [G loss: 0.834515]\n",
            "59 [D loss: 0.636968, acc.: 75.00%] [G loss: 0.867697]\n",
            "60 [D loss: 0.686310, acc.: 53.12%] [G loss: 0.899948]\n",
            "61 [D loss: 0.620116, acc.: 75.00%] [G loss: 0.836857]\n",
            "62 [D loss: 0.653817, acc.: 59.38%] [G loss: 0.916554]\n",
            "63 [D loss: 0.609143, acc.: 78.12%] [G loss: 0.915387]\n",
            "64 [D loss: 0.582331, acc.: 87.50%] [G loss: 0.920714]\n",
            "65 [D loss: 0.573879, acc.: 81.25%] [G loss: 0.860429]\n",
            "66 [D loss: 0.598798, acc.: 78.12%] [G loss: 0.906053]\n",
            "67 [D loss: 0.584634, acc.: 84.38%] [G loss: 0.869980]\n",
            "68 [D loss: 0.581496, acc.: 81.25%] [G loss: 0.913560]\n",
            "69 [D loss: 0.597176, acc.: 78.12%] [G loss: 0.861946]\n",
            "70 [D loss: 0.612210, acc.: 78.12%] [G loss: 0.870950]\n",
            "71 [D loss: 0.584210, acc.: 78.12%] [G loss: 0.888635]\n",
            "72 [D loss: 0.629379, acc.: 75.00%] [G loss: 0.882199]\n",
            "73 [D loss: 0.670125, acc.: 59.38%] [G loss: 0.877591]\n",
            "74 [D loss: 0.619732, acc.: 75.00%] [G loss: 0.855293]\n",
            "75 [D loss: 0.601861, acc.: 71.88%] [G loss: 0.856143]\n",
            "76 [D loss: 0.608252, acc.: 81.25%] [G loss: 0.898063]\n",
            "77 [D loss: 0.645800, acc.: 68.75%] [G loss: 0.893049]\n",
            "78 [D loss: 0.581551, acc.: 71.88%] [G loss: 0.911928]\n",
            "79 [D loss: 0.630443, acc.: 65.62%] [G loss: 0.919767]\n",
            "80 [D loss: 0.613936, acc.: 65.62%] [G loss: 0.899731]\n",
            "81 [D loss: 0.560831, acc.: 78.12%] [G loss: 0.851575]\n",
            "82 [D loss: 0.672613, acc.: 50.00%] [G loss: 0.923864]\n",
            "83 [D loss: 0.651804, acc.: 62.50%] [G loss: 0.849548]\n",
            "84 [D loss: 0.630142, acc.: 71.88%] [G loss: 0.861225]\n",
            "85 [D loss: 0.633957, acc.: 68.75%] [G loss: 0.844197]\n",
            "86 [D loss: 0.649498, acc.: 65.62%] [G loss: 0.846957]\n",
            "87 [D loss: 0.651389, acc.: 65.62%] [G loss: 0.836203]\n",
            "88 [D loss: 0.666445, acc.: 62.50%] [G loss: 0.878883]\n",
            "89 [D loss: 0.647354, acc.: 71.88%] [G loss: 0.886594]\n",
            "90 [D loss: 0.679399, acc.: 59.38%] [G loss: 0.891845]\n",
            "91 [D loss: 0.588783, acc.: 78.12%] [G loss: 0.922178]\n",
            "92 [D loss: 0.639894, acc.: 62.50%] [G loss: 0.890724]\n",
            "93 [D loss: 0.604549, acc.: 68.75%] [G loss: 0.869748]\n",
            "94 [D loss: 0.626570, acc.: 78.12%] [G loss: 0.839848]\n",
            "95 [D loss: 0.614425, acc.: 78.12%] [G loss: 0.873815]\n",
            "96 [D loss: 0.639831, acc.: 68.75%] [G loss: 0.888456]\n",
            "97 [D loss: 0.616029, acc.: 68.75%] [G loss: 0.880215]\n",
            "98 [D loss: 0.614184, acc.: 71.88%] [G loss: 0.916667]\n",
            "99 [D loss: 0.614535, acc.: 78.12%] [G loss: 0.891972]\n",
            "100 [D loss: 0.620997, acc.: 75.00%] [G loss: 0.894275]\n",
            "101 [D loss: 0.643321, acc.: 71.88%] [G loss: 0.814564]\n",
            "102 [D loss: 0.612176, acc.: 75.00%] [G loss: 0.859201]\n",
            "103 [D loss: 0.624255, acc.: 65.62%] [G loss: 0.862639]\n",
            "104 [D loss: 0.646341, acc.: 71.88%] [G loss: 0.826830]\n",
            "105 [D loss: 0.642894, acc.: 71.88%] [G loss: 0.901912]\n",
            "106 [D loss: 0.620021, acc.: 65.62%] [G loss: 0.886252]\n",
            "107 [D loss: 0.679862, acc.: 65.62%] [G loss: 0.836528]\n",
            "108 [D loss: 0.654278, acc.: 65.62%] [G loss: 0.788531]\n",
            "109 [D loss: 0.637436, acc.: 68.75%] [G loss: 0.828111]\n",
            "110 [D loss: 0.604313, acc.: 81.25%] [G loss: 0.867351]\n",
            "111 [D loss: 0.626680, acc.: 87.50%] [G loss: 0.901971]\n",
            "112 [D loss: 0.669663, acc.: 56.25%] [G loss: 0.839975]\n",
            "113 [D loss: 0.672353, acc.: 65.62%] [G loss: 0.813667]\n",
            "114 [D loss: 0.655608, acc.: 62.50%] [G loss: 0.832302]\n",
            "115 [D loss: 0.640003, acc.: 68.75%] [G loss: 0.864490]\n",
            "116 [D loss: 0.674017, acc.: 59.38%] [G loss: 0.825861]\n",
            "117 [D loss: 0.640397, acc.: 65.62%] [G loss: 0.895985]\n",
            "118 [D loss: 0.628202, acc.: 81.25%] [G loss: 0.931977]\n",
            "119 [D loss: 0.649599, acc.: 65.62%] [G loss: 0.870244]\n",
            "120 [D loss: 0.656458, acc.: 71.88%] [G loss: 0.874569]\n",
            "121 [D loss: 0.602131, acc.: 78.12%] [G loss: 0.846554]\n",
            "122 [D loss: 0.672971, acc.: 62.50%] [G loss: 0.860959]\n",
            "123 [D loss: 0.623990, acc.: 68.75%] [G loss: 0.819053]\n",
            "124 [D loss: 0.632098, acc.: 62.50%] [G loss: 0.863704]\n",
            "125 [D loss: 0.644026, acc.: 59.38%] [G loss: 0.809462]\n",
            "126 [D loss: 0.619022, acc.: 75.00%] [G loss: 0.861744]\n",
            "127 [D loss: 0.649827, acc.: 65.62%] [G loss: 0.784856]\n",
            "128 [D loss: 0.663119, acc.: 65.62%] [G loss: 0.785248]\n",
            "129 [D loss: 0.697525, acc.: 46.88%] [G loss: 0.798861]\n",
            "130 [D loss: 0.671686, acc.: 65.62%] [G loss: 0.803863]\n",
            "131 [D loss: 0.666831, acc.: 56.25%] [G loss: 0.832640]\n",
            "132 [D loss: 0.617229, acc.: 75.00%] [G loss: 0.858019]\n",
            "133 [D loss: 0.644699, acc.: 78.12%] [G loss: 0.813957]\n",
            "134 [D loss: 0.672779, acc.: 62.50%] [G loss: 0.847099]\n",
            "135 [D loss: 0.655194, acc.: 75.00%] [G loss: 0.841881]\n",
            "136 [D loss: 0.644775, acc.: 71.88%] [G loss: 0.832125]\n",
            "137 [D loss: 0.640718, acc.: 62.50%] [G loss: 0.853803]\n",
            "138 [D loss: 0.693439, acc.: 50.00%] [G loss: 0.830988]\n",
            "139 [D loss: 0.657260, acc.: 50.00%] [G loss: 0.830612]\n",
            "140 [D loss: 0.666646, acc.: 59.38%] [G loss: 0.807951]\n",
            "141 [D loss: 0.622092, acc.: 65.62%] [G loss: 0.796499]\n",
            "142 [D loss: 0.637728, acc.: 59.38%] [G loss: 0.780771]\n",
            "143 [D loss: 0.671030, acc.: 43.75%] [G loss: 0.837640]\n",
            "144 [D loss: 0.698878, acc.: 56.25%] [G loss: 0.809879]\n",
            "145 [D loss: 0.644411, acc.: 81.25%] [G loss: 0.906529]\n",
            "146 [D loss: 0.636762, acc.: 71.88%] [G loss: 0.821219]\n",
            "147 [D loss: 0.665361, acc.: 65.62%] [G loss: 0.814886]\n",
            "148 [D loss: 0.666584, acc.: 65.62%] [G loss: 0.842296]\n",
            "149 [D loss: 0.679125, acc.: 62.50%] [G loss: 0.806793]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bHU4fDX1_vC"
      },
      "source": [
        "# German Credit DB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "id": "UmmiPqzE1_Qj",
        "outputId": "150e38fc-44fa-4308-e4eb-d5ec6fbd2cdb"
      },
      "source": [
        "'''\n",
        "\n",
        "#Load the data using \"arff.loadarff\" then convert it to dataframe\n",
        "\n",
        "repository_path = os.path.join(os.getcwd(), 'DLHomework4')\n",
        "train_german_credit_path = os.path.join(repository_path, 'german_credit.arff')\n",
        "\n",
        "data = arff.loadarff(train_german_credit_path)\n",
        "df = pd.DataFrame(data[0])\n",
        "\n",
        "y_train = df['21'].replace({b'1':'1', b'2':'2'})\n",
        "\n",
        "        \n",
        "# Drop last column of a dataframe\n",
        "x_train = df.iloc[: , :-1]\n",
        "min_max_scaler = preprocessing.MinMaxScaler()\n",
        "x_scaled = min_max_scaler.fit_transform(x_train)\n",
        "x_train = pd.DataFrame(x_scaled)\n",
        "#x_train, x_test, y_train, y_test = train_test_split(x, y,test_size=3)\n",
        "'''"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n\\n#Load the data using \"arff.loadarff\" then convert it to dataframe\\n\\nrepository_path = os.path.join(os.getcwd(), \\'DLHomework4\\')\\ntrain_german_credit_path = os.path.join(repository_path, \\'german_credit.arff\\')\\n\\ndata = arff.loadarff(train_german_credit_path)\\ndf = pd.DataFrame(data[0])\\n\\ny_train = df[\\'21\\'].replace({b\\'1\\':\\'1\\', b\\'2\\':\\'2\\'})\\n\\n        \\n# Drop last column of a dataframe\\nx_train = df.iloc[: , :-1]\\nmin_max_scaler = preprocessing.MinMaxScaler()\\nx_scaled = min_max_scaler.fit_transform(x_train)\\nx_train = pd.DataFrame(x_scaled)\\n#x_train, x_test, y_train, y_test = train_test_split(x, y,test_size=3)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODzEaMEkbcFs"
      },
      "source": [
        "# Random Forest model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DjteC-kSbfnk",
        "outputId": "c250b964-9ab7-4f88-9da5-46ae926712e6"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "Xtrain, Xtest, ytrain, ytest = train_test_split(x_train, y_train,\n",
        "                                                 test_size=0.33, random_state=42)\n",
        "\n",
        "def BB_Model(sample_Xtrain,sample_ytrain):\n",
        "  model = RandomForestClassifier(n_estimators=100, random_state=0)\n",
        "  \n",
        "  model = RandomForestClassifier(n_estimators=1000)\n",
        "  model.fit(Xtrain, ytrain)\n",
        "  return model\n",
        "\n",
        "model=BB_Model(Xtrain,ytrain)\n",
        "ypred = model.predict(Xtest)\n",
        "accuracy_score(ytest, ypred)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7637795275590551"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgb_3lHA-Ena"
      },
      "source": [
        "# BUILD new generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xyrOPdW5-DpI",
        "outputId": "3f9fa043-b7f0-4199-a026-2b66ec93de77"
      },
      "source": [
        "from tensorflow.keras.layers import concatenate\n",
        "from tensorflow.keras import activations\n",
        "\n",
        "#Given input of noise (latent) vector, the Generator produces an sample.\n",
        "def build_generator(noise_shape, desired_confidence_shape):\n",
        "\n",
        "    input_noise = Input(shape=noise_shape)\n",
        "    input_confidence = Input(shape=desired_confidence_shape)\n",
        "\n",
        "#Define your generator network \n",
        "#Here we are only using Dense layers. But network can be complicated based\n",
        "#on the application. For example, you can use VGG for super res. GAN.         \n",
        "\n",
        "    x = Dense(noise_shape[0])(input_noise)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    x = BatchNormalization(momentum=0.2)(x)\n",
        "    x = Dense(noise_shape[0]*noise_shape[0])(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    x = BatchNormalization(momentum=0.2)(x)\n",
        "    x = Dense(noise_shape[0]*noise_shape[0]*noise_shape[0])(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    x = BatchNormalization(momentum=0.2)(x)\n",
        "    x = Dense(np.prod(noise_shape[0]))(x)\n",
        "    x = activations.tanh(x)\n",
        "    x = Model(inputs=input_noise, outputs=x)\n",
        "    \n",
        "\n",
        "    y = Dense(desired_confidence_shape[0])(input_confidence)\n",
        "    y = LeakyReLU(alpha=0.2)(y)\n",
        "    y = BatchNormalization(momentum=0.2)(y)\n",
        "    y = Dense(desired_confidence_shape[0]*desired_confidence_shape[0])(y)\n",
        "    y = LeakyReLU(alpha=0.2)(y)\n",
        "    y = BatchNormalization(momentum=0.2)(y)\n",
        "    y = Model(inputs=input_confidence, outputs=y)\n",
        "\n",
        "    combined = concatenate([x.output, y.output])\n",
        "\n",
        "    z = Dense(2, activation=\"relu\")(combined)\n",
        "    z = Dense(1, activation=\"linear\")(z)\n",
        "    outputs=Dense(8)(z)\n",
        "\n",
        "    model = Model(inputs=[x.input, y.input], outputs=outputs)\n",
        "    model.summary()\n",
        "    \n",
        "    return model\n",
        "\n",
        "#Alpha — α is a hyperparameter which controls the underlying value to which the\n",
        "#function saturates negatives network inputs.\n",
        "#Momentum — Speed up the training\n",
        "noise_shape = (8,)\n",
        "desired_confidence_shape=(1,)\n",
        "generator = build_generator(noise_shape,desired_confidence_shape)\n",
        "generator.compile(loss='binary_crossentropy', optimizer=optimizer)\n"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_133\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_119 (InputLayer)          [(None, 8)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_318 (Dense)               (None, 8)            72          input_119[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_194 (LeakyReLU)     (None, 8)            0           dense_318[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_124 (BatchN (None, 8)            32          leaky_re_lu_194[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense_319 (Dense)               (None, 64)           576         batch_normalization_124[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_195 (LeakyReLU)     (None, 64)           0           dense_319[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "input_120 (InputLayer)          [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_125 (BatchN (None, 64)           256         leaky_re_lu_195[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense_322 (Dense)               (None, 1)            2           input_120[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_320 (Dense)               (None, 512)          33280       batch_normalization_125[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_197 (LeakyReLU)     (None, 1)            0           dense_322[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_196 (LeakyReLU)     (None, 512)          0           dense_320[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_127 (BatchN (None, 1)            4           leaky_re_lu_197[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_126 (BatchN (None, 512)          2048        leaky_re_lu_196[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense_323 (Dense)               (None, 1)            2           batch_normalization_127[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dense_321 (Dense)               (None, 8)            4104        batch_normalization_126[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_198 (LeakyReLU)     (None, 1)            0           dense_323[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.tanh_13 (TFOpLambda)    (None, 8)            0           dense_321[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_128 (BatchN (None, 1)            4           leaky_re_lu_198[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "tf.concat_42 (TFOpLambda)       (None, 9)            0           tf.math.tanh_13[0][0]            \n",
            "                                                                 batch_normalization_128[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dense_324 (Dense)               (None, 2)            20          tf.concat_42[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dense_325 (Dense)               (None, 1)            3           dense_324[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_326 (Dense)               (None, 8)            16          dense_325[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 40,419\n",
            "Trainable params: 39,247\n",
            "Non-trainable params: 1,172\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vitSixKHneuR",
        "outputId": "83f4812a-88a2-4e66-92f4-1fa337816016"
      },
      "source": [
        "\n",
        "#Given an input sample, the Discriminator outputs the likelihood of the sample being real.\n",
        "#Binary classification - true or false (we're calling it validity)\n",
        "\n",
        "def build_discriminator(sample_shape, desired_confidence_shape):\n",
        "\n",
        "    \n",
        "    \n",
        "    if (type(sample_shape)=='keras.engine.keras_tensor.KerasTensor'):\n",
        "\n",
        "      x = Dense(sample_shape.shape)(sample_shape)\n",
        "      x = Dense(sample_shape.shape*sample_shape.shape*sample_shape.shape)(x)\n",
        "      x = LeakyReLU(alpha=0.2)(x)\n",
        "      x = Dense(sample_shape.shape*sample_shape.shape)(x)\n",
        "      x = LeakyReLU(alpha=0.2)(x)\n",
        "      x = Model(inputs=input_sample, outputs=x)\n",
        "\n",
        "      y = Dense(desired_confidence_shape.shape)(desired_confidence_shape)\n",
        "      y = LeakyReLU(alpha=0.2)(y)\n",
        "      y = Dense(desired_confidence_shape.shape*desired_confidence_shape.shape)(y)\n",
        "      y = LeakyReLU(alpha=0.2)(y)\n",
        "      y = Model(inputs=input_confidence, outputs=y)\n",
        "\n",
        "      combined = concatenate([x.output, y.output])\n",
        "\n",
        "      z = Dense(2, activation=\"sigmoid\")(combined)\n",
        "\n",
        "      model = Model(inputs=[x.input, y.input], outputs=z)\n",
        "      model.summary()\n",
        "      \n",
        "      return model\n",
        "\n",
        "    else:\n",
        "      input_sample = Input(shape=sample_shape)\n",
        "      input_confidence = Input(shape=desired_confidence_shape)\n",
        "#Define your generator network \n",
        "#Here we are only using Dense layers. But network can be complicated based\n",
        "#on the application. For example, you can use VGG for super res. GAN.         \n",
        "\n",
        "      x = Dense(sample_shape[0])(input_sample)\n",
        "      x = Dense(sample_shape[0]*sample_shape[0]*sample_shape[0])(x)\n",
        "      x = LeakyReLU(alpha=0.2)(x)\n",
        "      x = Dense(sample_shape[0]*sample_shape[0])(x)\n",
        "      x = LeakyReLU(alpha=0.2)(x)\n",
        "      x = Model(inputs=input_sample, outputs=x)\n",
        "\n",
        "      y = Dense(desired_confidence_shape[0])(input_confidence)\n",
        "      y = LeakyReLU(alpha=0.2)(y)\n",
        "      y = Dense(desired_confidence_shape[0]*desired_confidence_shape[0])(y)\n",
        "      y = LeakyReLU(alpha=0.2)(y)\n",
        "      y = Model(inputs=input_confidence, outputs=y)\n",
        "\n",
        "      combined = concatenate([x.output, y.output])\n",
        "\n",
        "      z = Dense(2, activation=\"sigmoid\")(combined)\n",
        "\n",
        "      model = Model(inputs=[x.input, y.input], outputs=z)\n",
        "      model.summary()\n",
        "      \n",
        "      return model\n",
        "\n",
        "    #return Model(diabetes_input, validity)\n",
        "#The validity is the Discriminator’s guess of input being real or not.\n",
        "noise_shape = (8,)\n",
        "desired_confidence_shape=(1,)\n",
        "discriminator = build_discriminator(noise_shape,desired_confidence_shape)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_145\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_129 (InputLayer)          [(None, 8)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_348 (Dense)               (None, 8)            72          input_129[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "input_130 (InputLayer)          [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_349 (Dense)               (None, 512)          4608        dense_348[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_351 (Dense)               (None, 1)            2           input_130[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_212 (LeakyReLU)     (None, 512)          0           dense_349[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_214 (LeakyReLU)     (None, 1)            0           dense_351[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_350 (Dense)               (None, 64)           32832       leaky_re_lu_212[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense_352 (Dense)               (None, 1)            2           leaky_re_lu_214[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_213 (LeakyReLU)     (None, 64)           0           dense_350[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_215 (LeakyReLU)     (None, 1)            0           dense_352[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf.concat_46 (TFOpLambda)       (None, 65)           0           leaky_re_lu_213[0][0]            \n",
            "                                                                 leaky_re_lu_215[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense_353 (Dense)               (None, 2)            132         tf.concat_46[0][0]               \n",
            "==================================================================================================\n",
            "Total params: 37,648\n",
            "Trainable params: 37,648\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjdHV0fI5VEY"
      },
      "source": [
        "#Now that we have constructed our two models it’s time to pit them against each other.\n",
        "#We do this by defining a training function, loading the data set, re-scaling our training\n",
        "#samples and setting the ground truths. \n",
        "def train(epochs, batch_size=128, save_interval=50):\n",
        "\n",
        "    half_batch = int(batch_size / 2)\n",
        "#We then loop through a number of epochs to train our Discriminator by first selecting\n",
        "#a random batch of samples from our true dataset, generating a set of samples from our\n",
        "#Generator, feeding both set of samples into our Discriminator, and finally setting the\n",
        "#loss parameters for both the real and fake samples, as well as the combined loss. \n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "\n",
        "      # ---------------------\n",
        "      #  Train Discriminator\n",
        "      # ---------------------\n",
        "      '''\n",
        "      # Select a random half batch of real samples\n",
        "\n",
        "      idx = np.random.randint(0, Xtrain.shape[0], half_batch)\n",
        "      diabetes = Xtrain.iloc[idx]\n",
        "\n",
        "      '''\n",
        "   \n",
        "\n",
        "      noise = np.random.normal(0, 1, (half_batch, 8))\n",
        "      scalar = np.random.normal(0, 1, (half_batch, 8))\n",
        "      # Generate a half batch of fake samples\n",
        "      gen_diabetes = generator.predict(noise,scalar)\n",
        "\n",
        "      # Train the discriminator on real and fake samples, separately\n",
        "      #Research showed that separate training is more effective. \n",
        "      d_loss_one = discriminator.train_on_batch(gen_diabetes, np.ones((half_batch, 1)))\n",
        "      d_loss_zero = discriminator.train_on_batch(gen_diabetes, np.zeros((half_batch, 1)))\n",
        "    #take average loss from real and fake samples. \n",
        "\n",
        "      d_loss = 0.5 * np.add(d_loss_real, d_loss_fake) \n",
        "      euclidean_distance = distance.euclidean(d_loss_real,d_loss_fake)\n",
        "      '''\n",
        "      if (euclidean_distance > 1.1):\n",
        "        print (\"real: \"+str(d_loss_real)+\" , fake: \"+str(d_loss_fake) + \" , Duclidean distance: \"+ str(euclidean_distance))\n",
        "        print ()\n",
        "        print(\"real diabetes values:\")\n",
        "        print(pd.DataFrame(diabetes))\n",
        "        print(\"fake diabetes values:\")\n",
        "        print(pd.DataFrame(gen_diabetes))\n",
        "      '''\n",
        "      \n",
        "      \n",
        "     \n",
        "        \n",
        "      \n",
        "    #And within the same loop we train our Generator, by setting the input noise and\n",
        "    #ultimately training the Generator to have the Discriminator label its samples as valid\n",
        "    #by specifying the gradient loss.\n",
        "      # ---------------------\n",
        "      #  Train Generator\n",
        "      # ---------------------\n",
        "    #Create noise vectors as input for generator. \n",
        "    #Create as many noise vectors as defined by the batch size. \n",
        "    #Based on normal distribution. Output will be of size (batch size, 100)\n",
        "      noise = np.random.normal(0, 1, (batch_size, 8)) \n",
        "\n",
        "      # The generator wants the discriminator to label the generated samples\n",
        "      # as valid (ones)\n",
        "      #This is where the genrator is trying to trick discriminator into believing\n",
        "      #the generated sample is true (hence value of 1 for y)\n",
        "      valid_y = np.array([1] * batch_size) #Creates an array of all ones of size=batch size\n",
        "\n",
        "      # Generator is part of combined where it got directly linked with the discriminator\n",
        "      # Train the generator with noise as x and 1 as y. \n",
        "      # Again, 1 as the output as it is adversarial and if generator did a great\n",
        "      #job of folling the discriminator then the output would be 1 (true)\n",
        "      g_loss = combined.train_on_batch(noise, valid_y)\n",
        "      if (g_loss > 1):\n",
        "        print(\"fake diabetes noise values that fooled the model:\")\n",
        "        print(pd.DataFrame(noise))\n",
        "\n",
        "    #Additionally, in order for us to keep track of our training process, we print the\n",
        "    #progress and save the sample sample output depending on the epoch interval specified.  \n",
        "    #Plot the progress\n",
        "      \n",
        "      print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
        "      \n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-CBB7aUanPzz",
        "outputId": "862a0bdb-d21c-4cbf-a224-6e9612374dc9"
      },
      "source": [
        "optimizer = Adam(0.0002, 0.5)  #Learning rate and momentum.\n",
        "noise_shape = (8,)\n",
        "desired_confidence_shape=(1,)\n",
        "discriminator = build_discriminator(noise_shape,desired_confidence_shape)\n",
        "discriminator.compile(loss='binary_crossentropy',\n",
        "    optimizer=optimizer,\n",
        "    metrics=['accuracy'])\n",
        "\n",
        "#build and compile our Discriminator, pick the loss function\n",
        "\n",
        "#SInce we are only generating (faking) samples, let us not track any metrics.\n",
        "generator = build_generator(noise_shape,desired_confidence_shape)\n",
        "generator.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "\n",
        "##This builds the Generator and defines the input noise. \n",
        "#In a GAN the Generator network takes noise z as an input to produce its samples.  \n",
        "z = Input(shape=(8,)) \n",
        "c = Input(shape=(1,))  #Our random input to the generator\n",
        "test_if_diabete = generator([z,c])\n",
        "\n",
        "#This ensures that when we combine our networks we only train the Generator.\n",
        "#While generator training we do not want discriminator weights to be adjusted. \n",
        "#This Doesn't affect the above descriminator training.     \n",
        "discriminator.trainable = False  \n",
        "\n",
        "#This specifies that our Discriminator will take the samples generated by our Generator\n",
        "#and true dataset and set its output to a parameter called valid, which will indicate\n",
        "#whether the input is real or not.  \n",
        "valid = discriminator(test_if_diabete,c)  #Validity check on the generated sample\n",
        "\n",
        "\n",
        "#Here we combined the models and also set our loss function and optimizer. \n",
        "#Again, we are only training the generator here. \n",
        "#The ultimate goal here is for the Generator to fool the Discriminator.  \n",
        "# The combined model  (stacked generator and discriminator) takes\n",
        "# noise as input => generates samples => determines validity\n",
        "\n",
        "combined = Model(z, valid)\n",
        "combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "\n",
        "\n",
        "train(epochs=150, batch_size=32, save_interval=10)\n",
        "\n",
        "#Save model for future use to generate fake samples\n",
        "#Not tested yet... make sure right model is being saved..\n",
        "#Compare with GAN4\n",
        "\n",
        "generator.save('generator_model2.h5')  #Test the model on GAN4_predict...\n",
        "#Change epochs back to 30K\n",
        "                \n",
        "#Epochs dictate the number of backward and forward propagations, the batch_size\n",
        "#indicates the number of training samples per backward/forward propagation, and the\n",
        "#sample_interval specifies after how many epochs we call our sample function."
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_148\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_131 (InputLayer)          [(None, 8)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_354 (Dense)               (None, 8)            72          input_131[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "input_132 (InputLayer)          [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_355 (Dense)               (None, 512)          4608        dense_354[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_357 (Dense)               (None, 1)            2           input_132[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_216 (LeakyReLU)     (None, 512)          0           dense_355[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_218 (LeakyReLU)     (None, 1)            0           dense_357[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_356 (Dense)               (None, 64)           32832       leaky_re_lu_216[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense_358 (Dense)               (None, 1)            2           leaky_re_lu_218[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_217 (LeakyReLU)     (None, 64)           0           dense_356[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_219 (LeakyReLU)     (None, 1)            0           dense_358[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf.concat_47 (TFOpLambda)       (None, 65)           0           leaky_re_lu_217[0][0]            \n",
            "                                                                 leaky_re_lu_219[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense_359 (Dense)               (None, 2)            132         tf.concat_47[0][0]               \n",
            "==================================================================================================\n",
            "Total params: 37,648\n",
            "Trainable params: 37,648\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Model: \"model_151\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_133 (InputLayer)          [(None, 8)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_360 (Dense)               (None, 8)            72          input_133[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_220 (LeakyReLU)     (None, 8)            0           dense_360[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_134 (BatchN (None, 8)            32          leaky_re_lu_220[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense_361 (Dense)               (None, 64)           576         batch_normalization_134[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_221 (LeakyReLU)     (None, 64)           0           dense_361[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "input_134 (InputLayer)          [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_135 (BatchN (None, 64)           256         leaky_re_lu_221[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense_364 (Dense)               (None, 1)            2           input_134[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_362 (Dense)               (None, 512)          33280       batch_normalization_135[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_223 (LeakyReLU)     (None, 1)            0           dense_364[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_222 (LeakyReLU)     (None, 512)          0           dense_362[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_137 (BatchN (None, 1)            4           leaky_re_lu_223[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_136 (BatchN (None, 512)          2048        leaky_re_lu_222[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense_365 (Dense)               (None, 1)            2           batch_normalization_137[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dense_363 (Dense)               (None, 8)            4104        batch_normalization_136[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_224 (LeakyReLU)     (None, 1)            0           dense_365[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.tanh_15 (TFOpLambda)    (None, 8)            0           dense_363[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_138 (BatchN (None, 1)            4           leaky_re_lu_224[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "tf.concat_48 (TFOpLambda)       (None, 9)            0           tf.math.tanh_15[0][0]            \n",
            "                                                                 batch_normalization_138[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dense_366 (Dense)               (None, 2)            20          tf.concat_48[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dense_367 (Dense)               (None, 1)            3           dense_366[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_368 (Dense)               (None, 8)            16          dense_367[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 40,419\n",
            "Trainable params: 39,247\n",
            "Non-trainable params: 1,172\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-74-c49c500c6a55>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m#and true dataset and set its output to a parameter called valid, which will indicate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m#whether the input is real or not.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mvalid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_if_diabete\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#Validity check on the generated sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    944\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_in_functional_construction_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m       return self._functional_construction_call(inputs, args, kwargs,\n\u001b[0;32m--> 946\u001b[0;31m                                                 input_list)\n\u001b[0m\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;31m# Maintains info about the `Layer.call` stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_functional_construction_call\u001b[0;34m(self, inputs, args, kwargs, input_list)\u001b[0m\n\u001b[1;32m   1082\u001b[0m       \u001b[0;31m# Check input assumptions set after layer building, e.g. input shape.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m       outputs = self._keras_tensor_symbolic_call(\n\u001b[0;32m-> 1084\u001b[0;31m           inputs, input_masks, args, kwargs)\n\u001b[0m\u001b[1;32m   1085\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1086\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_keras_tensor_symbolic_call\u001b[0;34m(self, inputs, input_masks, args, kwargs)\u001b[0m\n\u001b[1;32m    814\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeras_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKerasTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_signature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    815\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 816\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_infer_output_signature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_infer_output_signature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_infer_output_signature\u001b[0;34m(self, inputs, args, kwargs, input_masks)\u001b[0m\n\u001b[1;32m    854\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m           \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    414\u001b[0m     \"\"\"\n\u001b[1;32m    415\u001b[0m     return self._run_internal_graph(\n\u001b[0;32m--> 416\u001b[0;31m         inputs, training=training, mask=mask)\n\u001b[0m\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcompute_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py\u001b[0m in \u001b[0;36m_run_internal_graph\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    558\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m       \u001b[0mx_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m       \u001b[0;32massert\u001b[0m \u001b[0mx_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtensor_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Could not compute output '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m       \u001b[0moutput_tensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: Could not compute output KerasTensor(type_spec=TensorSpec(shape=(None, 2), dtype=tf.float32, name=None), name='dense_359/Sigmoid:0', description=\"created by layer 'dense_359'\")"
          ]
        }
      ]
    }
  ]
}