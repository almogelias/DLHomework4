{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DLHomework4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "history_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/almogelias/DLHomework4/blob/main/DLHomework4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCy8CL1baoyJ"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQOanPh61S3T",
        "outputId": "5495f520-e493-49e2-d6b9-c90fc8bf0992"
      },
      "source": [
        "\n",
        "from keras.datasets import mnist\n",
        "from keras.layers import Input, Dense, Reshape, Flatten\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.models import Sequential, Model\n",
        "from keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy.spatial import distance\n",
        "\n",
        "!git clone -s https://github.com/almogelias/DLHomework4.git DLHomework4\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'DLHomework4'...\n",
            "remote: Enumerating objects: 4, done.\u001b[K\n",
            "remote: Counting objects: 100% (4/4), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 4 (delta 0), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (4/4), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmisa9Ktasvz"
      },
      "source": [
        "# Preprocess of diabetes.arff file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wm6DWTA1n9A"
      },
      "source": [
        "\n",
        "from scipy.io import arff\n",
        "import pandas as pd\n",
        "import os\n",
        "from sklearn import preprocessing\n",
        "\n",
        "\n",
        "#Load the data using \"arff.loadarff\" then convert it to dataframe\n",
        "\n",
        "repository_path = os.path.join(os.getcwd(), 'DLHomework4')\n",
        "train_diabetes_path = os.path.join(repository_path, 'diabetes.arff')\n",
        "\n",
        "data = arff.loadarff(train_diabetes_path)\n",
        "df = pd.DataFrame(data[0])\n",
        "\n",
        "y_train = df['class'].replace({b'tested_positive':'1', b'tested_negative':'0'})\n",
        "\n",
        "        \n",
        "# Drop last column of a dataframe\n",
        "x_train = df.iloc[: , :-1]\n",
        "min_max_scaler = preprocessing.MinMaxScaler()\n",
        "x_scaled = min_max_scaler.fit_transform(x_train)\n",
        "x_train = pd.DataFrame(x_scaled)\n",
        "#x_train, x_test, y_train, y_test = train_test_split(x, y,test_size=3)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PK7Tv9oz1hp7"
      },
      "source": [
        "#Define input dimensions\n",
        "diabetes_dim = x_train.shape[1]\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dSkANdxazYE"
      },
      "source": [
        "# Generator Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QEHK-wMh6mjf",
        "outputId": "cc424643-cb0c-4c25-a7eb-e7421f643511"
      },
      "source": [
        "#Given input of noise (latent) vector, the Generator produces an sample.\n",
        "def build_generator():\n",
        "\n",
        "    noise_shape = (8,) #1D array of size 100 (latent vector / noise)\n",
        "\n",
        "#Define your generator network \n",
        "#Here we are only using Dense layers. But network can be complicated based\n",
        "#on the application. For example, you can use VGG for super res. GAN.         \n",
        "\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(diabetes_dim, input_shape=noise_shape))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(Dense(diabetes_dim*diabetes_dim))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(Dense(diabetes_dim*diabetes_dim*diabetes_dim))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    \n",
        "    model.add(Dense(np.prod(diabetes_dim), activation='tanh'))\n",
        "    #model.add(Reshape(diabetes_dim))\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    noise = Input(shape=noise_shape)\n",
        "    img = model(noise)    #Generated sample\n",
        "    \n",
        "    return Model(noise, img)\n",
        "\n",
        "#Alpha — α is a hyperparameter which controls the underlying value to which the\n",
        "#function saturates negatives network inputs.\n",
        "#Momentum — Speed up the training\n",
        "\n",
        "generator = build_generator()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "leaky_re_lu (LeakyReLU)      (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 64)                576       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)    (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 512)               33280     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_2 (LeakyReLU)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 8)                 4104      \n",
            "=================================================================\n",
            "Total params: 40,368\n",
            "Trainable params: 39,200\n",
            "Non-trainable params: 1,168\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOStgE9na2bM"
      },
      "source": [
        "#Discriminator Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQCE_DE_805i",
        "outputId": "00fe0c57-1c74-4d91-a10a-fa7c6960d0cd"
      },
      "source": [
        "\n",
        "#Given an input sample, the Discriminator outputs the likelihood of the sample being real.\n",
        "#Binary classification - true or false (we're calling it validity)\n",
        "\n",
        "def build_discriminator():\n",
        "\n",
        "    diabetes_dim_shape = (diabetes_dim,)\n",
        "    model1 = Sequential()\n",
        "\n",
        "    model1.add(Dense(diabetes_dim, input_shape=diabetes_dim_shape))\n",
        "    model1.add(Dense(diabetes_dim*diabetes_dim*diabetes_dim))\n",
        "    model1.add(LeakyReLU(alpha=0.2))\n",
        "    model1.add(Dense(diabetes_dim*diabetes_dim))\n",
        "    model1.add(LeakyReLU(alpha=0.2))\n",
        "    model1.add(Dense(1, activation='sigmoid'))\n",
        "    model1.summary()\n",
        "\n",
        "    diabetes_input = Input(shape=diabetes_dim)\n",
        "    validity = model1(diabetes_input)\n",
        "\n",
        "    return Model(diabetes_input, validity)\n",
        "#The validity is the Discriminator’s guess of input being real or not.\n",
        "discriminator = build_discriminator()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_4 (Dense)              (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 512)               4608      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_3 (LeakyReLU)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 64)                32832     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_4 (LeakyReLU)    (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 37,577\n",
            "Trainable params: 37,577\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9nzYdKFa7o0"
      },
      "source": [
        "#Training flow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djqKY7Y2-y5m"
      },
      "source": [
        "#Now that we have constructed our two models it’s time to pit them against each other.\n",
        "#We do this by defining a training function, loading the data set, re-scaling our training\n",
        "#samples and setting the ground truths. \n",
        "def train(epochs, batch_size=128, save_interval=50):\n",
        "\n",
        "    half_batch = int(batch_size / 2)\n",
        "#We then loop through a number of epochs to train our Discriminator by first selecting\n",
        "#a random batch of samples from our true dataset, generating a set of samples from our\n",
        "#Generator, feeding both set of samples into our Discriminator, and finally setting the\n",
        "#loss parameters for both the real and fake samples, as well as the combined loss. \n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "\n",
        "      # ---------------------\n",
        "      #  Train Discriminator\n",
        "      # ---------------------\n",
        "      \n",
        "      # Select a random half batch of real samples\n",
        "      idx = np.random.randint(0, x_train.shape[0], half_batch)\n",
        "      diabetes = x_train.iloc[idx]\n",
        "\n",
        "\n",
        "      noise = np.random.normal(0, 1, (half_batch, 8))\n",
        "\n",
        "      # Generate a half batch of fake samples\n",
        "      gen_diabetes = generator.predict(noise)\n",
        "\n",
        "      # Train the discriminator on real and fake samples, separately\n",
        "      #Research showed that separate training is more effective. \n",
        "      d_loss_real = discriminator.train_on_batch(diabetes, np.ones((half_batch, 1)))\n",
        "      d_loss_fake = discriminator.train_on_batch(gen_diabetes, np.zeros((half_batch, 1)))\n",
        "    #take average loss from real and fake samples. \n",
        "\n",
        "      d_loss = 0.5 * np.add(d_loss_real, d_loss_fake) \n",
        "      euclidean_distance = distance.euclidean(d_loss_real,d_loss_fake)\n",
        "      '''\n",
        "      if (euclidean_distance > 1.1):\n",
        "        print (\"real: \"+str(d_loss_real)+\" , fake: \"+str(d_loss_fake) + \" , Duclidean distance: \"+ str(euclidean_distance))\n",
        "        print ()\n",
        "        print(\"real diabetes values:\")\n",
        "        print(pd.DataFrame(diabetes))\n",
        "        print(\"fake diabetes values:\")\n",
        "        print(pd.DataFrame(gen_diabetes))\n",
        "      '''\n",
        "      \n",
        "      \n",
        "     \n",
        "        \n",
        "      \n",
        "    #And within the same loop we train our Generator, by setting the input noise and\n",
        "    #ultimately training the Generator to have the Discriminator label its samples as valid\n",
        "    #by specifying the gradient loss.\n",
        "      # ---------------------\n",
        "      #  Train Generator\n",
        "      # ---------------------\n",
        "    #Create noise vectors as input for generator. \n",
        "    #Create as many noise vectors as defined by the batch size. \n",
        "    #Based on normal distribution. Output will be of size (batch size, 100)\n",
        "      noise = np.random.normal(0, 1, (batch_size, 8)) \n",
        "\n",
        "      # The generator wants the discriminator to label the generated samples\n",
        "      # as valid (ones)\n",
        "      #This is where the genrator is trying to trick discriminator into believing\n",
        "      #the generated sample is true (hence value of 1 for y)\n",
        "      valid_y = np.array([1] * batch_size) #Creates an array of all ones of size=batch size\n",
        "\n",
        "      # Generator is part of combined where it got directly linked with the discriminator\n",
        "      # Train the generator with noise as x and 1 as y. \n",
        "      # Again, 1 as the output as it is adversarial and if generator did a great\n",
        "      #job of folling the discriminator then the output would be 1 (true)\n",
        "      g_loss = combined.train_on_batch(noise, valid_y)\n",
        "      if (g_loss > 1):\n",
        "        print(\"fake diabetes noise values that fooled the model:\")\n",
        "        print(pd.DataFrame(noise))\n",
        "\n",
        "    #Additionally, in order for us to keep track of our training process, we print the\n",
        "    #progress and save the sample sample output depending on the epoch interval specified.  \n",
        "    #Plot the progress\n",
        "      \n",
        "      print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
        "      \n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3T_Mcdy1pBvB",
        "outputId": "c4f07599-1b0f-4c5c-904c-17ddf4c4a793"
      },
      "source": [
        "optimizer = Adam(0.0002, 0.5)  #Learning rate and momentum.\n",
        "discriminator = build_discriminator()\n",
        "discriminator.compile(loss='binary_crossentropy',\n",
        "    optimizer=optimizer,\n",
        "    metrics=['accuracy'])\n",
        "\n",
        "#build and compile our Discriminator, pick the loss function\n",
        "\n",
        "#SInce we are only generating (faking) samples, let us not track any metrics.\n",
        "generator = build_generator()\n",
        "generator.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "\n",
        "##This builds the Generator and defines the input noise. \n",
        "#In a GAN the Generator network takes noise z as an input to produce its samples.  \n",
        "z = Input(shape=(8,))   #Our random input to the generator\n",
        "test_if_diabete = generator(z)\n",
        "\n",
        "#This ensures that when we combine our networks we only train the Generator.\n",
        "#While generator training we do not want discriminator weights to be adjusted. \n",
        "#This Doesn't affect the above descriminator training.     \n",
        "discriminator.trainable = False  \n",
        "\n",
        "#This specifies that our Discriminator will take the samples generated by our Generator\n",
        "#and true dataset and set its output to a parameter called valid, which will indicate\n",
        "#whether the input is real or not.  \n",
        "valid = discriminator(test_if_diabete)  #Validity check on the generated sample\n",
        "\n",
        "\n",
        "#Here we combined the models and also set our loss function and optimizer. \n",
        "#Again, we are only training the generator here. \n",
        "#The ultimate goal here is for the Generator to fool the Discriminator.  \n",
        "# The combined model  (stacked generator and discriminator) takes\n",
        "# noise as input => generates samples => determines validity\n",
        "\n",
        "combined = Model(z, valid)\n",
        "combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "\n",
        "\n",
        "train(epochs=150, batch_size=32, save_interval=10)\n",
        "\n",
        "#Save model for future use to generate fake samples\n",
        "#Not tested yet... make sure right model is being saved..\n",
        "#Compare with GAN4\n",
        "\n",
        "generator.save('generator_model.h5')  #Test the model on GAN4_predict...\n",
        "#Change epochs back to 30K\n",
        "                \n",
        "#Epochs dictate the number of backward and forward propagations, the batch_size\n",
        "#indicates the number of training samples per backward/forward propagation, and the\n",
        "#sample_interval specifies after how many epochs we call our sample function."
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_8 (Dense)              (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 512)               4608      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_5 (LeakyReLU)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 64)                32832     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_6 (LeakyReLU)    (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 37,577\n",
            "Trainable params: 37,577\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_12 (Dense)             (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_7 (LeakyReLU)    (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 64)                576       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_8 (LeakyReLU)    (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 512)               33280     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_9 (LeakyReLU)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 8)                 4104      \n",
            "=================================================================\n",
            "Total params: 40,368\n",
            "Trainable params: 39,200\n",
            "Non-trainable params: 1,168\n",
            "_________________________________________________________________\n",
            "0 [D loss: 0.683249, acc.: 87.50%] [G loss: 0.711491]\n",
            "1 [D loss: 0.669003, acc.: 93.75%] [G loss: 0.713788]\n",
            "2 [D loss: 0.654633, acc.: 78.12%] [G loss: 0.707068]\n",
            "3 [D loss: 0.636422, acc.: 87.50%] [G loss: 0.693929]\n",
            "4 [D loss: 0.625141, acc.: 84.38%] [G loss: 0.675964]\n",
            "5 [D loss: 0.631566, acc.: 65.62%] [G loss: 0.679250]\n",
            "6 [D loss: 0.614569, acc.: 78.12%] [G loss: 0.668926]\n",
            "7 [D loss: 0.605403, acc.: 71.88%] [G loss: 0.646699]\n",
            "8 [D loss: 0.605576, acc.: 71.88%] [G loss: 0.657923]\n",
            "9 [D loss: 0.578179, acc.: 81.25%] [G loss: 0.636516]\n",
            "10 [D loss: 0.623679, acc.: 62.50%] [G loss: 0.692764]\n",
            "11 [D loss: 0.585899, acc.: 84.38%] [G loss: 0.668388]\n",
            "12 [D loss: 0.588297, acc.: 78.12%] [G loss: 0.669642]\n",
            "13 [D loss: 0.601563, acc.: 75.00%] [G loss: 0.659821]\n",
            "14 [D loss: 0.574318, acc.: 84.38%] [G loss: 0.657195]\n",
            "15 [D loss: 0.629443, acc.: 68.75%] [G loss: 0.699645]\n",
            "16 [D loss: 0.611882, acc.: 71.88%] [G loss: 0.717638]\n",
            "17 [D loss: 0.602415, acc.: 71.88%] [G loss: 0.742698]\n",
            "18 [D loss: 0.586732, acc.: 81.25%] [G loss: 0.775744]\n",
            "19 [D loss: 0.616428, acc.: 71.88%] [G loss: 0.734882]\n",
            "20 [D loss: 0.565061, acc.: 75.00%] [G loss: 0.695565]\n",
            "21 [D loss: 0.579073, acc.: 75.00%] [G loss: 0.726021]\n",
            "22 [D loss: 0.636070, acc.: 62.50%] [G loss: 0.788574]\n",
            "23 [D loss: 0.561380, acc.: 71.88%] [G loss: 0.780399]\n",
            "24 [D loss: 0.575692, acc.: 78.12%] [G loss: 0.798520]\n",
            "25 [D loss: 0.518393, acc.: 84.38%] [G loss: 0.732002]\n",
            "26 [D loss: 0.553473, acc.: 75.00%] [G loss: 0.827378]\n",
            "27 [D loss: 0.543528, acc.: 84.38%] [G loss: 0.744627]\n",
            "28 [D loss: 0.682191, acc.: 65.62%] [G loss: 0.828105]\n",
            "29 [D loss: 0.647593, acc.: 68.75%] [G loss: 0.832575]\n",
            "30 [D loss: 0.579782, acc.: 75.00%] [G loss: 0.844998]\n",
            "31 [D loss: 0.556557, acc.: 78.12%] [G loss: 0.844107]\n",
            "32 [D loss: 0.581598, acc.: 75.00%] [G loss: 0.874219]\n",
            "33 [D loss: 0.564344, acc.: 84.38%] [G loss: 0.850977]\n",
            "34 [D loss: 0.579840, acc.: 75.00%] [G loss: 0.862995]\n",
            "35 [D loss: 0.631701, acc.: 68.75%] [G loss: 0.839812]\n",
            "36 [D loss: 0.634268, acc.: 75.00%] [G loss: 0.843536]\n",
            "37 [D loss: 0.582358, acc.: 78.12%] [G loss: 0.862874]\n",
            "38 [D loss: 0.647088, acc.: 62.50%] [G loss: 0.867577]\n",
            "39 [D loss: 0.558806, acc.: 84.38%] [G loss: 0.900402]\n",
            "40 [D loss: 0.588535, acc.: 84.38%] [G loss: 0.844760]\n",
            "41 [D loss: 0.601185, acc.: 71.88%] [G loss: 0.915441]\n",
            "42 [D loss: 0.609372, acc.: 68.75%] [G loss: 0.871539]\n",
            "43 [D loss: 0.647077, acc.: 75.00%] [G loss: 0.892251]\n",
            "44 [D loss: 0.678872, acc.: 56.25%] [G loss: 0.970073]\n",
            "45 [D loss: 0.588007, acc.: 71.88%] [G loss: 0.920248]\n",
            "46 [D loss: 0.605989, acc.: 71.88%] [G loss: 0.876359]\n",
            "47 [D loss: 0.626565, acc.: 68.75%] [G loss: 0.980649]\n",
            "48 [D loss: 0.541422, acc.: 87.50%] [G loss: 0.918203]\n",
            "49 [D loss: 0.560798, acc.: 84.38%] [G loss: 0.885848]\n",
            "50 [D loss: 0.612229, acc.: 65.62%] [G loss: 0.922578]\n",
            "51 [D loss: 0.626369, acc.: 75.00%] [G loss: 0.909210]\n",
            "52 [D loss: 0.556825, acc.: 75.00%] [G loss: 0.920717]\n",
            "53 [D loss: 0.600075, acc.: 71.88%] [G loss: 0.913174]\n",
            "54 [D loss: 0.647838, acc.: 71.88%] [G loss: 0.966107]\n",
            "55 [D loss: 0.654650, acc.: 71.88%] [G loss: 0.923429]\n",
            "56 [D loss: 0.608368, acc.: 75.00%] [G loss: 0.870167]\n",
            "57 [D loss: 0.625700, acc.: 68.75%] [G loss: 0.926580]\n",
            "58 [D loss: 0.596512, acc.: 71.88%] [G loss: 0.886376]\n",
            "59 [D loss: 0.621568, acc.: 68.75%] [G loss: 0.950135]\n",
            "60 [D loss: 0.568007, acc.: 84.38%] [G loss: 0.954138]\n",
            "61 [D loss: 0.590454, acc.: 75.00%] [G loss: 0.942772]\n",
            "62 [D loss: 0.642986, acc.: 75.00%] [G loss: 0.951814]\n",
            "63 [D loss: 0.625244, acc.: 75.00%] [G loss: 0.843625]\n",
            "64 [D loss: 0.589718, acc.: 75.00%] [G loss: 0.853412]\n",
            "65 [D loss: 0.577781, acc.: 84.38%] [G loss: 0.914981]\n",
            "66 [D loss: 0.600402, acc.: 81.25%] [G loss: 0.877315]\n",
            "67 [D loss: 0.543356, acc.: 87.50%] [G loss: 0.937483]\n",
            "68 [D loss: 0.580677, acc.: 81.25%] [G loss: 0.890104]\n",
            "69 [D loss: 0.622688, acc.: 81.25%] [G loss: 0.931099]\n",
            "70 [D loss: 0.563423, acc.: 84.38%] [G loss: 0.881016]\n",
            "71 [D loss: 0.638703, acc.: 68.75%] [G loss: 0.905386]\n",
            "72 [D loss: 0.653465, acc.: 65.62%] [G loss: 0.974391]\n",
            "73 [D loss: 0.637853, acc.: 75.00%] [G loss: 0.988062]\n",
            "74 [D loss: 0.630271, acc.: 68.75%] [G loss: 0.944509]\n",
            "75 [D loss: 0.575101, acc.: 87.50%] [G loss: 0.930968]\n",
            "76 [D loss: 0.611897, acc.: 71.88%] [G loss: 0.957105]\n",
            "77 [D loss: 0.685251, acc.: 56.25%] [G loss: 0.908147]\n",
            "78 [D loss: 0.611711, acc.: 81.25%] [G loss: 0.917384]\n",
            "79 [D loss: 0.639752, acc.: 62.50%] [G loss: 0.911476]\n",
            "80 [D loss: 0.688511, acc.: 53.12%] [G loss: 0.963458]\n",
            "81 [D loss: 0.613269, acc.: 75.00%] [G loss: 0.895020]\n",
            "82 [D loss: 0.596232, acc.: 81.25%] [G loss: 0.948051]\n",
            "83 [D loss: 0.624693, acc.: 75.00%] [G loss: 0.971547]\n",
            "84 [D loss: 0.659802, acc.: 65.62%] [G loss: 0.892478]\n",
            "85 [D loss: 0.576965, acc.: 84.38%] [G loss: 0.896305]\n",
            "86 [D loss: 0.658323, acc.: 65.62%] [G loss: 0.955552]\n",
            "87 [D loss: 0.638682, acc.: 75.00%] [G loss: 0.912446]\n",
            "88 [D loss: 0.617942, acc.: 75.00%] [G loss: 0.863938]\n",
            "89 [D loss: 0.558618, acc.: 87.50%] [G loss: 0.922070]\n",
            "90 [D loss: 0.631264, acc.: 65.62%] [G loss: 0.879257]\n",
            "91 [D loss: 0.668951, acc.: 68.75%] [G loss: 0.882771]\n",
            "92 [D loss: 0.605101, acc.: 78.12%] [G loss: 0.870994]\n",
            "93 [D loss: 0.602324, acc.: 90.62%] [G loss: 0.932431]\n",
            "94 [D loss: 0.616711, acc.: 81.25%] [G loss: 0.952114]\n",
            "95 [D loss: 0.609021, acc.: 71.88%] [G loss: 0.925678]\n",
            "96 [D loss: 0.642660, acc.: 68.75%] [G loss: 0.951294]\n",
            "97 [D loss: 0.651212, acc.: 65.62%] [G loss: 0.932993]\n",
            "98 [D loss: 0.622889, acc.: 81.25%] [G loss: 0.934760]\n",
            "99 [D loss: 0.605833, acc.: 78.12%] [G loss: 0.921383]\n",
            "100 [D loss: 0.659281, acc.: 56.25%] [G loss: 0.857717]\n",
            "101 [D loss: 0.639797, acc.: 71.88%] [G loss: 0.873471]\n",
            "102 [D loss: 0.582313, acc.: 84.38%] [G loss: 0.945248]\n",
            "103 [D loss: 0.631461, acc.: 68.75%] [G loss: 0.849192]\n",
            "104 [D loss: 0.610460, acc.: 84.38%] [G loss: 0.880895]\n",
            "105 [D loss: 0.590560, acc.: 81.25%] [G loss: 0.843135]\n",
            "106 [D loss: 0.634352, acc.: 65.62%] [G loss: 0.769169]\n",
            "107 [D loss: 0.639519, acc.: 71.88%] [G loss: 0.893873]\n",
            "108 [D loss: 0.555371, acc.: 78.12%] [G loss: 0.839641]\n",
            "109 [D loss: 0.677213, acc.: 56.25%] [G loss: 0.843355]\n",
            "110 [D loss: 0.629668, acc.: 71.88%] [G loss: 0.965093]\n",
            "111 [D loss: 0.574554, acc.: 84.38%] [G loss: 0.845735]\n",
            "112 [D loss: 0.697619, acc.: 56.25%] [G loss: 0.868168]\n",
            "113 [D loss: 0.690441, acc.: 53.12%] [G loss: 0.858865]\n",
            "114 [D loss: 0.645739, acc.: 65.62%] [G loss: 0.860635]\n",
            "115 [D loss: 0.657183, acc.: 62.50%] [G loss: 0.910974]\n",
            "116 [D loss: 0.682674, acc.: 46.88%] [G loss: 0.912847]\n",
            "117 [D loss: 0.690662, acc.: 43.75%] [G loss: 0.948484]\n",
            "118 [D loss: 0.635925, acc.: 65.62%] [G loss: 0.917890]\n",
            "119 [D loss: 0.600352, acc.: 65.62%] [G loss: 0.884064]\n",
            "120 [D loss: 0.650967, acc.: 43.75%] [G loss: 0.917730]\n",
            "121 [D loss: 0.665503, acc.: 56.25%] [G loss: 0.838093]\n",
            "122 [D loss: 0.640265, acc.: 68.75%] [G loss: 0.950867]\n",
            "123 [D loss: 0.637607, acc.: 59.38%] [G loss: 0.942255]\n",
            "124 [D loss: 0.674296, acc.: 43.75%] [G loss: 0.879000]\n",
            "125 [D loss: 0.615866, acc.: 62.50%] [G loss: 0.924189]\n",
            "126 [D loss: 0.658583, acc.: 56.25%] [G loss: 0.903440]\n",
            "127 [D loss: 0.687437, acc.: 59.38%] [G loss: 0.940593]\n",
            "128 [D loss: 0.611001, acc.: 56.25%] [G loss: 0.916794]\n",
            "129 [D loss: 0.628131, acc.: 46.88%] [G loss: 0.911283]\n",
            "130 [D loss: 0.687745, acc.: 46.88%] [G loss: 0.942616]\n",
            "131 [D loss: 0.626272, acc.: 56.25%] [G loss: 0.940797]\n",
            "132 [D loss: 0.667467, acc.: 59.38%] [G loss: 0.890500]\n",
            "133 [D loss: 0.667204, acc.: 68.75%] [G loss: 0.897392]\n",
            "134 [D loss: 0.649221, acc.: 50.00%] [G loss: 0.890573]\n",
            "135 [D loss: 0.627307, acc.: 56.25%] [G loss: 0.880036]\n",
            "136 [D loss: 0.631272, acc.: 65.62%] [G loss: 0.927063]\n",
            "137 [D loss: 0.653274, acc.: 50.00%] [G loss: 0.915167]\n",
            "138 [D loss: 0.616209, acc.: 65.62%] [G loss: 0.845304]\n",
            "139 [D loss: 0.626453, acc.: 56.25%] [G loss: 0.931436]\n",
            "140 [D loss: 0.655705, acc.: 62.50%] [G loss: 0.912611]\n",
            "141 [D loss: 0.648330, acc.: 62.50%] [G loss: 0.931385]\n",
            "142 [D loss: 0.670558, acc.: 53.12%] [G loss: 0.912143]\n",
            "143 [D loss: 0.627487, acc.: 71.88%] [G loss: 0.835967]\n",
            "144 [D loss: 0.673393, acc.: 43.75%] [G loss: 0.931790]\n",
            "145 [D loss: 0.660337, acc.: 53.12%] [G loss: 0.950838]\n",
            "146 [D loss: 0.628585, acc.: 56.25%] [G loss: 0.842824]\n",
            "147 [D loss: 0.658145, acc.: 59.38%] [G loss: 0.919221]\n",
            "148 [D loss: 0.643418, acc.: 65.62%] [G loss: 0.897793]\n",
            "149 [D loss: 0.626210, acc.: 65.62%] [G loss: 0.855507]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bHU4fDX1_vC"
      },
      "source": [
        "# German Credit DB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "id": "UmmiPqzE1_Qj",
        "outputId": "121b9c52-8980-4f20-ab25-1b07617639ab"
      },
      "source": [
        "'''\n",
        "\n",
        "#Load the data using \"arff.loadarff\" then convert it to dataframe\n",
        "\n",
        "repository_path = os.path.join(os.getcwd(), 'DLHomework4')\n",
        "train_german_credit_path = os.path.join(repository_path, 'german_credit.arff')\n",
        "\n",
        "data = arff.loadarff(train_german_credit_path)\n",
        "df = pd.DataFrame(data[0])\n",
        "\n",
        "y_train = df['21'].replace({b'1':'1', b'2':'2'})\n",
        "\n",
        "        \n",
        "# Drop last column of a dataframe\n",
        "x_train = df.iloc[: , :-1]\n",
        "min_max_scaler = preprocessing.MinMaxScaler()\n",
        "x_scaled = min_max_scaler.fit_transform(x_train)\n",
        "x_train = pd.DataFrame(x_scaled)\n",
        "#x_train, x_test, y_train, y_test = train_test_split(x, y,test_size=3)\n",
        "'''"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n\\n#Load the data using \"arff.loadarff\" then convert it to dataframe\\n\\nrepository_path = os.path.join(os.getcwd(), \\'DLHomework4\\')\\ntrain_german_credit_path = os.path.join(repository_path, \\'german_credit.arff\\')\\n\\ndata = arff.loadarff(train_german_credit_path)\\ndf = pd.DataFrame(data[0])\\n\\ny_train = df[\\'21\\'].replace({b\\'1\\':\\'1\\', b\\'2\\':\\'2\\'})\\n\\n        \\n# Drop last column of a dataframe\\nx_train = df.iloc[: , :-1]\\nmin_max_scaler = preprocessing.MinMaxScaler()\\nx_scaled = min_max_scaler.fit_transform(x_train)\\nx_train = pd.DataFrame(x_scaled)\\n#x_train, x_test, y_train, y_test = train_test_split(x, y,test_size=3)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODzEaMEkbcFs"
      },
      "source": [
        "# Random Forest model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DjteC-kSbfnk",
        "outputId": "631c59cc-f60b-4e33-ad76-98a534940c2f"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "Xtrain, Xtest, ytrain, ytest = train_test_split(x_train, y_train,\n",
        "                                                 test_size=0.33, random_state=42)\n",
        "\n",
        "def BB_Model(sample_Xtrain,sample_ytrain):\n",
        "  model = RandomForestClassifier(n_estimators=100, random_state=0)\n",
        "  \n",
        "  model = RandomForestClassifier(n_estimators=1000)\n",
        "  model.fit(Xtrain, ytrain)\n",
        "  return model\n",
        "\n",
        "model=BB_Model(Xtrain,ytrain)\n",
        "ypred = model.predict(Xtest)\n",
        "accuracy_score(ytest, ypred)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7480314960629921"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgb_3lHA-Ena"
      },
      "source": [
        "# BUILD new generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xyrOPdW5-DpI",
        "outputId": "19a80e17-f029-406d-83ff-7d58d172de2e"
      },
      "source": [
        "from tensorflow.keras.layers import concatenate\n",
        "#Given input of noise (latent) vector, the Generator produces an sample.\n",
        "def build_generator(noise_shape, desired_confidence_shape):\n",
        "\n",
        "    input_noise = Input(shape=noise_shape)\n",
        "    input_confidence = Input(shape=desired_confidence_shape)\n",
        "\n",
        "#Define your generator network \n",
        "#Here we are only using Dense layers. But network can be complicated based\n",
        "#on the application. For example, you can use VGG for super res. GAN.         \n",
        "\n",
        "    x = Dense(noise_shape[0])(input_noise)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    x = BatchNormalization(momentum=0.2)(x)\n",
        "    x = Dense(noise_shape[0]*noise_shape[0])(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    x = BatchNormalization(momentum=0.2)(x)\n",
        "    x = Dense(noise_shape[0]*noise_shape[0]*noise_shape[0])(x)\n",
        "    x = BatchNormalization(momentum=0.2)(x)\n",
        "    x = Model(inputs=input_noise, outputs=x)\n",
        "\n",
        "    y = Dense(desired_confidence_shape[0])(input_confidence)\n",
        "    y = LeakyReLU(alpha=0.2)(y)\n",
        "    y = BatchNormalization(momentum=0.2)(y)\n",
        "    y = Dense(desired_confidence_shape[0]*desired_confidence_shape[0])(y)\n",
        "    y = LeakyReLU(alpha=0.2)(y)\n",
        "    y = BatchNormalization(momentum=0.2)(y)\n",
        "    y = Model(inputs=input_confidence, outputs=y)\n",
        "\n",
        "    combined = concatenate([x.output, y.output])\n",
        "\n",
        "    z = Dense(2, activation=\"relu\")(combined)\n",
        "    z = Dense(1, activation=\"linear\")(z)\n",
        "\n",
        "    model = Model(inputs=[x.input, y.input], outputs=z)\n",
        "    model.summary()\n",
        "    \n",
        "    return model\n",
        "\n",
        "#Alpha — α is a hyperparameter which controls the underlying value to which the\n",
        "#function saturates negatives network inputs.\n",
        "#Momentum — Speed up the training\n",
        "noise_shape = (8,)\n",
        "desired_confidence_shape=(1,)\n",
        "generator = build_generator(noise_shape,desired_confidence_shape)\n",
        "generator.compile(loss='binary_crossentropy', optimizer=optimizer)\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_7\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_8 (InputLayer)            [(None, 8)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_21 (Dense)                (None, 8)            72          input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_13 (LeakyReLU)      (None, 8)            0           dense_21[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "input_9 (InputLayer)            [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 8)            32          leaky_re_lu_13[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dense_24 (Dense)                (None, 1)            2           input_9[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_22 (Dense)                (None, 64)           576         batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_15 (LeakyReLU)      (None, 1)            0           dense_24[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_14 (LeakyReLU)      (None, 64)           0           dense_22[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 1)            4           leaky_re_lu_15[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 64)           256         leaky_re_lu_14[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dense_25 (Dense)                (None, 1)            2           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dense_23 (Dense)                (None, 512)          33280       batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_16 (LeakyReLU)      (None, 1)            0           dense_25[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 512)          2048        dense_23[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 1)            4           leaky_re_lu_16[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.concat (TFOpLambda)          (None, 513)          0           batch_normalization_11[0][0]     \n",
            "                                                                 batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dense_26 (Dense)                (None, 2)            1028        tf.concat[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_27 (Dense)                (None, 1)            3           dense_26[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 37,307\n",
            "Trainable params: 36,135\n",
            "Non-trainable params: 1,172\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vitSixKHneuR",
        "outputId": "495fd696-5e54-4365-c14f-ba6342af3def"
      },
      "source": [
        "\n",
        "#Given an input sample, the Discriminator outputs the likelihood of the sample being real.\n",
        "#Binary classification - true or false (we're calling it validity)\n",
        "\n",
        "def build_discriminator(sample_shape, desired_confidence_shape):\n",
        "\n",
        "    \n",
        "    input_sample = Input(shape=sample_shape)\n",
        "    input_confidence = Input(shape=desired_confidence_shape)\n",
        "\n",
        "#Define your generator network \n",
        "#Here we are only using Dense layers. But network can be complicated based\n",
        "#on the application. For example, you can use VGG for super res. GAN.         \n",
        "\n",
        "    x = Dense(sample_shape[0])(input_sample)\n",
        "    x = Dense(sample_shape[0]*sample_shape[0]*sample_shape[0])(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    x = Dense(sample_shape[0]*sample_shape[0])(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    x = Model(inputs=input_sample, outputs=x)\n",
        "\n",
        "    y = Dense(desired_confidence_shape[0])(input_confidence)\n",
        "    y = LeakyReLU(alpha=0.2)(y)\n",
        "    y = Dense(desired_confidence_shape[0]*desired_confidence_shape[0])(y)\n",
        "    y = LeakyReLU(alpha=0.2)(y)\n",
        "    y = Model(inputs=input_confidence, outputs=y)\n",
        "\n",
        "    combined = concatenate([x.output, y.output])\n",
        "\n",
        "    z = Dense(2, activation=\"sigmoid\")(combined)\n",
        "\n",
        "    model = Model(inputs=[x.input, y.input], outputs=z)\n",
        "    model.summary()\n",
        "    \n",
        "    return model\n",
        "\n",
        "    #return Model(diabetes_input, validity)\n",
        "#The validity is the Discriminator’s guess of input being real or not.\n",
        "noise_shape = (8,)\n",
        "desired_confidence_shape=(1,)\n",
        "discriminator = build_discriminator(noise_shape,desired_confidence_shape)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_10\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_12 (InputLayer)           [(None, 8)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_31 (Dense)                (None, 8)            72          input_12[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "input_13 (InputLayer)           [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_32 (Dense)                (None, 512)          4608        dense_31[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_34 (Dense)                (None, 1)            2           input_13[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_19 (LeakyReLU)      (None, 512)          0           dense_32[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_21 (LeakyReLU)      (None, 1)            0           dense_34[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_33 (Dense)                (None, 64)           32832       leaky_re_lu_19[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dense_35 (Dense)                (None, 1)            2           leaky_re_lu_21[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_20 (LeakyReLU)      (None, 64)           0           dense_33[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_22 (LeakyReLU)      (None, 1)            0           dense_35[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "tf.concat_1 (TFOpLambda)        (None, 65)           0           leaky_re_lu_20[0][0]             \n",
            "                                                                 leaky_re_lu_22[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dense_36 (Dense)                (None, 2)            132         tf.concat_1[0][0]                \n",
            "==================================================================================================\n",
            "Total params: 37,648\n",
            "Trainable params: 37,648\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjdHV0fI5VEY"
      },
      "source": [
        "#Now that we have constructed our two models it’s time to pit them against each other.\n",
        "#We do this by defining a training function, loading the data set, re-scaling our training\n",
        "#samples and setting the ground truths. \n",
        "def train(epochs, batch_size=128, save_interval=50):\n",
        "\n",
        "    half_batch = int(batch_size / 2)\n",
        "#We then loop through a number of epochs to train our Discriminator by first selecting\n",
        "#a random batch of samples from our true dataset, generating a set of samples from our\n",
        "#Generator, feeding both set of samples into our Discriminator, and finally setting the\n",
        "#loss parameters for both the real and fake samples, as well as the combined loss. \n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "\n",
        "      # ---------------------\n",
        "      #  Train Discriminator\n",
        "      # ---------------------\n",
        "      '''\n",
        "      # Select a random half batch of real samples\n",
        "\n",
        "      idx = np.random.randint(0, Xtrain.shape[0], half_batch)\n",
        "      diabetes = Xtrain.iloc[idx]\n",
        "\n",
        "      '''\n",
        "   \n",
        "\n",
        "      noise = np.random.normal(0, 1, (half_batch, 8))\n",
        "      scalar = np.random.normal(0, 1, (half_batch, 8))\n",
        "      # Generate a half batch of fake samples\n",
        "      gen_diabetes = generator.predict(noise,scalar)\n",
        "\n",
        "      # Train the discriminator on real and fake samples, separately\n",
        "      #Research showed that separate training is more effective. \n",
        "      d_loss_one = discriminator.train_on_batch(gen_diabetes, np.ones((half_batch, 1)))\n",
        "      d_loss_zero = discriminator.train_on_batch(gen_diabetes, np.zeros((half_batch, 1)))\n",
        "    #take average loss from real and fake samples. \n",
        "\n",
        "      d_loss = 0.5 * np.add(d_loss_real, d_loss_fake) \n",
        "      euclidean_distance = distance.euclidean(d_loss_real,d_loss_fake)\n",
        "      '''\n",
        "      if (euclidean_distance > 1.1):\n",
        "        print (\"real: \"+str(d_loss_real)+\" , fake: \"+str(d_loss_fake) + \" , Duclidean distance: \"+ str(euclidean_distance))\n",
        "        print ()\n",
        "        print(\"real diabetes values:\")\n",
        "        print(pd.DataFrame(diabetes))\n",
        "        print(\"fake diabetes values:\")\n",
        "        print(pd.DataFrame(gen_diabetes))\n",
        "      '''\n",
        "      \n",
        "      \n",
        "     \n",
        "        \n",
        "      \n",
        "    #And within the same loop we train our Generator, by setting the input noise and\n",
        "    #ultimately training the Generator to have the Discriminator label its samples as valid\n",
        "    #by specifying the gradient loss.\n",
        "      # ---------------------\n",
        "      #  Train Generator\n",
        "      # ---------------------\n",
        "    #Create noise vectors as input for generator. \n",
        "    #Create as many noise vectors as defined by the batch size. \n",
        "    #Based on normal distribution. Output will be of size (batch size, 100)\n",
        "      noise = np.random.normal(0, 1, (batch_size, 8)) \n",
        "\n",
        "      # The generator wants the discriminator to label the generated samples\n",
        "      # as valid (ones)\n",
        "      #This is where the genrator is trying to trick discriminator into believing\n",
        "      #the generated sample is true (hence value of 1 for y)\n",
        "      valid_y = np.array([1] * batch_size) #Creates an array of all ones of size=batch size\n",
        "\n",
        "      # Generator is part of combined where it got directly linked with the discriminator\n",
        "      # Train the generator with noise as x and 1 as y. \n",
        "      # Again, 1 as the output as it is adversarial and if generator did a great\n",
        "      #job of folling the discriminator then the output would be 1 (true)\n",
        "      g_loss = combined.train_on_batch(noise, valid_y)\n",
        "      if (g_loss > 1):\n",
        "        print(\"fake diabetes noise values that fooled the model:\")\n",
        "        print(pd.DataFrame(noise))\n",
        "\n",
        "    #Additionally, in order for us to keep track of our training process, we print the\n",
        "    #progress and save the sample sample output depending on the epoch interval specified.  \n",
        "    #Plot the progress\n",
        "      \n",
        "      print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
        "      \n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-CBB7aUanPzz",
        "outputId": "20cd5c79-58be-4310-d647-044a2828a7c4"
      },
      "source": [
        "optimizer = Adam(0.0002, 0.5)  #Learning rate and momentum.\n",
        "noise_shape = (8,)\n",
        "desired_confidence_shape=(1,)\n",
        "discriminator = build_discriminator(noise_shape,desired_confidence_shape)\n",
        "discriminator.compile(loss='binary_crossentropy',\n",
        "    optimizer=optimizer,\n",
        "    metrics=['accuracy'])\n",
        "\n",
        "#build and compile our Discriminator, pick the loss function\n",
        "\n",
        "#SInce we are only generating (faking) samples, let us not track any metrics.\n",
        "generator = build_generator(noise_shape,desired_confidence_shape)\n",
        "generator.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "\n",
        "##This builds the Generator and defines the input noise. \n",
        "#In a GAN the Generator network takes noise z as an input to produce its samples.  \n",
        "z = Input(shape=(8,))   #Our random input to the generator\n",
        "test_if_diabete = generator(z)\n",
        "\n",
        "#This ensures that when we combine our networks we only train the Generator.\n",
        "#While generator training we do not want discriminator weights to be adjusted. \n",
        "#This Doesn't affect the above descriminator training.     \n",
        "discriminator.trainable = False  \n",
        "\n",
        "#This specifies that our Discriminator will take the samples generated by our Generator\n",
        "#and true dataset and set its output to a parameter called valid, which will indicate\n",
        "#whether the input is real or not.  \n",
        "valid = discriminator(test_if_diabete)  #Validity check on the generated sample\n",
        "\n",
        "\n",
        "#Here we combined the models and also set our loss function and optimizer. \n",
        "#Again, we are only training the generator here. \n",
        "#The ultimate goal here is for the Generator to fool the Discriminator.  \n",
        "# The combined model  (stacked generator and discriminator) takes\n",
        "# noise as input => generates samples => determines validity\n",
        "\n",
        "combined = Model(z, valid)\n",
        "combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "\n",
        "\n",
        "train(epochs=150, batch_size=32, save_interval=10)\n",
        "\n",
        "#Save model for future use to generate fake samples\n",
        "#Not tested yet... make sure right model is being saved..\n",
        "#Compare with GAN4\n",
        "\n",
        "generator.save('generator_model2.h5')  #Test the model on GAN4_predict...\n",
        "#Change epochs back to 30K\n",
        "                \n",
        "#Epochs dictate the number of backward and forward propagations, the batch_size\n",
        "#indicates the number of training samples per backward/forward propagation, and the\n",
        "#sample_interval specifies after how many epochs we call our sample function."
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_16\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_16 (InputLayer)           [(None, 8)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_43 (Dense)                (None, 8)            72          input_16[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "input_17 (InputLayer)           [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_44 (Dense)                (None, 512)          4608        dense_43[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_46 (Dense)                (None, 1)            2           input_17[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_27 (LeakyReLU)      (None, 512)          0           dense_44[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_29 (LeakyReLU)      (None, 1)            0           dense_46[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_45 (Dense)                (None, 64)           32832       leaky_re_lu_27[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dense_47 (Dense)                (None, 1)            2           leaky_re_lu_29[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_28 (LeakyReLU)      (None, 64)           0           dense_45[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_30 (LeakyReLU)      (None, 1)            0           dense_47[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "tf.concat_3 (TFOpLambda)        (None, 65)           0           leaky_re_lu_28[0][0]             \n",
            "                                                                 leaky_re_lu_30[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dense_48 (Dense)                (None, 2)            132         tf.concat_3[0][0]                \n",
            "==================================================================================================\n",
            "Total params: 37,648\n",
            "Trainable params: 37,648\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Model: \"model_19\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_18 (InputLayer)           [(None, 8)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_49 (Dense)                (None, 8)            72          input_18[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_31 (LeakyReLU)      (None, 8)            0           dense_49[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "input_19 (InputLayer)           [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 8)            32          leaky_re_lu_31[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dense_52 (Dense)                (None, 1)            2           input_19[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_50 (Dense)                (None, 64)           576         batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_33 (LeakyReLU)      (None, 1)            0           dense_52[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_32 (LeakyReLU)      (None, 64)           0           dense_50[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 1)            4           leaky_re_lu_33[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 64)           256         leaky_re_lu_32[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dense_53 (Dense)                (None, 1)            2           batch_normalization_17[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dense_51 (Dense)                (None, 512)          33280       batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_34 (LeakyReLU)      (None, 1)            0           dense_53[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 512)          2048        dense_51[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, 1)            4           leaky_re_lu_34[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.concat_4 (TFOpLambda)        (None, 513)          0           batch_normalization_16[0][0]     \n",
            "                                                                 batch_normalization_18[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dense_54 (Dense)                (None, 2)            1028        tf.concat_4[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_55 (Dense)                (None, 1)            3           dense_54[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 37,307\n",
            "Trainable params: 36,135\n",
            "Non-trainable params: 1,172\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-dd0ec8979f20>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m#In a GAN the Generator network takes noise z as an input to produce its samples.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m#Our random input to the generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mtest_if_diabete\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m#This ensures that when we combine our networks we only train the Generator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    944\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_in_functional_construction_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m       return self._functional_construction_call(inputs, args, kwargs,\n\u001b[0;32m--> 946\u001b[0;31m                                                 input_list)\n\u001b[0m\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;31m# Maintains info about the `Layer.call` stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_functional_construction_call\u001b[0;34m(self, inputs, args, kwargs, input_list)\u001b[0m\n\u001b[1;32m   1082\u001b[0m       \u001b[0;31m# Check input assumptions set after layer building, e.g. input shape.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m       outputs = self._keras_tensor_symbolic_call(\n\u001b[0;32m-> 1084\u001b[0;31m           inputs, input_masks, args, kwargs)\n\u001b[0m\u001b[1;32m   1085\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1086\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_keras_tensor_symbolic_call\u001b[0;34m(self, inputs, input_masks, args, kwargs)\u001b[0m\n\u001b[1;32m    814\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeras_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKerasTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_signature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    815\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 816\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_infer_output_signature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_infer_output_signature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_infer_output_signature\u001b[0;34m(self, inputs, args, kwargs, input_masks)\u001b[0m\n\u001b[1;32m    854\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m           \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    414\u001b[0m     \"\"\"\n\u001b[1;32m    415\u001b[0m     return self._run_internal_graph(\n\u001b[0;32m--> 416\u001b[0;31m         inputs, training=training, mask=mask)\n\u001b[0m\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcompute_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py\u001b[0m in \u001b[0;36m_run_internal_graph\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    558\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m       \u001b[0mx_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m       \u001b[0;32massert\u001b[0m \u001b[0mx_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtensor_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Could not compute output '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m       \u001b[0moutput_tensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: Could not compute output KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='dense_55/BiasAdd:0', description=\"created by layer 'dense_55'\")"
          ]
        }
      ]
    }
  ]
}