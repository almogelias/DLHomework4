{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DLHomework4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/almogelias/DLHomework4/blob/main/DLHomework4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCy8CL1baoyJ"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQOanPh61S3T",
        "outputId": "11271bdc-2e4a-474e-d2e8-f53439bd780f"
      },
      "source": [
        "\n",
        "from keras.datasets import mnist\n",
        "from keras.layers import Input, Dense, Reshape, Flatten\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.models import Sequential, Model\n",
        "from keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy.spatial import distance\n",
        "\n",
        "!git clone -s https://github.com/almogelias/DLHomework4.git DLHomework4\n",
        "\n"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'DLHomework4' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmisa9Ktasvz"
      },
      "source": [
        "# Preprocess of diabetes.arff file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wm6DWTA1n9A"
      },
      "source": [
        "\n",
        "from scipy.io import arff\n",
        "import pandas as pd\n",
        "import os\n",
        "from sklearn import preprocessing\n",
        "\n",
        "\n",
        "#Load the data using \"arff.loadarff\" then convert it to dataframe\n",
        "\n",
        "repository_path = os.path.join(os.getcwd(), 'DLHomework4')\n",
        "train_diabetes_path = os.path.join(repository_path, 'diabetes.arff')\n",
        "\n",
        "data = arff.loadarff(train_diabetes_path)\n",
        "df = pd.DataFrame(data[0])\n",
        "\n",
        "y_train = df['class'].replace({b'tested_positive':'1', b'tested_negative':'0'})\n",
        "\n",
        "        \n",
        "# Drop last column of a dataframe\n",
        "x_train = df.iloc[: , :-1]\n",
        "min_max_scaler = preprocessing.MinMaxScaler()\n",
        "x_scaled = min_max_scaler.fit_transform(x_train)\n",
        "x_train = pd.DataFrame(x_scaled)\n",
        "#x_train, x_test, y_train, y_test = train_test_split(x, y,test_size=3)"
      ],
      "execution_count": 215,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PK7Tv9oz1hp7"
      },
      "source": [
        "#Define input dimensions\n",
        "diabetes_dim = x_train.shape[1]\n"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dSkANdxazYE"
      },
      "source": [
        "# Generator Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QEHK-wMh6mjf",
        "outputId": "bbaa2b8b-ca85-42d0-9d60-80a957ae8623"
      },
      "source": [
        "#Given input of noise (latent) vector, the Generator produces an sample.\n",
        "def build_generator():\n",
        "\n",
        "    noise_shape = (8,) #1D array of size 100 (latent vector / noise)\n",
        "\n",
        "#Define your generator network \n",
        "#Here we are only using Dense layers. But network can be complicated based\n",
        "#on the application. For example, you can use VGG for super res. GAN.         \n",
        "\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(diabetes_dim, input_shape=noise_shape))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(Dense(diabetes_dim*diabetes_dim))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(Dense(diabetes_dim*diabetes_dim*diabetes_dim))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    \n",
        "    model.add(Dense(np.prod(diabetes_dim), activation='tanh'))\n",
        "    #model.add(Reshape(diabetes_dim))\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    noise = Input(shape=noise_shape)\n",
        "    img = model(noise)    #Generated sample\n",
        "    \n",
        "    return Model(noise, img)\n",
        "\n",
        "#Alpha — α is a hyperparameter which controls the underlying value to which the\n",
        "#function saturates negatives network inputs.\n",
        "#Momentum — Speed up the training\n",
        "\n",
        "generator = build_generator()"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_162 (Dense)            (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_96 (LeakyReLU)   (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_76 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "dense_163 (Dense)            (None, 64)                576       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_97 (LeakyReLU)   (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_77 (Batc (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "dense_164 (Dense)            (None, 512)               33280     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_98 (LeakyReLU)   (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_78 (Batc (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "dense_165 (Dense)            (None, 8)                 4104      \n",
            "=================================================================\n",
            "Total params: 40,368\n",
            "Trainable params: 39,200\n",
            "Non-trainable params: 1,168\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOStgE9na2bM"
      },
      "source": [
        "#Discriminator Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQCE_DE_805i",
        "outputId": "fd743f0f-6923-4145-ce21-858b8bbb3fbe"
      },
      "source": [
        "\n",
        "#Given an input sample, the Discriminator outputs the likelihood of the sample being real.\n",
        "#Binary classification - true or false (we're calling it validity)\n",
        "\n",
        "def build_discriminator():\n",
        "\n",
        "    diabetes_dim_shape = (diabetes_dim,)\n",
        "    model1 = Sequential()\n",
        "\n",
        "    model1.add(Dense(diabetes_dim, input_shape=diabetes_dim_shape))\n",
        "    model1.add(Dense(diabetes_dim*diabetes_dim*diabetes_dim))\n",
        "    model1.add(LeakyReLU(alpha=0.2))\n",
        "    model1.add(Dense(diabetes_dim*diabetes_dim))\n",
        "    model1.add(LeakyReLU(alpha=0.2))\n",
        "    model1.add(Dense(1, activation='sigmoid'))\n",
        "    model1.summary()\n",
        "\n",
        "    diabetes_input = Input(shape=diabetes_dim)\n",
        "    validity = model1(diabetes_input)\n",
        "\n",
        "    return Model(diabetes_input, validity)\n",
        "#The validity is the Discriminator’s guess of input being real or not.\n",
        "discriminator = build_discriminator()"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_166 (Dense)            (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "dense_167 (Dense)            (None, 512)               4608      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_99 (LeakyReLU)   (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_168 (Dense)            (None, 64)                32832     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_100 (LeakyReLU)  (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_169 (Dense)            (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 37,577\n",
            "Trainable params: 37,577\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9nzYdKFa7o0"
      },
      "source": [
        "#Training flow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djqKY7Y2-y5m"
      },
      "source": [
        "#Now that we have constructed our two models it’s time to pit them against each other.\n",
        "#We do this by defining a training function, loading the data set, re-scaling our training\n",
        "#samples and setting the ground truths. \n",
        "def train(epochs, batch_size=128, save_interval=50):\n",
        "\n",
        "    half_batch = int(batch_size / 2)\n",
        "#We then loop through a number of epochs to train our Discriminator by first selecting\n",
        "#a random batch of samples from our true dataset, generating a set of samples from our\n",
        "#Generator, feeding both set of samples into our Discriminator, and finally setting the\n",
        "#loss parameters for both the real and fake samples, as well as the combined loss. \n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "\n",
        "      # ---------------------\n",
        "      #  Train Discriminator\n",
        "      # ---------------------\n",
        "      \n",
        "      # Select a random half batch of real samples\n",
        "      idx = np.random.randint(0, x_train.shape[0], half_batch)\n",
        "      diabetes = x_train.iloc[idx]\n",
        "\n",
        "\n",
        "      noise = np.random.normal(0, 1, (half_batch, 8))\n",
        "\n",
        "      # Generate a half batch of fake samples\n",
        "      gen_diabetes = generator.predict(noise)\n",
        "\n",
        "      # Train the discriminator on real and fake samples, separately\n",
        "      #Research showed that separate training is more effective. \n",
        "      d_loss_real = discriminator.train_on_batch(diabetes, np.ones((half_batch, 1)))\n",
        "      d_loss_fake = discriminator.train_on_batch(gen_diabetes, np.zeros((half_batch, 1)))\n",
        "    #take average loss from real and fake samples. \n",
        "\n",
        "      d_loss = 0.5 * np.add(d_loss_real, d_loss_fake) \n",
        "      euclidean_distance = distance.euclidean(d_loss_real,d_loss_fake)\n",
        "      '''\n",
        "      if (euclidean_distance > 1.1):\n",
        "        print (\"real: \"+str(d_loss_real)+\" , fake: \"+str(d_loss_fake) + \" , Duclidean distance: \"+ str(euclidean_distance))\n",
        "        print ()\n",
        "        print(\"real diabetes values:\")\n",
        "        print(pd.DataFrame(diabetes))\n",
        "        print(\"fake diabetes values:\")\n",
        "        print(pd.DataFrame(gen_diabetes))\n",
        "      '''\n",
        "      \n",
        "      \n",
        "     \n",
        "        \n",
        "      \n",
        "    #And within the same loop we train our Generator, by setting the input noise and\n",
        "    #ultimately training the Generator to have the Discriminator label its samples as valid\n",
        "    #by specifying the gradient loss.\n",
        "      # ---------------------\n",
        "      #  Train Generator\n",
        "      # ---------------------\n",
        "    #Create noise vectors as input for generator. \n",
        "    #Create as many noise vectors as defined by the batch size. \n",
        "    #Based on normal distribution. Output will be of size (batch size, 100)\n",
        "      noise = np.random.normal(0, 1, (batch_size, 8)) \n",
        "\n",
        "      # The generator wants the discriminator to label the generated samples\n",
        "      # as valid (ones)\n",
        "      #This is where the genrator is trying to trick discriminator into believing\n",
        "      #the generated sample is true (hence value of 1 for y)\n",
        "      valid_y = np.array([1] * batch_size) #Creates an array of all ones of size=batch size\n",
        "\n",
        "      # Generator is part of combined where it got directly linked with the discriminator\n",
        "      # Train the generator with noise as x and 1 as y. \n",
        "      # Again, 1 as the output as it is adversarial and if generator did a great\n",
        "      #job of folling the discriminator then the output would be 1 (true)\n",
        "      g_loss = combined.train_on_batch(noise, valid_y)\n",
        "      if (g_loss > 1):\n",
        "        print(\"fake diabetes noise values that fooled the model:\")\n",
        "        print(pd.DataFrame(noise))\n",
        "\n",
        "    #Additionally, in order for us to keep track of our training process, we print the\n",
        "    #progress and save the sample sample output depending on the epoch interval specified.  \n",
        "    #Plot the progress\n",
        "      \n",
        "      print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
        "      \n"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3T_Mcdy1pBvB",
        "outputId": "ec70548e-39f6-4df2-aa88-682be51961e8"
      },
      "source": [
        "optimizer = Adam(0.0002, 0.5)  #Learning rate and momentum.\n",
        "discriminator = build_discriminator()\n",
        "discriminator.compile(loss='binary_crossentropy',\n",
        "    optimizer=optimizer,\n",
        "    metrics=['accuracy'])\n",
        "\n",
        "#build and compile our Discriminator, pick the loss function\n",
        "\n",
        "#SInce we are only generating (faking) samples, let us not track any metrics.\n",
        "generator = build_generator()\n",
        "generator.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "\n",
        "##This builds the Generator and defines the input noise. \n",
        "#In a GAN the Generator network takes noise z as an input to produce its samples.  \n",
        "z = Input(shape=(8,))   #Our random input to the generator\n",
        "test_if_diabete = generator(z)\n",
        "\n",
        "#This ensures that when we combine our networks we only train the Generator.\n",
        "#While generator training we do not want discriminator weights to be adjusted. \n",
        "#This Doesn't affect the above descriminator training.     \n",
        "discriminator.trainable = False  \n",
        "\n",
        "#This specifies that our Discriminator will take the samples generated by our Generator\n",
        "#and true dataset and set its output to a parameter called valid, which will indicate\n",
        "#whether the input is real or not.  \n",
        "valid = discriminator(test_if_diabete)  #Validity check on the generated sample\n",
        "\n",
        "\n",
        "#Here we combined the models and also set our loss function and optimizer. \n",
        "#Again, we are only training the generator here. \n",
        "#The ultimate goal here is for the Generator to fool the Discriminator.  \n",
        "# The combined model  (stacked generator and discriminator) takes\n",
        "# noise as input => generates samples => determines validity\n",
        "\n",
        "combined = Model(z, valid)\n",
        "combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "\n",
        "\n",
        "train(epochs=150, batch_size=32, save_interval=10)\n",
        "\n",
        "#Save model for future use to generate fake samples\n",
        "#Not tested yet... make sure right model is being saved..\n",
        "#Compare with GAN4\n",
        "\n",
        "generator.save('generator_model.h5')  #Test the model on GAN4_predict...\n",
        "#Change epochs back to 30K\n",
        "                \n",
        "#Epochs dictate the number of backward and forward propagations, the batch_size\n",
        "#indicates the number of training samples per backward/forward propagation, and the\n",
        "#sample_interval specifies after how many epochs we call our sample function."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_8 (Dense)              (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 512)               4608      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_5 (LeakyReLU)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 64)                32832     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_6 (LeakyReLU)    (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 37,577\n",
            "Trainable params: 37,577\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_12 (Dense)             (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_7 (LeakyReLU)    (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 64)                576       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_8 (LeakyReLU)    (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 512)               33280     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_9 (LeakyReLU)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 8)                 4104      \n",
            "=================================================================\n",
            "Total params: 40,368\n",
            "Trainable params: 39,200\n",
            "Non-trainable params: 1,168\n",
            "_________________________________________________________________\n",
            "0 [D loss: 0.713182, acc.: 43.75%] [G loss: 0.717235]\n",
            "1 [D loss: 0.701918, acc.: 37.50%] [G loss: 0.722402]\n",
            "2 [D loss: 0.695901, acc.: 37.50%] [G loss: 0.708318]\n",
            "3 [D loss: 0.681725, acc.: 78.12%] [G loss: 0.709501]\n",
            "4 [D loss: 0.681348, acc.: 68.75%] [G loss: 0.701680]\n",
            "5 [D loss: 0.672858, acc.: 68.75%] [G loss: 0.708896]\n",
            "6 [D loss: 0.654756, acc.: 87.50%] [G loss: 0.712621]\n",
            "7 [D loss: 0.653781, acc.: 75.00%] [G loss: 0.719017]\n",
            "8 [D loss: 0.644538, acc.: 75.00%] [G loss: 0.732520]\n",
            "9 [D loss: 0.659934, acc.: 68.75%] [G loss: 0.720823]\n",
            "10 [D loss: 0.645750, acc.: 81.25%] [G loss: 0.731910]\n",
            "11 [D loss: 0.639133, acc.: 71.88%] [G loss: 0.715424]\n",
            "12 [D loss: 0.600150, acc.: 81.25%] [G loss: 0.731424]\n",
            "13 [D loss: 0.599875, acc.: 81.25%] [G loss: 0.739843]\n",
            "14 [D loss: 0.597067, acc.: 81.25%] [G loss: 0.726657]\n",
            "15 [D loss: 0.621358, acc.: 78.12%] [G loss: 0.703180]\n",
            "16 [D loss: 0.611595, acc.: 75.00%] [G loss: 0.705067]\n",
            "17 [D loss: 0.599653, acc.: 78.12%] [G loss: 0.724347]\n",
            "18 [D loss: 0.602617, acc.: 75.00%] [G loss: 0.753101]\n",
            "19 [D loss: 0.689938, acc.: 62.50%] [G loss: 0.719045]\n",
            "20 [D loss: 0.613107, acc.: 71.88%] [G loss: 0.746607]\n",
            "21 [D loss: 0.578439, acc.: 78.12%] [G loss: 0.749813]\n",
            "22 [D loss: 0.632183, acc.: 71.88%] [G loss: 0.743550]\n",
            "23 [D loss: 0.631443, acc.: 68.75%] [G loss: 0.735439]\n",
            "24 [D loss: 0.647027, acc.: 62.50%] [G loss: 0.735495]\n",
            "25 [D loss: 0.659351, acc.: 68.75%] [G loss: 0.798428]\n",
            "26 [D loss: 0.578792, acc.: 78.12%] [G loss: 0.789238]\n",
            "27 [D loss: 0.660506, acc.: 71.88%] [G loss: 0.838808]\n",
            "28 [D loss: 0.620551, acc.: 78.12%] [G loss: 0.784015]\n",
            "29 [D loss: 0.577680, acc.: 81.25%] [G loss: 0.764543]\n",
            "30 [D loss: 0.645158, acc.: 75.00%] [G loss: 0.820986]\n",
            "31 [D loss: 0.616468, acc.: 71.88%] [G loss: 0.795563]\n",
            "32 [D loss: 0.623566, acc.: 75.00%] [G loss: 0.822447]\n",
            "33 [D loss: 0.602699, acc.: 78.12%] [G loss: 0.758897]\n",
            "34 [D loss: 0.670484, acc.: 62.50%] [G loss: 0.772951]\n",
            "35 [D loss: 0.600049, acc.: 87.50%] [G loss: 0.777649]\n",
            "36 [D loss: 0.656614, acc.: 65.62%] [G loss: 0.823080]\n",
            "37 [D loss: 0.623611, acc.: 75.00%] [G loss: 0.779205]\n",
            "38 [D loss: 0.611383, acc.: 71.88%] [G loss: 0.807948]\n",
            "39 [D loss: 0.652834, acc.: 56.25%] [G loss: 0.802630]\n",
            "40 [D loss: 0.651670, acc.: 62.50%] [G loss: 0.839414]\n",
            "41 [D loss: 0.624797, acc.: 75.00%] [G loss: 0.817614]\n",
            "42 [D loss: 0.632677, acc.: 68.75%] [G loss: 0.825714]\n",
            "43 [D loss: 0.561537, acc.: 87.50%] [G loss: 0.852039]\n",
            "44 [D loss: 0.574288, acc.: 78.12%] [G loss: 0.872415]\n",
            "45 [D loss: 0.599361, acc.: 78.12%] [G loss: 0.798759]\n",
            "46 [D loss: 0.616869, acc.: 78.12%] [G loss: 0.801488]\n",
            "47 [D loss: 0.626442, acc.: 71.88%] [G loss: 0.830224]\n",
            "48 [D loss: 0.558849, acc.: 81.25%] [G loss: 0.813842]\n",
            "49 [D loss: 0.613145, acc.: 75.00%] [G loss: 0.867921]\n",
            "50 [D loss: 0.614148, acc.: 71.88%] [G loss: 0.837226]\n",
            "51 [D loss: 0.666057, acc.: 65.62%] [G loss: 0.868817]\n",
            "52 [D loss: 0.623896, acc.: 68.75%] [G loss: 0.810032]\n",
            "53 [D loss: 0.627400, acc.: 75.00%] [G loss: 0.900477]\n",
            "54 [D loss: 0.676603, acc.: 65.62%] [G loss: 0.806702]\n",
            "55 [D loss: 0.575392, acc.: 81.25%] [G loss: 0.842631]\n",
            "56 [D loss: 0.586122, acc.: 81.25%] [G loss: 0.890691]\n",
            "57 [D loss: 0.676302, acc.: 68.75%] [G loss: 0.916163]\n",
            "58 [D loss: 0.654969, acc.: 65.62%] [G loss: 0.822025]\n",
            "59 [D loss: 0.617982, acc.: 71.88%] [G loss: 0.883694]\n",
            "60 [D loss: 0.643791, acc.: 71.88%] [G loss: 0.878131]\n",
            "61 [D loss: 0.625126, acc.: 71.88%] [G loss: 0.853939]\n",
            "62 [D loss: 0.627323, acc.: 65.62%] [G loss: 0.890557]\n",
            "63 [D loss: 0.676631, acc.: 62.50%] [G loss: 0.906983]\n",
            "64 [D loss: 0.654066, acc.: 56.25%] [G loss: 0.909834]\n",
            "65 [D loss: 0.636667, acc.: 68.75%] [G loss: 0.863477]\n",
            "66 [D loss: 0.687360, acc.: 62.50%] [G loss: 0.871678]\n",
            "67 [D loss: 0.645870, acc.: 62.50%] [G loss: 0.954550]\n",
            "68 [D loss: 0.628049, acc.: 75.00%] [G loss: 0.909766]\n",
            "69 [D loss: 0.601682, acc.: 81.25%] [G loss: 0.885039]\n",
            "70 [D loss: 0.583274, acc.: 90.62%] [G loss: 0.918137]\n",
            "71 [D loss: 0.646190, acc.: 59.38%] [G loss: 0.948631]\n",
            "72 [D loss: 0.626889, acc.: 75.00%] [G loss: 0.918438]\n",
            "73 [D loss: 0.650336, acc.: 68.75%] [G loss: 0.936003]\n",
            "74 [D loss: 0.624626, acc.: 78.12%] [G loss: 0.894882]\n",
            "75 [D loss: 0.637833, acc.: 68.75%] [G loss: 0.915823]\n",
            "76 [D loss: 0.640760, acc.: 68.75%] [G loss: 0.922099]\n",
            "77 [D loss: 0.633402, acc.: 62.50%] [G loss: 0.919044]\n",
            "78 [D loss: 0.617930, acc.: 71.88%] [G loss: 0.918476]\n",
            "79 [D loss: 0.613546, acc.: 68.75%] [G loss: 0.853992]\n",
            "80 [D loss: 0.635807, acc.: 62.50%] [G loss: 0.866475]\n",
            "81 [D loss: 0.609369, acc.: 68.75%] [G loss: 0.873520]\n",
            "82 [D loss: 0.639813, acc.: 65.62%] [G loss: 0.990440]\n",
            "83 [D loss: 0.601252, acc.: 75.00%] [G loss: 0.830755]\n",
            "84 [D loss: 0.606113, acc.: 68.75%] [G loss: 0.869929]\n",
            "85 [D loss: 0.581413, acc.: 71.88%] [G loss: 0.844903]\n",
            "86 [D loss: 0.570996, acc.: 81.25%] [G loss: 0.893858]\n",
            "87 [D loss: 0.643350, acc.: 78.12%] [G loss: 0.937517]\n",
            "88 [D loss: 0.649545, acc.: 68.75%] [G loss: 0.914549]\n",
            "89 [D loss: 0.630448, acc.: 71.88%] [G loss: 0.869512]\n",
            "90 [D loss: 0.628867, acc.: 71.88%] [G loss: 0.929744]\n",
            "91 [D loss: 0.605972, acc.: 68.75%] [G loss: 0.935548]\n",
            "92 [D loss: 0.618065, acc.: 62.50%] [G loss: 0.905510]\n",
            "93 [D loss: 0.584312, acc.: 81.25%] [G loss: 0.915456]\n",
            "94 [D loss: 0.666123, acc.: 46.88%] [G loss: 0.898007]\n",
            "95 [D loss: 0.628758, acc.: 62.50%] [G loss: 0.865665]\n",
            "96 [D loss: 0.598021, acc.: 75.00%] [G loss: 0.890770]\n",
            "97 [D loss: 0.591256, acc.: 84.38%] [G loss: 0.955416]\n",
            "98 [D loss: 0.630548, acc.: 65.62%] [G loss: 0.923004]\n",
            "99 [D loss: 0.581539, acc.: 75.00%] [G loss: 0.934111]\n",
            "100 [D loss: 0.657741, acc.: 65.62%] [G loss: 0.866493]\n",
            "101 [D loss: 0.592287, acc.: 75.00%] [G loss: 0.900547]\n",
            "102 [D loss: 0.649161, acc.: 62.50%] [G loss: 0.936926]\n",
            "103 [D loss: 0.627315, acc.: 71.88%] [G loss: 0.992658]\n",
            "104 [D loss: 0.675830, acc.: 50.00%] [G loss: 0.946102]\n",
            "105 [D loss: 0.636568, acc.: 71.88%] [G loss: 0.976979]\n",
            "106 [D loss: 0.647278, acc.: 62.50%] [G loss: 0.971116]\n",
            "107 [D loss: 0.600492, acc.: 71.88%] [G loss: 0.919736]\n",
            "108 [D loss: 0.668165, acc.: 59.38%] [G loss: 0.999280]\n",
            "109 [D loss: 0.647425, acc.: 75.00%] [G loss: 0.906533]\n",
            "110 [D loss: 0.619984, acc.: 68.75%] [G loss: 0.948145]\n",
            "111 [D loss: 0.527353, acc.: 90.62%] [G loss: 0.959158]\n",
            "112 [D loss: 0.547559, acc.: 71.88%] [G loss: 0.947540]\n",
            "113 [D loss: 0.634303, acc.: 65.62%] [G loss: 0.917160]\n",
            "114 [D loss: 0.638690, acc.: 59.38%] [G loss: 0.900476]\n",
            "115 [D loss: 0.608444, acc.: 71.88%] [G loss: 0.962212]\n",
            "116 [D loss: 0.631139, acc.: 65.62%] [G loss: 0.964381]\n",
            "117 [D loss: 0.612160, acc.: 81.25%] [G loss: 0.945721]\n",
            "118 [D loss: 0.626749, acc.: 68.75%] [G loss: 0.945707]\n",
            "119 [D loss: 0.561452, acc.: 78.12%] [G loss: 0.947121]\n",
            "120 [D loss: 0.657294, acc.: 59.38%] [G loss: 0.894182]\n",
            "121 [D loss: 0.605551, acc.: 71.88%] [G loss: 0.855162]\n",
            "122 [D loss: 0.617590, acc.: 68.75%] [G loss: 0.936952]\n",
            "123 [D loss: 0.616741, acc.: 75.00%] [G loss: 0.960028]\n",
            "fake diabetes noise values that fooled the model:\n",
            "           0         1         2  ...         5         6         7\n",
            "0  -1.534876  0.555888 -2.054606  ...  2.195950 -0.492233  0.546643\n",
            "1   0.282304 -0.462034  0.303926  ...  0.200589  0.857353  0.313716\n",
            "2   0.977103 -0.080701  0.615030  ...  0.448682 -0.233912 -2.470960\n",
            "3  -0.540224  1.444436  1.516151  ...  1.210901  1.632250 -0.004097\n",
            "4   0.239933 -1.569075  1.460343  ... -0.618344 -0.868502 -0.045509\n",
            "5   0.840995  1.207559  1.664586  ... -0.459090 -1.205809  0.356578\n",
            "6   1.638663  0.099192 -0.132941  ...  0.799098 -0.311142  0.819023\n",
            "7   2.207523  0.018010  0.132863  ... -1.528473  1.275174  0.317134\n",
            "8   1.190206  0.664979 -0.330567  ... -0.562818  2.740423 -0.386150\n",
            "9  -0.235459  1.545937 -0.564912  ...  1.010344  0.233217  0.709015\n",
            "10  1.630138 -0.484436 -2.276566  ...  0.329181  0.543491  0.186327\n",
            "11 -1.456326 -0.735213 -1.864609  ... -0.647058  0.044962  1.809152\n",
            "12 -0.143588 -0.259079  0.081066  ...  0.764915  0.846051 -1.136801\n",
            "13  1.886388  0.361632  1.276979  ...  2.604792  1.111344 -0.317763\n",
            "14  0.609207 -0.744445 -0.546992  ... -0.370146  0.331414 -0.323463\n",
            "15  0.336541 -0.241094  2.281076  ... -2.086218 -0.887107 -0.982055\n",
            "16  0.555854  1.811474 -1.307656  ...  1.434013  0.370947  0.594522\n",
            "17  0.492481  0.400467  0.271095  ...  1.344133 -0.128507  0.581915\n",
            "18  1.082523  0.514215  0.085662  ... -0.630288  1.150897 -0.582313\n",
            "19  0.388774 -0.624999  0.096960  ... -0.042241 -0.099116  0.347417\n",
            "20  0.984838  0.799433  0.785132  ...  0.861749  0.719226 -0.396155\n",
            "21 -0.651224 -2.414800  0.559426  ...  0.196213 -1.187671  1.472094\n",
            "22 -1.069288  0.138587  0.904965  ... -0.305758  0.210057  0.182195\n",
            "23  1.778503 -1.824510 -0.832181  ... -0.593419  1.192520 -0.659200\n",
            "24  0.751951 -0.641832 -1.558679  ...  0.088015 -0.862608  0.743655\n",
            "25 -0.174982 -1.950440  1.496989  ... -0.529650  0.624989  0.823377\n",
            "26  0.157861 -1.888325 -0.140225  ...  1.175346 -0.315045 -1.250688\n",
            "27  0.695804 -0.567641 -0.692103  ... -1.133972  0.673755 -0.186871\n",
            "28 -2.192489  0.467561  1.434995  ... -1.162650  0.211438  0.209470\n",
            "29  0.686515 -0.517613 -1.562917  ...  0.796119 -0.485198  0.758675\n",
            "30 -0.722327 -1.248378 -0.997453  ...  0.111280 -1.598552  0.056599\n",
            "31 -0.044438 -1.389343  1.073932  ... -0.683076  2.558891  0.590059\n",
            "\n",
            "[32 rows x 8 columns]\n",
            "124 [D loss: 0.675900, acc.: 59.38%] [G loss: 1.003250]\n",
            "125 [D loss: 0.676686, acc.: 46.88%] [G loss: 0.924133]\n",
            "126 [D loss: 0.642107, acc.: 68.75%] [G loss: 0.885778]\n",
            "127 [D loss: 0.609827, acc.: 78.12%] [G loss: 0.849933]\n",
            "128 [D loss: 0.594668, acc.: 81.25%] [G loss: 0.972095]\n",
            "129 [D loss: 0.653743, acc.: 53.12%] [G loss: 0.941004]\n",
            "130 [D loss: 0.610310, acc.: 81.25%] [G loss: 0.914369]\n",
            "131 [D loss: 0.596609, acc.: 68.75%] [G loss: 0.906007]\n",
            "132 [D loss: 0.669625, acc.: 62.50%] [G loss: 0.906353]\n",
            "133 [D loss: 0.670034, acc.: 65.62%] [G loss: 0.893851]\n",
            "134 [D loss: 0.675010, acc.: 53.12%] [G loss: 0.917030]\n",
            "135 [D loss: 0.626829, acc.: 71.88%] [G loss: 0.901763]\n",
            "136 [D loss: 0.594007, acc.: 75.00%] [G loss: 0.940239]\n",
            "137 [D loss: 0.656667, acc.: 84.38%] [G loss: 0.886078]\n",
            "138 [D loss: 0.639638, acc.: 62.50%] [G loss: 0.855871]\n",
            "139 [D loss: 0.653661, acc.: 62.50%] [G loss: 0.887825]\n",
            "140 [D loss: 0.630249, acc.: 71.88%] [G loss: 0.872599]\n",
            "141 [D loss: 0.653377, acc.: 53.12%] [G loss: 0.902620]\n",
            "142 [D loss: 0.652099, acc.: 62.50%] [G loss: 0.906271]\n",
            "143 [D loss: 0.681182, acc.: 65.62%] [G loss: 0.900712]\n",
            "144 [D loss: 0.684187, acc.: 65.62%] [G loss: 0.889217]\n",
            "145 [D loss: 0.640061, acc.: 75.00%] [G loss: 0.941490]\n",
            "146 [D loss: 0.635552, acc.: 71.88%] [G loss: 0.837742]\n",
            "147 [D loss: 0.663838, acc.: 65.62%] [G loss: 0.876946]\n",
            "148 [D loss: 0.655551, acc.: 65.62%] [G loss: 0.871365]\n",
            "149 [D loss: 0.628945, acc.: 68.75%] [G loss: 0.880563]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bHU4fDX1_vC"
      },
      "source": [
        "# German Credit DB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "id": "UmmiPqzE1_Qj",
        "outputId": "94644ea0-e538-42bd-c603-3ea4d3d740d6"
      },
      "source": [
        "'''\n",
        "\n",
        "#Load the data using \"arff.loadarff\" then convert it to dataframe\n",
        "\n",
        "repository_path = os.path.join(os.getcwd(), 'DLHomework4')\n",
        "train_german_credit_path = os.path.join(repository_path, 'german_credit.arff')\n",
        "\n",
        "data = arff.loadarff(train_german_credit_path)\n",
        "df = pd.DataFrame(data[0])\n",
        "\n",
        "y_train = df['21'].replace({b'1':'1', b'2':'2'})\n",
        "\n",
        "        \n",
        "# Drop last column of a dataframe\n",
        "x_train = df.iloc[: , :-1]\n",
        "min_max_scaler = preprocessing.MinMaxScaler()\n",
        "x_scaled = min_max_scaler.fit_transform(x_train)\n",
        "x_train = pd.DataFrame(x_scaled)\n",
        "#x_train, x_test, y_train, y_test = train_test_split(x, y,test_size=3)\n",
        "'''"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n\\n#Load the data using \"arff.loadarff\" then convert it to dataframe\\n\\nrepository_path = os.path.join(os.getcwd(), \\'DLHomework4\\')\\ntrain_german_credit_path = os.path.join(repository_path, \\'german_credit.arff\\')\\n\\ndata = arff.loadarff(train_german_credit_path)\\ndf = pd.DataFrame(data[0])\\n\\ny_train = df[\\'21\\'].replace({b\\'1\\':\\'1\\', b\\'2\\':\\'2\\'})\\n\\n        \\n# Drop last column of a dataframe\\nx_train = df.iloc[: , :-1]\\nmin_max_scaler = preprocessing.MinMaxScaler()\\nx_scaled = min_max_scaler.fit_transform(x_train)\\nx_train = pd.DataFrame(x_scaled)\\n#x_train, x_test, y_train, y_test = train_test_split(x, y,test_size=3)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODzEaMEkbcFs"
      },
      "source": [
        "# Random Forest model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DjteC-kSbfnk",
        "outputId": "3830534f-ff34-44dd-8eef-b013ed0007bf"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "Xtrain, Xtest, ytrain, ytest = train_test_split(x_train, y_train,\n",
        "                                                 test_size=0.33, random_state=42)\n",
        "\n",
        "def BB_Model(sample_Xtrain,sample_ytrain):\n",
        "  model = RandomForestClassifier(n_estimators=100, random_state=0)\n",
        "  \n",
        "  model = RandomForestClassifier(n_estimators=1000)\n",
        "  model.fit(Xtrain, ytrain)\n",
        "  return model\n",
        "\n",
        "bb_model=BB_Model(Xtrain,ytrain)\n",
        "ypred = bb_model.predict(Xtest)\n",
        "accuracy_score(ytest, ypred)"
      ],
      "execution_count": 216,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7559055118110236"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 216
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgb_3lHA-Ena"
      },
      "source": [
        "# BUILD new generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xyrOPdW5-DpI",
        "outputId": "bc5be586-9a41-425e-e633-62efe9750c94"
      },
      "source": [
        "from tensorflow.keras.layers import concatenate\n",
        "from tensorflow.keras import activations\n",
        "\n",
        "#Given input of noise (latent) vector, the Generator produces an sample.\n",
        "def build_generator(noise_shape, desired_confidence_shape):\n",
        "\n",
        "    input_noise = Input(shape=noise_shape)\n",
        "    input_confidence = Input(shape=desired_confidence_shape)\n",
        "\n",
        "#Define your generator network \n",
        "#Here we are only using Dense layers. But network can be complicated based\n",
        "#on the application. For example, you can use VGG for super res. GAN.         \n",
        "\n",
        "    x = Dense(noise_shape[0])(input_noise)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    x = BatchNormalization(momentum=0.2)(x)\n",
        "    x = Dense(noise_shape[0]*noise_shape[0])(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    x = BatchNormalization(momentum=0.2)(x)\n",
        "    x = Dense(noise_shape[0]*noise_shape[0]*noise_shape[0])(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    x = BatchNormalization(momentum=0.2)(x)\n",
        "    x = Dense(np.prod(noise_shape[0]))(x)\n",
        "    x = activations.tanh(x)\n",
        "    x = Model(inputs=input_noise, outputs=x)\n",
        "    \n",
        "\n",
        "    y = Dense(desired_confidence_shape[0])(input_confidence)\n",
        "    y = LeakyReLU(alpha=0.2)(y)\n",
        "    y = BatchNormalization(momentum=0.2)(y)\n",
        "    y = Dense(desired_confidence_shape[0]*desired_confidence_shape[0])(y)\n",
        "    y = LeakyReLU(alpha=0.2)(y)\n",
        "    y = BatchNormalization(momentum=0.2)(y)\n",
        "    y = Model(inputs=input_confidence, outputs=y)\n",
        "\n",
        "    combined = concatenate([x.output, y.output])\n",
        "\n",
        "    z = Dense(2, activation=\"relu\")(combined)\n",
        "    z = Dense(1, activation=\"linear\")(z)\n",
        "    outputs=Dense(8)(z)\n",
        "\n",
        "    model = Model(inputs=(x.input, y.input), outputs=outputs)\n",
        "    model.summary()\n",
        "    \n",
        "    return model\n",
        "\n",
        "#Alpha — α is a hyperparameter which controls the underlying value to which the\n",
        "#function saturates negatives network inputs.\n",
        "#Momentum — Speed up the training\n",
        "noise_shape = (8,)\n",
        "desired_confidence_shape=(1,)\n",
        "generator = build_generator(noise_shape,desired_confidence_shape)\n",
        "generator.compile(loss='binary_crossentropy', optimizer=optimizer)\n"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_66\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_57 (InputLayer)           [(None, 8)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_178 (Dense)               (None, 8)            72          input_57[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_107 (LeakyReLU)     (None, 8)            0           dense_178[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_85 (BatchNo (None, 8)            32          leaky_re_lu_107[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense_179 (Dense)               (None, 64)           576         batch_normalization_85[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_108 (LeakyReLU)     (None, 64)           0           dense_179[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "input_58 (InputLayer)           [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_86 (BatchNo (None, 64)           256         leaky_re_lu_108[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense_182 (Dense)               (None, 1)            2           input_58[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_180 (Dense)               (None, 512)          33280       batch_normalization_86[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_110 (LeakyReLU)     (None, 1)            0           dense_182[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_109 (LeakyReLU)     (None, 512)          0           dense_180[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_88 (BatchNo (None, 1)            4           leaky_re_lu_110[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_87 (BatchNo (None, 512)          2048        leaky_re_lu_109[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense_183 (Dense)               (None, 1)            2           batch_normalization_88[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dense_181 (Dense)               (None, 8)            4104        batch_normalization_87[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_111 (LeakyReLU)     (None, 1)            0           dense_183[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.tanh_14 (TFOpLambda)    (None, 8)            0           dense_181[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_89 (BatchNo (None, 1)            4           leaky_re_lu_111[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "tf.concat_16 (TFOpLambda)       (None, 9)            0           tf.math.tanh_14[0][0]            \n",
            "                                                                 batch_normalization_89[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dense_184 (Dense)               (None, 2)            20          tf.concat_16[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dense_185 (Dense)               (None, 1)            3           dense_184[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_186 (Dense)               (None, 8)            16          dense_185[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 40,419\n",
            "Trainable params: 39,247\n",
            "Non-trainable params: 1,172\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vitSixKHneuR",
        "outputId": "9d4bbec4-cebd-4eb2-cf07-cf0db4d05998"
      },
      "source": [
        "\n",
        "#Given an input sample, the Discriminator outputs the likelihood of the sample being real.\n",
        "#Binary classification - true or false (we're calling it validity)\n",
        "\n",
        "def build_discriminator(sample_shape, desired_confidence_shape, BB_output_shape):\n",
        "\n",
        "    \n",
        "    \n",
        "    if (type(sample_shape)=='keras.engine.keras_tensor.KerasTensor'):\n",
        "      input_sample = Input(shape=sample_shape[1])\n",
        "      input_confidence = Input(shape=desired_confidence_shape[1])\n",
        "      input_BB_output_shape = Input(shape=BB_output_shape[1])\n",
        "      x = Dense(input_sample[0])(input_sample)\n",
        "      x = Dense(input_sample[0]*input_sample[0]*input_sample[0])(x)\n",
        "      x = LeakyReLU(alpha=0.2)(x)\n",
        "      x = Dense(input_sample[0]*input_sample[0])(x)\n",
        "      x = LeakyReLU(alpha=0.2)(x)\n",
        "      x = Model(inputs=sample_shape, outputs=x)\n",
        "\n",
        "      z = Dense(sample_shape[0])(input_sample)\n",
        "      z = Dense(sample_shape[0]*sample_shape[0])(z)\n",
        "      z = LeakyReLU(alpha=0.2)(z)\n",
        "      z = Dense(sample_shape[0]*sample_shape[0])(z)\n",
        "      z = LeakyReLU(alpha=0.2)(z)\n",
        "      z = Model(inputs=input_sample, outputs=z)\n",
        "\n",
        "      y = Dense(input_confidence[0])(input_confidence)\n",
        "      y = LeakyReLU(alpha=0.2)(y)\n",
        "      y = Dense(input_confidence[0]*input_confidence[0])(y)\n",
        "      y = LeakyReLU(alpha=0.2)(y)\n",
        "      y = Model(inputs=input_confidence, outputs=y)\n",
        "\n",
        "      \n",
        "      combined = concatenate([x.output,z.output, y.output])\n",
        "\n",
        "      \n",
        "\n",
        "      o = Dense(2, activation=\"sigmoid\")(combined)\n",
        "      \n",
        "      model = Model(inputs=[x.input, y.input], outputs=o)\n",
        "      model.summary()\n",
        "      \n",
        "      return model\n",
        "\n",
        "    else:\n",
        "      input_sample = Input(shape=sample_shape)\n",
        "      input_confidence = Input(shape=desired_confidence_shape)\n",
        "      input_BB_output_shape = Input(shape=BB_output_shape)\n",
        "#Define your generator network \n",
        "#Here we are only using Dense layers. But network can be complicated based\n",
        "#on the application. For example, you can use VGG for super res. GAN.         \n",
        "\n",
        "      x = Dense(sample_shape[0])(input_sample)\n",
        "      x = Dense(sample_shape[0]*sample_shape[0]*sample_shape[0])(x)\n",
        "      x = LeakyReLU(alpha=0.2)(x)\n",
        "      x = Dense(sample_shape[0]*sample_shape[0])(x)\n",
        "      x = LeakyReLU(alpha=0.2)(x)\n",
        "      x = Model(inputs=input_sample, outputs=x)\n",
        "\n",
        "      z = Dense(sample_shape[0])(input_sample)\n",
        "      z = Dense(sample_shape[0]*sample_shape[0])(z)\n",
        "      z = LeakyReLU(alpha=0.2)(z)\n",
        "      z = Dense(sample_shape[0]*sample_shape[0])(z)\n",
        "      z = LeakyReLU(alpha=0.2)(z)\n",
        "      z = Model(inputs=input_sample, outputs=z)\n",
        "\n",
        "\n",
        "      y = Dense(desired_confidence_shape[0])(input_confidence)\n",
        "      y = LeakyReLU(alpha=0.2)(y)\n",
        "      y = Dense(desired_confidence_shape[0]*desired_confidence_shape[0])(y)\n",
        "      y = LeakyReLU(alpha=0.2)(y)\n",
        "      y = Model(inputs=input_confidence, outputs=y)\n",
        "\n",
        "      combined = concatenate([x.output,z.output, y.output])\n",
        "\n",
        "      o = Dense(2, activation=\"sigmoid\")(combined)\n",
        "      \n",
        "      model = Model(inputs=[x.input, y.input], outputs=o)\n",
        "      model.summary()\n",
        "      \n",
        "      return model\n",
        "\n",
        "    #return Model(diabetes_input, validity)\n",
        "#The validity is the Discriminator’s guess of input being real or not.\n",
        "noise_shape = (8,)\n",
        "desired_confidence_shape=(1,)\n",
        "desired_confidence_shape=(500,)\n",
        "discriminator = build_discriminator(noise_shape,desired_confidence_shape,desired_confidence_shape)"
      ],
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_118\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_97 (InputLayer)           [(None, 8)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_291 (Dense)               (None, 8)            72          input_97[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_294 (Dense)               (None, 8)            72          input_97[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "input_98 (InputLayer)           [(None, 500)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_292 (Dense)               (None, 512)          4608        dense_291[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_295 (Dense)               (None, 64)           576         dense_294[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_297 (Dense)               (None, 500)          250500      input_98[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_178 (LeakyReLU)     (None, 512)          0           dense_292[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_180 (LeakyReLU)     (None, 64)           0           dense_295[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_182 (LeakyReLU)     (None, 500)          0           dense_297[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_293 (Dense)               (None, 64)           32832       leaky_re_lu_178[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense_296 (Dense)               (None, 64)           4160        leaky_re_lu_180[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense_298 (Dense)               (None, 250000)       125250000   leaky_re_lu_182[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_179 (LeakyReLU)     (None, 64)           0           dense_293[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_181 (LeakyReLU)     (None, 64)           0           dense_296[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_183 (LeakyReLU)     (None, 250000)       0           dense_298[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf.concat_30 (TFOpLambda)       (None, 250128)       0           leaky_re_lu_179[0][0]            \n",
            "                                                                 leaky_re_lu_181[0][0]            \n",
            "                                                                 leaky_re_lu_183[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense_299 (Dense)               (None, 2)            500258      tf.concat_30[0][0]               \n",
            "==================================================================================================\n",
            "Total params: 126,043,078\n",
            "Trainable params: 126,043,078\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjdHV0fI5VEY"
      },
      "source": [
        "#Now that we have constructed our two models it’s time to pit them against each other.\n",
        "#We do this by defining a training function, loading the data set, re-scaling our training\n",
        "#samples and setting the ground truths. \n",
        "def train(epochs, batch_size=128, save_interval=50):\n",
        "\n",
        "    half_batch = int(batch_size / 2)\n",
        "#We then loop through a number of epochs to train our Discriminator by first selecting\n",
        "#a random batch of samples from our true dataset, generating a set of samples from our\n",
        "#Generator, feeding both set of samples into our Discriminator, and finally setting the\n",
        "#loss parameters for both the real and fake samples, as well as the combined loss. \n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "\n",
        "      # ---------------------\n",
        "      #  Train Discriminator\n",
        "      # ---------------------\n",
        "      '''\n",
        "      # Select a random half batch of real samples\n",
        "\n",
        "      idx = np.random.randint(0, Xtrain.shape[0], half_batch)\n",
        "      diabetes = Xtrain.iloc[idx]\n",
        "\n",
        "      '''\n",
        "   \n",
        "\n",
        "      noise = np.random.normal(0, 1, (half_batch, 8))\n",
        "      scalar = np.random.normal(0, 1, (half_batch, 8))\n",
        "      # Generate a half batch of fake samples\n",
        "      gen_diabetes = generator.predict((noise,scalar))\n",
        "\n",
        "      # Train the discriminator on real and fake samples, separately\n",
        "      #Research showed that separate training is more effective. \n",
        "      d_loss_one = discriminator.train_on_batch(gen_diabetes, np.ones((half_batch, 1)))\n",
        "      d_loss_zero = discriminator.train_on_batch(gen_diabetes, np.zeros((half_batch, 1)))\n",
        "    #take average loss from real and fake samples. \n",
        "\n",
        "      d_loss = 0.5 * np.add(d_loss_real, d_loss_fake) \n",
        "      euclidean_distance = distance.euclidean(d_loss_real,d_loss_fake)\n",
        "      '''\n",
        "      if (euclidean_distance > 1.1):\n",
        "        print (\"real: \"+str(d_loss_real)+\" , fake: \"+str(d_loss_fake) + \" , Duclidean distance: \"+ str(euclidean_distance))\n",
        "        print ()\n",
        "        print(\"real diabetes values:\")\n",
        "        print(pd.DataFrame(diabetes))\n",
        "        print(\"fake diabetes values:\")\n",
        "        print(pd.DataFrame(gen_diabetes))\n",
        "      '''\n",
        "      \n",
        "      \n",
        "     \n",
        "        \n",
        "      \n",
        "    #And within the same loop we train our Generator, by setting the input noise and\n",
        "    #ultimately training the Generator to have the Discriminator label its samples as valid\n",
        "    #by specifying the gradient loss.\n",
        "      # ---------------------\n",
        "      #  Train Generator\n",
        "      # ---------------------\n",
        "    #Create noise vectors as input for generator. \n",
        "    #Create as many noise vectors as defined by the batch size. \n",
        "    #Based on normal distribution. Output will be of size (batch size, 100)\n",
        "      noise = np.random.normal(0, 1, (batch_size, 8)) \n",
        "\n",
        "      # The generator wants the discriminator to label the generated samples\n",
        "      # as valid (ones)\n",
        "      #This is where the genrator is trying to trick discriminator into believing\n",
        "      #the generated sample is true (hence value of 1 for y)\n",
        "      valid_y = np.array([1] * batch_size) #Creates an array of all ones of size=batch size\n",
        "\n",
        "      # Generator is part of combined where it got directly linked with the discriminator\n",
        "      # Train the generator with noise as x and 1 as y. \n",
        "      # Again, 1 as the output as it is adversarial and if generator did a great\n",
        "      #job of folling the discriminator then the output would be 1 (true)\n",
        "      g_loss = combined.train_on_batch(noise, valid_y)\n",
        "      if (g_loss > 1):\n",
        "        print(\"fake diabetes noise values that fooled the model:\")\n",
        "        print(pd.DataFrame(noise))\n",
        "\n",
        "    #Additionally, in order for us to keep track of our training process, we print the\n",
        "    #progress and save the sample sample output depending on the epoch interval specified.  \n",
        "    #Plot the progress\n",
        "      \n",
        "      print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
        "      \n"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-CBB7aUanPzz",
        "outputId": "bd4de2d7-3a8b-4ad0-d186-dcb663d61d58"
      },
      "source": [
        "optimizer = Adam(0.0002, 0.5)  #Learning rate and momentum.\n",
        "noise_shape = (8,)\n",
        "desired_confidence_shape=(1,)\n",
        "BB_output_shape=(500,)\n",
        "discriminator = build_discriminator(noise_shape,desired_confidence_shape,BB_output_shape)\n",
        "discriminator.compile(loss='binary_crossentropy',\n",
        "    optimizer=optimizer,\n",
        "    metrics=['accuracy'])\n",
        "\n",
        "#build and compile our Discriminator, pick the loss function\n",
        "\n",
        "#SInce we are only generating (faking) samples, let us not track any metrics.\n",
        "generator = build_generator(noise_shape,desired_confidence_shape)\n",
        "generator.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "\n",
        "##This builds the Generator and defines the input noise. \n",
        "#In a GAN the Generator network takes noise z as an input to produce its samples.  \n",
        "z = Input(shape=(8,)) \n",
        "c = Input(shape=(1,))  #Our random input to the generator\n",
        "test_if_diabete = generator([z,c])\n",
        "\n",
        "#This ensures that when we combine our networks we only train the Generator.\n",
        "#While generator training we do not want discriminator weights to be adjusted. \n",
        "#This Doesn't affect the above descriminator training.     \n",
        "discriminator.trainable = False  \n",
        "\n",
        "#This specifies that our Discriminator will take the samples generated by our Generator\n",
        "#and true dataset and set its output to a parameter called valid, which will indicate\n",
        "#whether the input is real or not. \n",
        " \n",
        "valid = discriminator([test_if_diabete,c])  #Validity check on the generated sample\n",
        "\n",
        "\n",
        "#Here we combined the models and also set our loss function and optimizer. \n",
        "#Again, we are only training the generator here. \n",
        "#The ultimate goal here is for the Generator to fool the Discriminator.  \n",
        "# The combined model  (stacked generator and discriminator) takes\n",
        "# noise as input => generates samples => determines validity\n",
        "\n",
        "combined = Model([z, valid])\n",
        "combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "\n",
        "\n",
        "train(epochs=150, batch_size=1000, save_interval=10)\n",
        "\n",
        "#Save model for future use to generate fake samples\n",
        "#Not tested yet... make sure right model is being saved..\n",
        "#Compare with GAN4\n",
        "\n",
        "generator.save('generator_model2.h5')  #Test the model on GAN4_predict...\n",
        "#Change epochs back to 30K\n",
        "                \n",
        "#Epochs dictate the number of backward and forward propagations, the batch_size\n",
        "#indicates the number of training samples per backward/forward propagation, and the\n",
        "#sample_interval specifies after how many epochs we call our sample function."
      ],
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_122\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_100 (InputLayer)          [(None, 8)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_300 (Dense)               (None, 8)            72          input_100[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_303 (Dense)               (None, 8)            72          input_100[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "input_101 (InputLayer)          [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_301 (Dense)               (None, 512)          4608        dense_300[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_304 (Dense)               (None, 64)           576         dense_303[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_306 (Dense)               (None, 1)            2           input_101[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_184 (LeakyReLU)     (None, 512)          0           dense_301[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_186 (LeakyReLU)     (None, 64)           0           dense_304[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_188 (LeakyReLU)     (None, 1)            0           dense_306[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_302 (Dense)               (None, 64)           32832       leaky_re_lu_184[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense_305 (Dense)               (None, 64)           4160        leaky_re_lu_186[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense_307 (Dense)               (None, 1)            2           leaky_re_lu_188[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_185 (LeakyReLU)     (None, 64)           0           dense_302[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_187 (LeakyReLU)     (None, 64)           0           dense_305[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_189 (LeakyReLU)     (None, 1)            0           dense_307[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf.concat_31 (TFOpLambda)       (None, 129)          0           leaky_re_lu_185[0][0]            \n",
            "                                                                 leaky_re_lu_187[0][0]            \n",
            "                                                                 leaky_re_lu_189[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense_308 (Dense)               (None, 2)            260         tf.concat_31[0][0]               \n",
            "==================================================================================================\n",
            "Total params: 42,584\n",
            "Trainable params: 42,584\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Model: \"model_125\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_103 (InputLayer)          [(None, 8)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_309 (Dense)               (None, 8)            72          input_103[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_190 (LeakyReLU)     (None, 8)            0           dense_309[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_105 (BatchN (None, 8)            32          leaky_re_lu_190[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense_310 (Dense)               (None, 64)           576         batch_normalization_105[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_191 (LeakyReLU)     (None, 64)           0           dense_310[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "input_104 (InputLayer)          [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_106 (BatchN (None, 64)           256         leaky_re_lu_191[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense_313 (Dense)               (None, 1)            2           input_104[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_311 (Dense)               (None, 512)          33280       batch_normalization_106[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_193 (LeakyReLU)     (None, 1)            0           dense_313[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_192 (LeakyReLU)     (None, 512)          0           dense_311[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_108 (BatchN (None, 1)            4           leaky_re_lu_193[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_107 (BatchN (None, 512)          2048        leaky_re_lu_192[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense_314 (Dense)               (None, 1)            2           batch_normalization_108[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dense_312 (Dense)               (None, 8)            4104        batch_normalization_107[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_194 (LeakyReLU)     (None, 1)            0           dense_314[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.tanh_18 (TFOpLambda)    (None, 8)            0           dense_312[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_109 (BatchN (None, 1)            4           leaky_re_lu_194[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "tf.concat_32 (TFOpLambda)       (None, 9)            0           tf.math.tanh_18[0][0]            \n",
            "                                                                 batch_normalization_109[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dense_315 (Dense)               (None, 2)            20          tf.concat_32[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dense_316 (Dense)               (None, 1)            3           dense_315[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_317 (Dense)               (None, 8)            16          dense_316[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 40,419\n",
            "Trainable params: 39,247\n",
            "Non-trainable params: 1,172\n",
            "__________________________________________________________________________________________________\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 1) for input KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name='input_104'), name='input_104', description=\"created by layer 'input_104'\"), but it was called on an input with incompatible shape (None, 8).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-180-53255131569d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;31m#Save model for future use to generate fake samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-104-07a9919eeb66>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epochs, batch_size, save_interval)\u001b[0m\n\u001b[1;32m     27\u001b[0m       \u001b[0mscalar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhalf_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m       \u001b[0;31m# Generate a half batch of fake samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m       \u001b[0mgen_diabetes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscalar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m       \u001b[0;31m# Train the discriminator on real and fake samples, separately\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1700\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1701\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1702\u001b[0;31m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1703\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    931\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    762\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    763\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 764\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3048\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3049\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3050\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3051\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3052\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3443\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3444\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3445\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3287\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3288\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3289\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3290\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    997\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    998\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 999\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1000\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    673\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    984\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /usr/local/lib/python3.7/dist-packages/keras/engine/training.py:1544 predict_function  *\n        return step_function(self, iterator)\n    /usr/local/lib/python3.7/dist-packages/keras/engine/training.py:1527 run_step  *\n        outputs = model.predict_step(data)\n    /usr/local/lib/python3.7/dist-packages/keras/engine/training.py:1500 predict_step  *\n        return self(x, training=False)\n    /usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py:989 __call__  *\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    /usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:416 call  *\n        inputs, training=training, mask=mask)\n    /usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:551 _run_internal_graph  *\n        outputs = node.layer(*args, **kwargs)\n    /usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py:989 __call__  *\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    /usr/local/lib/python3.7/dist-packages/keras/engine/input_spec.py:248 assert_input_compatibility  *\n        raise ValueError(\n\n    ValueError: Input 0 of layer dense_313 is incompatible with the layer: expected axis -1 of input shape to have value 1 but received input with shape (None, 8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        },
        "id": "u3HksBpdF5JD",
        "outputId": "8cad0e94-ee3f-47d5-9aaf-71ecdb9f824a"
      },
      "source": [
        "half_batch = int(254 / 2)\n",
        "#We then loop through a number of epochs to train our Discriminator by first selecting\n",
        "#a random batch of samples from our true dataset, generating a set of samples from our\n",
        "#Generator, feeding both set of samples into our Discriminator, and finally setting the\n",
        "#loss parameters for both the real and fake samples, as well as the combined loss. \n",
        "    \n",
        "for epoch in range(50):\n",
        "\n",
        "  # ---------------------\n",
        "  #  Train Discriminator\n",
        "  # ---------------------\n",
        "  '''\n",
        "  # Select a random half batch of real samples\n",
        "\n",
        "  idx = np.random.randint(0, Xtrain.shape[0], half_batch)\n",
        "  diabetes = Xtrain.iloc[idx]\n",
        "\n",
        "  '''\n",
        "\n",
        "\n",
        "  noise = np.random.normal(0, 1, (half_batch, 8))\n",
        "  scalar = np.random.uniform(0, 1,(half_batch,1))\n",
        "  # Generate a half batch of fake samples\n",
        "  gen_diabetes = generator.predict((noise,scalar))\n",
        "  x_scaled = min_max_scaler.fit_transform(gen_diabetes)\n",
        "  Xtest = pd.DataFrame(x_scaled)\n",
        "  #Xtest=pd.DataFrame(gen_diabetes)\n",
        "  ypred=bb_model.predict(Xtest).reshape(half_batch,1)\n",
        "  # Train the discriminator on real and fake samples, separately\n",
        "  #Research showed that separate training is more effective. \n",
        "  zero_indexes=np.where(ypred == '0')[0].tolist()\n",
        "  one_indexes=np.where(ypred == '1')[0].tolist()\n",
        "  ###Xtest_zero = \n",
        "  d_loss_one = discriminator.train_on_batch(gen_diabetes,scalar,np.ones((half_batch, 1)))\n",
        "  d_loss_zero = discriminator.train_on_batch(gen_diabetes,np.zeros((half_batch, 1)))\n",
        "#take average loss from real and fake samples. \n",
        "\n",
        "  d_loss = 0.5 * np.add(d_loss_real, d_loss_fake) \n",
        "  euclidean_distance = distance.euclidean(d_loss_real,d_loss_fake)\n",
        "  '''\n",
        "  if (euclidean_distance > 1.1):\n",
        "    print (\"real: \"+str(d_loss_real)+\" , fake: \"+str(d_loss_fake) + \" , Duclidean distance: \"+ str(euclidean_distance))\n",
        "    print ()\n",
        "    print(\"real diabetes values:\")\n",
        "    print(pd.DataFrame(diabetes))\n",
        "    print(\"fake diabetes values:\")\n",
        "    print(pd.DataFrame(gen_diabetes))\n",
        "  '''\n",
        "  \n",
        "  \n",
        " \n",
        "    \n",
        "  \n",
        "#And within the same loop we train our Generator, by setting the input noise and\n",
        "#ultimately training the Generator to have the Discriminator label its samples as valid\n",
        "#by specifying the gradient loss.\n",
        "  # ---------------------\n",
        "  #  Train Generator\n",
        "  # ---------------------\n",
        "#Create noise vectors as input for generator. \n",
        "#Create as many noise vectors as defined by the batch size. \n",
        "#Based on normal distribution. Output will be of size (batch size, 100)\n",
        "  noise = np.random.normal(0, 1, (batch_size, 8)) \n",
        "\n",
        "  # The generator wants the discriminator to label the generated samples\n",
        "  # as valid (ones)\n",
        "  #This is where the genrator is trying to trick discriminator into believing\n",
        "  #the generated sample is true (hence value of 1 for y)\n",
        "  valid_y = np.array([1] * batch_size) #Creates an array of all ones of size=batch size\n",
        "\n",
        "  # Generator is part of combined where it got directly linked with the discriminator\n",
        "  # Train the generator with noise as x and 1 as y. \n",
        "  # Again, 1 as the output as it is adversarial and if generator did a great\n",
        "  #job of folling the discriminator then the output would be 1 (true)\n",
        "  g_loss = combined.train_on_batch(noise, valid_y)\n",
        "  if (g_loss > 1):\n",
        "    print(\"fake diabetes noise values that fooled the model:\")\n",
        "    print(pd.DataFrame(noise))\n",
        "\n",
        "#Additionally, in order for us to keep track of our training process, we print the\n",
        "#progress and save the sample sample output depending on the epoch interval specified.  \n",
        "#Plot the progress\n",
        "  \n",
        "  print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))"
      ],
      "execution_count": 220,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-220-41bb660d3f61>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m   \u001b[0;31m# Train the discriminator on real and fake samples, separately\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m   \u001b[0;31m#Research showed that separate training is more effective.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m   \u001b[0md_loss_one\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_diabetes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscalar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhalf_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m   \u001b[0md_loss_zero\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_diabetes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhalf_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m#take average loss from real and fake samples.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[1;32m   1798\u001b[0m                                                     class_weight)\n\u001b[1;32m   1799\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1800\u001b[0;31m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1801\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1802\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    922\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 924\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    925\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3020\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3021\u001b[0m       (graph_function,\n\u001b[0;32m-> 3022\u001b[0;31m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0m\u001b[1;32m   3023\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m   3024\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3439\u001b[0m               call_context_key in self._function_cache.missed):\n\u001b[1;32m   3440\u001b[0m             return self._define_function_with_shape_relaxation(\n\u001b[0;32m-> 3441\u001b[0;31m                 args, kwargs, flat_args, filtered_flat_args, cache_key_context)\n\u001b[0m\u001b[1;32m   3442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3443\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_define_function_with_shape_relaxation\u001b[0;34m(self, args, kwargs, flat_args, filtered_flat_args, cache_key_context)\u001b[0m\n\u001b[1;32m   3361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3362\u001b[0m     graph_function = self._create_graph_function(\n\u001b[0;32m-> 3363\u001b[0;31m         args, kwargs, override_flat_arg_shapes=relaxed_arg_shapes)\n\u001b[0m\u001b[1;32m   3364\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marg_relaxed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrank_only_cache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3287\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3288\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3289\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3290\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    997\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    998\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 999\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1000\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    673\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    984\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /usr/local/lib/python3.7/dist-packages/keras/engine/training.py:830 train_function  *\n        return step_function(self, iterator)\n    /usr/local/lib/python3.7/dist-packages/keras/engine/training.py:813 run_step  *\n        outputs = model.train_step(data)\n    /usr/local/lib/python3.7/dist-packages/keras/engine/training.py:770 train_step  *\n        y_pred = self(x, training=True)\n    /usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py:989 __call__  *\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    /usr/local/lib/python3.7/dist-packages/keras/engine/input_spec.py:197 assert_input_compatibility  *\n        raise ValueError('Layer ' + layer_name + ' expects ' +\n\n    ValueError: Layer model_122 expects 2 input(s), but it received 1 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(None, 8) dtype=float32>]\n"
          ]
        }
      ]
    }
  ]
}